<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Eda on Ding&#39;s Coding Forge</title>
    <link>https://dingyu.dev/categories/eda/</link>
    <description>Recent content in Eda on Ding&#39;s Coding Forge</description>
    <image>
      <title>Ding&#39;s Coding Forge</title>
      <url>https://dingyu.dev/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://dingyu.dev/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- 0.144.0</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="https://dingyu.dev/categories/eda/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[EDA] Kafka (MSK) Monitoring with Burrow Prometheus And Thanos</title>
      <link>https://dingyu.dev/posts/dance-with-burrow/</link>
      <pubDate>Fri, 21 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://dingyu.dev/posts/dance-with-burrow/</guid>
      <description>People often rely solely on basic Kafka metrics like Input/Output Bytes, missing crucial insights into their event-driven architecture. This post demonstrates how to set up comprehensive topic-level monitoring for Kafka (AWS MSK) using Burrow, Prometheus, and Thanos. By combining Burrow for accurate consumer lag tracking, Prometheus for metric collection, and Thanos for long-term data storage and high availability, you&amp;#39;ll achieve effective Kafka monitoring without the high costs and limitations of cloud-native solutions.</description>
    </item>
    <item>
      <title>[EDA] Schema Registry</title>
      <link>https://dingyu.dev/posts/schema-registry/</link>
      <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
      <guid>https://dingyu.dev/posts/schema-registry/</guid>
      <description>People often underestimate the importance of documenting schemas before starting to code, especially when working with stream processing. In this post, Iâ€™ll explain why using a schema registry is essential and why designing schemas upfront is crucial before diving into coding.</description>
    </item>
    <item>
      <title>[EDA] Flink Dynamic Job Case Study</title>
      <link>https://dingyu.dev/posts/flink-dynamic-job/</link>
      <pubDate>Fri, 06 Dec 2024 00:00:00 +0000</pubDate>
      <guid>https://dingyu.dev/posts/flink-dynamic-job/</guid>
      <description>This post explores dynamic rule-based stream processing using Apache Flink for real-time fraud detection. It covers key topics such as dynamic key partitioning, broadcast state for rule updates, and custom window processing to efficiently evaluate transactions without redeploying jobs. The implementation ensures low-latency fraud detection by minimizing shuffle overhead, dynamically applying grouping keys, and leveraging stateful processing. Additionally, it discusses event retention strategies, performance considerations, and architecture trade-offs for building a scalable, high-performance fraud detection system.</description>
    </item>
  </channel>
</rss>

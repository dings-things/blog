[{"content":"This article is about testing\u0026hellip; What happens to the server if tens of millions of API requests per second are received, and these APIs communicate with an external API that responds in 10 seconds?\n⨳ Assume that the TCP connection timeout is not configured separately.\nWhat is a Goroutine? Goroutines are often called lightweight threads\u0026hellip; So is it just a thread with less memory?\nAdvantages of Goroutines Low Context Switching Cost As hardware has advanced, the concepts of multithreading and multitasking have emerged to utilize it efficiently.\nMultithreading: Multiple threads running within a single process\nMultitasking: Simultaneously executing multiple tasks (appearing so)\nOther Languages (C/C++/Java) Go Language The Go runtime performs lightweight context switching by using its own G (Goroutine), M (OS Thread), and P (Scheduler Context), without switching the kernel-level TCB (Task Control Block) directly.\nCreation and Destruction Cost Creating and destroying an OS Thread after use requires high cost. To avoid paying this cost every time, thread pools are often used.\nWhat about in Go?\nThe M in Go’s GMP model corresponds to the OS Thread. While a goroutine runs through the connection of a P(processor) and an M, the M can be created or destroyed as needed.\nHowever, this is optimized by the Go runtime scheduler, allowing it to be managed with much lower cost compared to general programming languages.\nLow Memory Consumption When created as threads, they require around 1MB of stack including memory protection between threads.\nIn contrast, a goroutine only needs 2KB of stack initially, resulting in significant lightweighting. (Of course, additional memory allocation may be required depending on the code executed by the goroutine—this applies to other languages as well.)\nHere’s a numerical comparison:\nLanguage Basic Execution Unit Default Stack Size Estimated Total Memory Use C/C++ pthread 8MB About 8MB or more Java java.lang.Thread 1MB About 1MB or more Go goroutine (G) 2KB (initial) Several KB GMP: Go Scheduler Architecture The Go runtime efficiently assigns, schedules, and manages goroutines using a system known as GMP:\nG: Goroutine (logical execution unit) M: Machine (an OS thread) P: Processor (scheduling context) This architecture allows Go to perform lightweight context switching and schedule massive numbers of goroutines across a limited set of OS threads.\nComponent Meaning Role Key Information G (Goroutine) Logical execution unit Code that needs to be executed Stores stack, instruction pointer, goroutine status, etc. M (Machine) OS thread Executes G Binds to P, executes G, and may handle syscalls P (Processor) Scheduling context Schedules G onto M Owns a Local Run Queue (LRQ), can access GRQ and perform work stealing LRQ (Local Run Queue) Per-P queue Stores Gs owned by a P Lower contention, faster scheduling GRQ (Global Run Queue) Shared queue Backup storage of runnable Gs Used when LRQs are empty or overflown Each P can be thought of as a “logical core” limited by GOMAXPROCS. It binds to an M (OS thread) and pulls Gs from its LRQ first, falling back to GRQ if needed.\nThe system is designed to:\nReduce kernel-level context switching Avoid excessive OS threads Enable highly scalable I/O concurrency P (Processor) P defaults to GOMAXPROCS = number of CPU cores P is assigned to one M, and each P owns its own Local Run Queue P holds context info of G It calls findRunnable() to decide which G to run next [runtime/proc.go]\nfunc findRunnable() (gp *g, inheritTime, tryWakeP bool) { mp := getg().m pp := mp.p.ptr() // local runq if gp, inheritTime := runqget(pp); gp != nil { return gp, inheritTime, false } // global runq if sched.runqsize != 0 { lock(\u0026amp;sched.lock) gp := globrunqget(pp, 0) unlock(\u0026amp;sched.lock) if gp != nil { return gp, false, false } } // Poll network. if netpollinited() \u0026amp;\u0026amp; netpollAnyWaiters() \u0026amp;\u0026amp; sched.lastpoll.Load() != 0 { if list, delta := netpoll(0); !list.empty() { gp := list.pop() injectglist(\u0026amp;list) netpollAdjustWaiters(delta) casgstatus(gp, _Gwaiting, _Grunnable) return gp, false, false } } // Spinning Ms: steal work from other Ps. if mp.spinning || 2*sched.nmspinning.Load() \u0026lt; gomaxprocs-sched.npidle.Load() { if !mp.spinning { mp.becomeSpinning() } gp, inheritTime, _, _, _ := stealWork(nanotime()) if gp != nil { return gp, inheritTime, false } } // fallback: no G found return nil, false, false } Order Source Description Use case ① Local Run Queue (LRQ) Unique queue per P Fastest and lowest cost → first ② Global Run Queue (GRQ) Shared queue among all Ps Used when LRQ is empty ③ Network Poller (Netpoll) Gs woken by epoll/kqueue/I/O events Revives Gs after network I/O ④ Work Stealing (other P’s LRQ) Steal Gs from another P’s LRQ Used when own LRQ \u0026amp; GRQ are empty Then naturally, this question arises\u0026hellip;\nAfter a blocking operation completes, how does the G return to P? This is the process of goroutine switching within a single P (processor).\nG1 is a goroutine performing a syscall (e.g., HTTPRequest)\nStep Description ① G1 enters a syscall (net.Read() is called) ② M1 blocks on the syscall → calls entersyscall(), P1 is detached ③ P1 is handed over to M2 → M2 is either idle or created via newm() ④ M2 picks and executes G2 from P1’s run queue ⑤ The OS detects syscall completion via epoll, kqueue, or IOCP ⑥ netpoller marks G1 as runnable → G1.status = _Grunnable ⑦ Scheduler later selects and resumes G1 through schedule() What if no runnable Goroutine is found in findRunnable()? The current M (OS thread) has no G to run stopm() is called → the current M is parked The P held by M is returned via releasep() The returned P is placed into the idle P queue Ps created up to the number of GOMAXPROCS do not disappear and stay in the idle state When a new runnable G appears, it reuses an idle P to resume execution M may be created again or an idle M reused if needed M (Machine) M receives a G and actually executes it as an OS Thread The default value for maxcount is 10000 What happens to M1 when its G (running a blocking operation) enters a syscall?\n[runtime/proc.go]\nfunc exitsyscall() { gp := getg() // Validate syscall stack frame if sys.GetCallerSP() \u0026gt; gp.syscallsp { throw(\u0026#34;exitsyscall: syscall frame is no longer valid\u0026#34;) } gp.waitsince = 0 oldp := gp.m.oldp.ptr() gp.m.oldp = 0 // Fast path: try to reacquire P and resume execution // if P is IDLE, return true and resume running goroutine if exitsyscallfast(oldp) { ... casgstatus(gp, _Gsyscall, _Grunning) // mark G as running return } // Slow path: failed to reacquire P // Call scheduler (park M and let scheduler run G later) mcall(exitsyscall0) } Condition Handling P can be reacquired G1 resumes immediately (execute(g)) P cannot be reacquired G1 is enqueued as runnable, M1 is stopped (stopm()) Too many idle Ms M1 may be completely terminated Lack of idle Ms M1 may be reused (newm() is avoided if possible) Multithreading with M (OS Thread) in other languages One OS thread is created per request, and that thread performs epoll_wait/syscall N worker threads are created, each handling epoll_wait or read Cons To handle 1,000 concurrent requests, up to 1,000 threads are required Each M (epoll_wait) blocks in syscall → leads to context switches *Cache misses, kernel entry cost, and scheduling overhead increase sharply Cache miss: After a context switch, if the required data is no longer in the CPU cache, it must be reloaded from memory (which is slower)\nNetpoller M The Netpoller M is a dedicated OS thread (M) maintained by the Go runtime, connected to the kernel\u0026rsquo;s I/O readiness monitoring systems like epoll, kqueue, etc.\nWhen goroutines use non-blocking I/O (fd), this M exclusively monitors the readable/writable status of those fds A single M can monitor thousands of fds, efficiently handling a large number of I/O goroutines with minimal resources Flow of two goroutines that include syscalls and the Netpoller M\nWhy can’t M make a syscall directly? Example: Assume G1 calls conn.Read() on a TCP socket\nIf the peer suddenly disconnects or NAT timeout occurs but the kernel does not send an EOF signal?\nThe read(fd) syscall never returns (remains in a blocked state) The M executing G1 becomes blocked on the syscall The P owned by that M is also released, reducing overall concurrency Because of this, Go checks the fd’s readiness using epoll before calling read() to avoid indefinite blocking.\nWhy does the Netpoller M exclusively handle epoll? Example: 1,000 Ms each calling epoll_wait(fd)\nAll 1,000 Ms independently call epoll_wait() epoll_wait is also a syscall, which incurs kernel-to-user space transition overhead We must also consider how frequently to perform epoll → adds polling interval overhead G (Goroutine) Goroutine Scheduling Let’s look at the execution flow of a goroutine.\ngo dosomething() is called to spawn a new goroutine.\nCurrent status\nLRQ is full A new goroutine is spawned A new goroutine (G0) checks if there’s space in the left P’s LRQ The current P\u0026rsquo;s LRQ is full (in practice, LRQ size = 256)\nHalf of the current LRQ is moved to GRQ (e.g., G2 → GRQ) G0 is inserted into the LRQ as P is now available\nG1 runs and G0 is set as runnext Since LRQ is empty, a G is taken from GRQ (fetches GRQ length / GOMAXPROCS + 1)\nG0 runs on M and sends a request to the socket G0 registers readiness using epoll_ctl(..., EPOLLIN)\nTo avoid blocking, it detaches from the current P Netpoller M checks readiness using epoll_wait\nM returns to IDLE state, and G0 waits via Gopark (later deleted or reattached to a P) Netpoller M detects readiness of G0’s fd\nNetpoller M marks G0 as Goready after detecting readiness P fails to find a G in LRQ and GRQ\nP directly fetches the ready G from netpoll and executes it Caveats GRQ Starvation Only LRQs may be checked repeatedly, causing GRQ starvation schedTick variable ensures GRQ is checked every 61 ticks Why 61? It’s a prime number experimentally proven to perform well. Like hash map sizing, prime numbers help avoid distribution conflicts with application patterns [runtime/proc.go]\nfunc findRunnable() (gp *g, inheritTime, tryWakeP bool) { mp := getg().m pp := mp.p.ptr() // Check the global runnable queue once in a while to ensure fairness if pp.schedtick%61 == 0 \u0026amp;\u0026amp; !sched.runq.empty() { lock(\u0026amp;sched.lock) gp := globrunqget() unlock(\u0026amp;sched.lock) if gp != nil { return gp, false, false } } // local runq if gp, inheritTime := runqget(pp); gp != nil { return gp, inheritTime, false } ... } Time slice based preemption To prevent one goroutine from monopolizing a processor, Go defines a default 10ms time slice. After this time, the running G is preempted and returned to GRQ.\n[runtime/proc.go]\nfunc sysmon() { ... for { ... lock(\u0026amp;sched.sysmonlock) now = nanotime() ... // retake P\u0026#39;s blocked in syscalls // and preempt long running G\u0026#39;s if retake(now) != 0 { idle = 0 } else { idle++ } ... unlock(\u0026amp;sched.sysmonlock) } } What happens when handling tens of millions of network I/O? Go\u0026rsquo;s goroutines can perform efficiently even under large-scale network I/O thanks to lightweight context switching, small memory usage, and epoll-based polling architecture.\nHowever, if the external API response is slow (e.g., 10 seconds), and tens of millions of requests per second flood in, the following bottlenecks and risks arise:\nPotential Error Cases 1. Explosive increase in goroutine count If every request waits \u0026gt;10s due to external API, goroutines pile up in waiting state 10M req/s × 10s = up to 100M concurrent goroutines At 2KB per goroutine, memory usage = 100M × 2KB ≒ 200GB+ System memory gets exhausted → OOM 2. File descriptor (fd) limit Each TCP connection consumes an fd Linux ulimit -n typically limits open files to thousands or tens of thousands New requests eventually fail due to fd exhaustion Test: Is running 100M goroutines okay? func init() { http.HandleFunc(\u0026#34;/slow\u0026#34;, func(w http.ResponseWriter, r *http.Request) { time.Sleep(10 * time.Second) }) go func() { log.Println(\u0026#34;slow API server start in :18080\u0026#34;) http.ListenAndServe(\u0026#34;:18080\u0026#34;, nil) }() } func main() { // ===== trace start ===== traceFile, err := os.Create(\u0026#34;trace.out\u0026#34;) if err != nil { log.Fatalf(\u0026#34;trace file creation failed: %v\u0026#34;, err) } if err := trace.Start(traceFile); err != nil { log.Fatalf(\u0026#34;trace start failed: %v\u0026#34;, err) } defer func() { trace.Stop() traceFile.Close() }() // ===================== startProfiler() for i := 0; i \u0026lt; 100000000; i++ { go func(i int) { client := \u0026amp;http.Client{} resp, err := client.Get(\u0026#34;http://localhost:18080/slow\u0026#34;) if err != nil { fmt.Printf(\u0026#34;[%d] Error: %v\u0026#34;, i, err) return } io.Copy(io.Discard, resp.Body) resp.Body.Close() }(i) if i%100000 == 0 { fmt.Printf( \u0026#34;Current Request Count: %d, Goroutine Count: %d\u0026#34;, i, runtime.NumGoroutine(), ) } } select {} } heap profile net/http.(*Transport).getConn: Fails to reuse connections → FD surge → heap grows runtime.malg: Goroutines wait → their stacks remain → heap inflates cpu profile runtime.cgocall: Bursts of HTTP syscalls runtime.(*timers).adjust: Too many timers from time.Sleep increase min-heap ops Unbounded goroutines waiting = heap bloat = OOM = crash Even with connection pooling:\nclient := \u0026amp;http.Client{ Transport: \u0026amp;http.Transport{ MaxIdleConns: 10000, MaxIdleConnsPerHost: 10000, IdleConnTimeout: 90 * time.Second, DisableKeepAlives: false, }, Timeout: 30 * time.Second, } Still ends in OOM due to: REFS Dmitry Vtukov GopherCon 2021: Madhav Jivrajani - Queues, Fairness, and The Go Scheduler ","permalink":"https://dingyu.dev/en/posts/gmp/","summary":"Goroutines are one of the key advantages of the Go language, known for being lightweight—even when spawning hundreds of thousands or more. However, what happens if these goroutines remain in a parked state for extended periods? This article explains the operational principles of goroutine parking and delves into the internals of Go\u0026rsquo;s GMP runtime model. It explores the system load and performance impact when many goroutines are inactive, and introduces practical strategies to detect and prevent such issues.","title":"[Go] Is It Okay to Run 100 Million Goroutines?"},{"content":"Background One day, a colleague introduced a session about Redlock in a study group I participate in.\nIt was such great content that I wanted to dig deeper with my own understanding.\nIt would be helpful to review Redlock Algorithm beforehand.\nLock for what? Locks are mainly used to ensure efficiency and correctness.\nEfficiency Prevent redundant work from being executed unnecessarily.\nex. N nodes performing the same heavy task (taking 10 minutes) simultaneously, leading to cost/time waste. Correctness Enable consistent and accurate data processing on shared resources across concurrent processes.\nex. N nodes processing a user\u0026rsquo;s withdrawal logic simultaneously, causing the user\u0026rsquo;s account to be charged N times. According to Martin Kleppmann, if you\u0026rsquo;re considering using Redis Lock for efficiency, it is recommended not to use Redlock.\nItem Single Redis Lock Redis Redlock Algorithm Lock Target Single Redis instance 5 independent Redis instances Lock Creation Method SET key value NX PX \u0026lt;TTL\u0026gt; Attempt SET key value NX PX \u0026lt;TTL\u0026gt; on all 5 nodes Success Condition Successful lock on one Redis Successful lock on majority (3 out of 5) nodes Failure Handling Lock information lost if Redis fails Majority lock remains safe even if some nodes fail Split Brain Handling Impossible Partially possible (not perfect) Consistency Weak consistency (single instance) Strengthened consistency during lock acquisition (multi-instance) Complexity Simple (easy to implement) Complex (requires handling lock acquisition time, clock drift) Fault Tolerance Low Relatively higher Performance Fast (single node access) May be slower (communication with 5 nodes) Main Use Case Small systems, single-server environments Global distributed systems, high-availability lock systems If the Redis node crashes unexpectedly\nTimeout occurs while trying to acquire the lock → application response delay or business logic execution failure\nIncomplete Lock Using a single Redis node cannot guarantee high availability and stability during failure scenarios.\nFail case 1: Lock release due to GC Stop-the-World pause Duration of STW is unpredictable. Even Concurrent GC cannot avoid STW. Fail case 2: After acquiring lock, external port operations (API, DB, HDFS\u0026hellip;) experience packet loss After acquiring lock, delays during IO operations → TTL (lease) expiration → another thread may acquire the lock and perform the same operation. Delays caused by packet loss in external network operations → TTL (lease) expiration \u0026hellip; SPoF Solution: Master - Slave Structure During failover, TTL expiration may lead to unlock, causing data corruption. Stability Solution: Safe Lock with Fencing Similar to first commit wins in MVCC, transaction handling at the storage level is based on version (token).\nclient 1 successfully acquires the lock (with token33) but encounters delay during storage write (GC, network delay, etc.) client 1\u0026rsquo;s lock lease expires. client 2 acquires the lock (with token34) and completes the write operation before client 1 finishes. client 1 attempts storage write → storage rejects token33 because it\u0026rsquo;s older than token34 (transaction fail). The biggest problem is: who generates the fencing token? In a distributed environment, implementing a counter requires another leader election\u0026hellip; (an infinite loop)\nRedlock Operation Flow Record the current time in milliseconds. Try to acquire the lock on all N Redis instances sequentially with the same key and a random value. Set a short timeout for each attempt so that if a node is down, move to the next instance immediately. Calculate the time taken to acquire locks, and if locks are successfully acquired on the majority of instances and the time taken is less than the lock\u0026rsquo;s validity time, the lock is considered acquired. If the lock is acquired, set the new validity time as (initial validity − elapsed time). If the lock is not acquired, or if the remaining validity time is negative (exceeded during acquisition), release the locks from all instances. Bad Timing Issue Category Description General Distributed System Assumes \u0026ldquo;cannot trust time\u0026rdquo; → ensures safety unconditionally, only liveness depends on timing Redlock Relies on time (clock accuracy, network delay) to guarantee lock safety Problem If clocks jump forward/backward (GC, NTP, network delay), lock expiration calculations may fail and lock can be broken Result Not just liveness degradation — safety violations (e.g., data corruption, duplicate execution) can occur Scenario Description First (Clock Jump) Redis C node\u0026rsquo;s clock jumps forward, causing early TTL expiration. Client 1 thinks it still holds the lock, but Client 2 acquires it again, leading both to believe they own the lock. Second (GC Pause) Client 1 sends lock requests but pauses (GC), during which locks expire. Client 2 acquires new locks, while Client 1 later processes stale success responses. Synchrony assumptions of Redlock Condition Description Bounded Network Delay Packets must arrive within a guaranteed maximum delay Bounded Process Pause GC or system pauses must stay within a limited time Bounded Clock Drift Clock drift must be small; NTP synchronization must be reliable ➔ That is, all delays, pauses, and clock drifts must be much smaller than the lock\u0026rsquo;s TTL (time-to-live) for Redlock to function correctly.\nIs it realistic to expect such conditions? Remember GitHub\u0026rsquo;s 90-second packet delay.\nUltimately\u0026hellip; Redlock is an algorithm that relies on time, and due to clock jumps, GC STW, and network packet loss, it cannot guarantee correctness.\nSince Redis was never designed for \u0026ldquo;consensus\u0026rdquo; but rather as a key-value store, for truly reliable locks, it is better to use solutions like Zookeeper or Raft instead of Redlock.\n","permalink":"https://dingyu.dev/en/posts/distributed-locking/","summary":"How are leaders managed in distributed environments? Introducing two approaches to concurrency control.","title":"[DB] Redlock and Lease in Distributed Systems"},{"content":"Setting Up Local SASL Mechanism Kafka Cluster with Docker Compose When using Kafka with SASL-based authentication, it\u0026rsquo;s important to replicate a similar authentication environment locally for testing.\nThis post documents how to build a Kafka cluster in a local environment that simultaneously supports both SCRAM-SHA-256 and SCRAM-SHA-512, and how to test the Event Dispatcher application on top of it.\nIt was extremely difficult to find references for setting up SASL and TLS locally\u0026hellip;\nSo while struggling through the setup, I decided to leave this guide as a gift for my future self.\n(TLS setup with Makefile for certificates was a bit too complex, so I\u0026rsquo;ll cover that in a separate post later.)\nGoals The Event Dispatcher consumes events from a source Kafka and produces to a destination Kafka depending on event types. Source and Destination Kafka are on the same cluster, but differentiated by different SASL mechanisms. Source Kafka uses SCRAM-SHA-256, Destination Kafka uses SCRAM-SHA-512. The goal is to replicate the production environment structure without modifying the application code even during tests. Kafka cluster has 3 brokers to set ISR (in-sync replicas) to 2. Kafka UI should allow manual produce tests to verify application behavior. Why Set Up a Local Environment? Debugging on a shared dev server was inconvenient because of limited stack trace access and the need to go through VPN/Bastion gateways. All internal Kafka clusters use SASL, so creating a reusable setup would benefit many projects. Configuration should be modifiable for testing (like transaction settings, exactly-once semantics, ISR tuning) without code changes or deployments. A local infrastructure was necessary to maintain identical codebases during testing. Docker Compose Configuration version: \u0026#39;3.9\u0026#39; networks: kafka_network: volumes: kafka_data_0: kafka_data_1: kafka_data_2: services: zookeeper: image: bitnami/zookeeper:3.8.1 container_name: zookeeper environment: - ALLOW_ANONYMOUS_LOGIN=yes ports: - \u0026#39;2181:2181\u0026#39; networks: - kafka_network kafka-0: image: bitnami/kafka:3.7.0 container_name: kafka-0 depends_on: - zookeeper ports: - \u0026#39;${KAFKA_BROKER_0_PORT}:9092\u0026#39; environment: KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_CFG_LISTENERS: SASL_PLAINTEXT://:9092 KAFKA_CFG_ADVERTISED_LISTENERS: SASL_PLAINTEXT://host.docker.internal:${KAFKA_BROKER_0_PORT} KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: SASL_PLAINTEXT:SASL_PLAINTEXT KAFKA_CFG_INTER_BROKER_LISTENER_NAME: SASL_PLAINTEXT KAFKA_CFG_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512,SCRAM-SHA-256 KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512 KAFKA_CLIENT_USERS: ${512_SASL_USER},${256_SASL_USER} KAFKA_CLIENT_PASSWORDS: ${512_SASL_PASSWORD},${256_SASL_PASSWORD} KAFKA_INTER_BROKER_USER: ${512_SASL_USER} KAFKA_INTER_BROKER_PASSWORD: ${512_SASL_PASSWORD} volumes: - kafka_data_0:/bitnami/kafka networks: - kafka_network hostname: kafka kafka-1: image: bitnami/kafka:3.7.0 container_name: kafka-1 depends_on: - zookeeper ports: - \u0026#39;${KAFKA_BROKER_1_PORT}:9092\u0026#39; environment: KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_CFG_LISTENERS: SASL_PLAINTEXT://:9092 KAFKA_CFG_ADVERTISED_LISTENERS: SASL_PLAINTEXT://host.docker.internal:${KAFKA_BROKER_1_PORT} KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: SASL_PLAINTEXT:SASL_PLAINTEXT KAFKA_CFG_INTER_BROKER_LISTENER_NAME: SASL_PLAINTEXT KAFKA_CFG_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512,SCRAM-SHA-256 KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512 KAFKA_CLIENT_USERS: ${512_SASL_USER},${256_SASL_USER} KAFKA_CLIENT_PASSWORDS: ${512_SASL_PASSWORD},${256_SASL_PASSWORD} KAFKA_INTER_BROKER_USER: ${512_SASL_USER} KAFKA_INTER_BROKER_PASSWORD: ${512_SASL_PASSWORD} volumes: - kafka_data_1:/bitnami/kafka networks: - kafka_network hostname: kafka-1 kafka-2: image: bitnami/kafka:3.7.0 container_name: kafka-2 depends_on: - zookeeper ports: - \u0026#39;${KAFKA_BROKER_2_PORT}:9092\u0026#39; environment: KAFKA_CFG_ZOOKEEPER_CONNECT: zookeeper:2181 KAFKA_CFG_LISTENERS: SASL_PLAINTEXT://:9092 KAFKA_CFG_ADVERTISED_LISTENERS: SASL_PLAINTEXT://host.docker.internal:${KAFKA_BROKER_2_PORT} KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP: SASL_PLAINTEXT:SASL_PLAINTEXT KAFKA_CFG_INTER_BROKER_LISTENER_NAME: SASL_PLAINTEXT KAFKA_CFG_SASL_ENABLED_MECHANISMS: SCRAM-SHA-512,SCRAM-SHA-256 KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL: SCRAM-SHA-512 KAFKA_CLIENT_USERS: ${512_SASL_USER},${256_SASL_USER} KAFKA_CLIENT_PASSWORDS: ${512_SASL_PASSWORD},${256_SASL_PASSWORD} KAFKA_INTER_BROKER_USER: ${512_SASL_USER} KAFKA_INTER_BROKER_PASSWORD: ${512_SASL_PASSWORD} volumes: - kafka_data_2:/bitnami/kafka networks: - kafka_network hostname: kafka-2 kafka-ui: image: provectuslabs/kafka-ui:latest container_name: kafka-ui depends_on: - kafka-0 ports: - \u0026#39;8080:8080\u0026#39; environment: KAFKA_CLUSTERS_0_NAME: Local-Zookeeper-Cluster KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: host.docker.internal:${KAFKA_BROKER_0_PORT},host.docker.internal:${KAFKA_BROKER_1_PORT},host.docker.internal:${KAFKA_BROKER_2_PORT} KAFKA_CLUSTERS_0_PROPERTIES_SECURITY_PROTOCOL: SASL_PLAINTEXT KAFKA_CLUSTERS_0_PROPERTIES_SASL_MECHANISM: SCRAM-SHA-512 KAFKA_CLUSTERS_0_PROPERTIES_SASL_JAAS_CONFIG: org.apache.kafka.common.security.scram.ScramLoginModule required username=\u0026#34;${512_SASL_USER}\u0026#34; password=\u0026#34;${512_SASL_PASSWORD}\u0026#34;; networks: - kafka_network your-app: env_file: - .env build: context: . dockerfile: dev.Dockerfile args: - VERSION=dev environment: - BOOTSTRAP_SERVERS_256=host.docker.internal:${KAFKA_BROKER_0_PORT},host.docker.internal:${KAFKA_BROKER_1_PORT},host.docker.internal:${KAFKA_BROKER_2_PORT} - BOOTSTRAP_SERVERS_512=host.docker.internal:${KAFKA_BROKER_0_PORT},host.docker.internal:${KAFKA_BROKER_1_PORT},host.docker.internal:${KAFKA_BROKER_2_PORT} image: your-app container_name: your-app networks: - kafka_network restart: always depends_on: - kafka-0 - kafka-1 - kafka-2 Why Bitnami? Simple User Registration with Environment Variables Bitnami Kafka allows automatic SASL user registration simply by setting the following environment variables:\nKAFKA_CLIENT_USERS=user256,user512 KAFKA_CLIENT_PASSWORDS=pass256,pass512 In contrast, the official Kafka image requires manually running kafka-configs.sh or creating a custom entrypoint.\nExample with official Kafka:\nbash -c \u0026#39; /opt/bitnami/scripts/kafka/setup.sh \u0026amp;\u0026amp; kafka-configs.sh --zookeeper zookeeper:2181 --alter \\ --add-config \u0026#34;SCRAM-SHA-512=[iterations=8192,password=pass]\u0026#34; \\ --entity-type users --entity-name user \u0026amp;\u0026amp; /opt/bitnami/scripts/kafka/run.sh\u0026#39; Bitnami handles user registration automatically during container startup.\nBuilt-in SASL and Zookeeper Integration Bitnami allows setting SASL and Zookeeper configurations through environment variables without editing server.properties:\nKAFKA_CFG_SASL_ENABLED_MECHANISMS KAFKA_CFG_LISTENER_NAME_\u0026lt;listener\u0026gt;_SASL_ENABLED_MECHANISMS KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL Environment Variable Description Purpose KAFKA_CFG_ZOOKEEPER_CONNECT Address of Zookeeper (host:port) Required for Kafka to store/share cluster metadata KAFKA_CFG_LISTENERS Protocol and port for external connections Defines how clients and brokers communicate KAFKA_CFG_ADVERTISED_LISTENERS Address advertised to clients Provides connection info to Kafka clients KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP Maps listener names to security protocols Activates SASL authentication KAFKA_CFG_INTER_BROKER_LISTENER_NAME Listener used for broker-to-broker communication Sets which listener (auth method) is used internally KAFKA_CFG_SASL_ENABLED_MECHANISMS List of allowed SASL mechanisms Defines available SASL mechanisms KAFKA_CFG_SASL_MECHANISM_INTER_BROKER_PROTOCOL SASL mechanism for inter-broker communication Selects SCRAM algorithm for internal authentication KAFKA_CLIENT_USERS Comma-separated list of users Registers users in Kafka KAFKA_CLIENT_PASSWORDS Comma-separated list of passwords Associates passwords with users KAFKA_INTER_BROKER_USER User for inter-broker authentication Specifies the user for broker-to-broker SASL authentication KAFKA_INTER_BROKER_PASSWORD Password for inter-broker user Password for the above user The environment includes:\n3-node Bitnami Kafka cluster (supporting both SCRAM-SHA-256 and SCRAM-SHA-512) Zookeeper Kafka UI for management and testing Event Dispatcher application (Kafka Consumer/Producer) After setting up the .env file, start the infrastructure with:\ndocker compose --env-file .env up --build To stop and clean up the cluster (including volumes):\ndocker compose down -v Example .env file:\n256_SASL_USER=user256 256_SASL_PASSWORD=pass256 512_SASL_USER=user512 512_SASL_PASSWORD=pass512 # Kafka Settings KAFKA_BROKER_0_PORT=9092 KAFKA_BROKER_1_PORT=9093 KAFKA_BROKER_2_PORT=9094 Detailed Component Overview The diagram below summarizes how the Kafka cluster, Event Dispatcher, and Kafka UI interact within the local environment. It describes the initialization and authentication sequences based on actual startup logs.\n1. Zookeeper Initialization Zookeeper container starts in standalone mode, acting as metadata storage for Kafka brokers. Kafka brokers automatically register SCRAM users via KAFKA_CLIENT_USERS and KAFKA_CLIENT_PASSWORDS. user256 (SCRAM-SHA-256) and user512 (SCRAM-SHA-512) are registered in Zookeeper. 2. Kafka Broker Startup and Zookeeper Connection Brokers (kafka-0, kafka-1, kafka-2) connect sequentially to Zookeeper. Once connected, brokers participate in controller election and become ready to serve requests. 3. Controller Election One broker is elected as the controller. The controller synchronizes broker states, partition metadata, and leader elections. Messages like LeaderAndIsr and UpdateMetadataRequest are exchanged to stabilize the cluster. 4. Kafka UI Connection Kafka UI connects to the brokers using user512 via SCRAM-SHA-512 authentication. After authentication, topics can be browsed, and test messages can be produced. 5. Event Dispatcher Connection Event Dispatcher connects to the source Kafka using SCRAM-SHA-256 (user256) and starts consuming. It connects to the destination Kafka using SCRAM-SHA-512 (user512) and produces processed messages. Full Sequence Diagram Troubleshooting and Solutions 1. Incorrect KAFKA_CFG_ADVERTISED_LISTENERS Initially, setting localhost caused connection failures from Kafka UI and Event Dispatcher.\nSymptoms:\nKafka UI or Event Dispatcher couldn\u0026rsquo;t connect. connection refused or EOF errors. Solution:\nUse host.docker.internal or the actual host IP instead of localhost. KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://host.docker.internal:9092 2. Authentication Failures with Different SASL Mechanisms User registrations weren\u0026rsquo;t properly set for both SCRAM mechanisms.\nSymptoms:\nKafka UI succeeds, Event Dispatcher fails with EOF or auth error. Kafka logs show Failed to authenticate user. Solution:\nEnsure users are registered properly using --mechanism option. Verify using logs or kafka-configs.sh. --entity-type users --entity-name user512 --alter --add-config \u0026#39;SCRAM-SHA-512=[iterations=4096,password=pass512]\u0026#39; Eventually switched to setting KAFKA_CLIENT_USERS and KAFKA_CLIENT_PASSWORDS.\n3. Produce Failures in Kafka UI Kafka UI failed to produce messages due to auth or metadata issues.\nSymptoms:\nProduce attempts failed. Topic browse failed or auth errors in logs. Solution:\nCorrectly configure SCRAM authentication in Kafka UI settings. Ensure the user uses SCRAM-SHA-512. 4. ISR and Cluster ID Conflicts Restarting the cluster caused cluster ID mismatch errors.\nSymptoms:\nBroker failed to start or controller election failed. Errors like Cluster ID mismatch or log directory is not empty. Solution:\nTear down volumes completely during reset: docker compose down -v If necessary, manually delete kafka_data_* directories. 5. Missing SASL Config in Producers/Consumers Producers/Consumers were missing SASL configs.\nSymptoms:\nkafka: client has run out of available brokers to talk to Connection attempts ended with EOF. Solution:\nAdd SASL settings in .env and client library configs. (e.g., for Go clients: set Config.Net.SASL.Mechanism properly) Conclusion As security and authentication become essential in Kafka environments, setting up a SASL-authenticated local cluster is highly valuable.\nBy supporting multiple SASL mechanisms within a single cluster, you can replicate production authentication flows without maintaining separate clusters.\nThe Kafka UI was particularly impressive — being able to produce and consume messages via a GUI interface was a huge productivity boost.\n","permalink":"https://dingyu.dev/en/posts/local-sasl-kafka/","summary":"E2E testing is critical for validating production-ready applications. But how do you simulate a secure Kafka environment locally—especially with SASL authentication? This post demonstrates how to spin up a Kafka cluster with both SCRAM-SHA-256 and SCRAM-SHA-512 using Docker Compose.","title":"[EDA] Running a Local Kafka Cluster with SASL SCRAM Authentication (Docker Compose)"},{"content":"Background One of the biggest trade-offs between asynchronous and synchronous programming is that while performance may be improved, it becomes harder to trace.\nThe same is true for event-driven architecture (EDA) using Kafka. It is not easy to track the flow of asynchronously processed data in real-time. In particular, issues often go unnoticed until after they occur, making Kafka monitoring increasingly critical.\nIn event-driven systems, performance monitoring is crucial. In REST API-based architectures, traffic is typically handled using Kubernetes Horizontal Pod Autoscaler (HPA) based on CPU/memory usage and request rates. However, in Kafka-based architectures, the key performance factors are the number of partitions and the processing speed of consumers.\nWhat if there are too few partitions or slow consumers causing lag? → Developers must manually analyze performance and perform partition scaling or consumer scale-out.\nTo detect and address such issues in advance, a Kafka monitoring system was built.\nWe considered three different options for monitoring Kafka performance:\nDesign Comparison of Kafka Monitoring Solutions AWS CloudWatch Using AWS CloudWatch for Kafka monitoring allows metric collection at the PER_TOPIC_PER_PARTITION level.\nKey Monitoring Metrics Metric (AWS/MSK) Description EstimatedTimeLag Time lag (in seconds) of the partition offset after the consumer group reads data OffsetLag Number of offset lag in the partition after consumption by the consumer group Item Description ✅ Easy setup Integrated with AWS MSK by default ✅ Supports CloudWatch Alarm + SNS Simple alert setup available ❌ Viewable only on AWS Console Hard to integrate with external monitoring tools ❌ Cost concerns Additional fees for topic-level monitoring Helm-based Monitoring on EKS (Internal solution, failed attempt) We attempted Kafka monitoring using an internal Helm chart, but it failed due to MSK and EKS residing in different regions.\nItem Description ✅ Internal system integration Smooth integration with in-house systems ❌ MSK and EKS are in different regions Integration not possible ❌ Cost if EKS is redeployed in MSK region Additional expenses may occur Ultimately, this approach was abandoned.\nEC2-based Docker Compose Monitoring (Final Choice) Eventually, we opted to deploy EC2 in the same VPC as MSK and build the Kafka monitoring stack manually.\nUsed JMX Exporter \u0026amp; Node Exporter for Kafka metric collection Used Burrow to monitor consumer lag Enabled long-term monitoring via Thanos and Prometheus Item Description ✅ Cost-efficient Can run on low-cost T-series EC2 instances ✅ Scalable Easily customizable and extensible ✅ Fine-grained Kafka monitoring Enables detailed tracking via Burrow ❌ Initial setup burden Manual configuration with potential trial-and-error ❌ Lack of Burrow \u0026amp; Thanos ops experience Required team to learn and operate monitoring stack from scratch Surprisingly, starting from scratch became a benefit, so we decided to build EC2-based monitoring ourselves.\nArchitecture Overview MSK Terminology zookeeper: Stores Kafka metadata and manages Kafka states broker: The server/node where Kafka runs JMX Exporter: Exposes various metrics from Apache Kafka (brokers, producers, consumers) for monitoring via JMX Node Exporter: Exposes CPU and disk metrics Kafka Monitoring Architecture Evolution: Integrating Prometheus, Thanos, and Burrow Our monitoring architecture evolved from standalone Prometheus (V1), to Thanos integration (V2), to including Kafka consumer monitoring with Burrow (V3).\nEach version is compared below with its respective pros and cons.\nV1: Standalone Prometheus Metric Collection Used Prometheus to gather Kafka metrics and added both CloudWatch and Prometheus as data sources in Grafana for monitoring.\nArchitecture Diagram Pros Simple setup using only Prometheus Easy to compare metrics between Prometheus and CloudWatch in Grafana Cons If Prometheus goes down, the entire monitoring stack becomes unavailable → SPOF risk Prometheus stores all metrics in-memory (TSDB), increasing memory usage with metric volume Large TSDB size can degrade Prometheus performance V2: Prometheus + Thanos Sidecar for HA To ensure high availability, we ran two Prometheus instances and added Thanos Sidecars to connect them to a central Thanos Query + Store server.\nArchitecture Diagram Why Thanos? Avoid metric duplication: Thanos Query deduplicates metrics collected from multiple Prometheus instances Separate short-term (Prometheus) and long-term (Thanos Store) storage: Enables restoration from S3 Reduce TSDB size: Lowers memory usage and cost Pros HA support: Continues monitoring even if one Prometheus fails Long-term metric storage: Offload to S3/GCS Resource efficiency: Reduces Prometheus memory usage Cons Increased operational complexity: Requires Sidecar, Query, and Object Storage (S3) configuration Performance limits of Thanos Query: Heavy queries may impact performance V3: Burrow + Prometheus + Thanos Sidecar for Kafka Consumer Monitoring To cut costs and enhance Kafka Consumer Lag monitoring, we replaced CloudWatch with Burrow for metric collection.\nNow the monitoring stack is fully built with Burrow + Prometheus + Thanos, removing CloudWatch.\nArchitecture Diagram Burrow collects Kafka consumer metrics periodically, Prometheus scrapes those metrics, and Thanos Query gathers them from Sidecar.\nWhy Burrow? Cost savings: Eliminates CloudWatch fees Detailed Consumer Lag visibility: Real-time offset tracking for Kafka consumer groups ACL-aware monitoring: Burrow provides fine-grained lag info per topic and group, unlike CloudWatch Pros Reduces CloudWatch cost Detailed insights into Consumer Group lag and partition status Integrates well with existing Prometheus + Thanos stack Cons Setup overhead: Initial integration with Kafka clusters and ACL permissions needed Limited alerting in Burrow: Works best when combined with Prometheus Alertmanager or Grafana Alerta Version Comparison Summary Architecture Pros Cons Verdict V1: Prometheus only Easy and fast setup Lacks HA, high memory usage Inefficient due to SPOF V2: Prometheus + Thanos HA, long-term storage, memory optimization More complex setup Good for scalable systems V3: Burrow + Prometheus + Thanos Reduces cost, adds detailed monitoring, integrates well Needs setup, weak built-in alerts ✅ Final choice To monitor Kafka effectively, a combination of Burrow + Prometheus + Thanos was the optimal solution for comprehensive Kafka consumer lag visibility.\nImplementation Burrow Kafka Consumer Lag Monitoring with Burrow While Kafka clients can expose records-lag-max using the metrics() method, this only reflects the lag of the slowest partition, making it difficult to get a full picture of the consumer’s performance. Additionally, if the consumer stops, lag will no longer be measured, requiring an external monitoring system. A representative solution is LinkedIn’s Burrow.\nBurrow: Kafka Consumer Monitoring Reinvented\nConsumer A: Lag is consistently decreasing → Healthy Consumer B: Lag temporarily spiked but recovered → Healthy Consumer C: Lag remains constant → Healthy Consumer D: Lag temporarily increased but recovered → Healthy traffic pattern The Problem with Lag Thresholds Threshold-based detection is prone to false positives. For example, if a threshold of 250 is set, consumers B and D, which are behaving normally, could be incorrectly flagged as unhealthy.\n⚠️ You cannot determine the health of a Kafka consumer solely based on MaxLag!\nHow Burrow Solves This Burrow reads the internal Kafka topic where consumer offsets are committed and evaluates the state of each consumer independently. It’s not dependent on any specific consumer, and automatically monitors all consumers to enable objective status analysis.\nHow Burrow Works Burrow uses a sliding window technique to analyze the last N offset commits. LinkedIn recommends using 10 commits (approx. 10 minutes) to evaluate the following:\nIs the consumer committing offsets regularly? Are the offsets increasing? Is lag increasing? Is there a sustained pattern of increasing lag? Based on this, Burrow categorizes the consumer’s status as:\n✅ OK: Operating normally ⚠️ Warning: Lag is increasing ❌ Error: Consumer has stopped or stalled Burrow detects anomalies through pattern analysis rather than thresholds, and exposes this information via HTTP API and alerting integrations.\nExample Burrow API GET /v2/kafka/local/consumer/dingyu/status Returns the current state of a consumer and details about affected topics and partitions.\nIntegration Guide 1. Prerequisites OS: Amazon Linux 2 or Ubuntu 20.04+ Install Docker and Docker Compose Open these ports in the EC2 security group: Prometheus: 9090 Thanos Sidecar: 10901, 10902 Burrow: 8000 2. Install Packages # Install Docker sudo yum install -y docker sudo systemctl enable docker --now # Install Docker Compose sudo curl -L \u0026#34;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 3. Folder Structure MSK-MONITORING/ ├── templates/ # Configuration templates │ ├── burrow.tmpl.toml # Burrow config template │ ├── prometheus.tmpl.yaml # Prometheus config template │ ├── targets.tmpl.json # Prometheus targets ├── deploy.sh # Deployment script ├── docker-compose.yaml # Docker Compose file ├── Makefile # Build and render utility ├── README.md # Project documentation 4. Core Components 4.1 Burrow Monitors Kafka consumer states Configured using burrow.tmpl.toml with environment variable substitution Connects to MSK with SASL/TLS Exposes status via HTTP Burrow Troubleshooting SASL authentication lacked documentation, resulting in heavy trial and error Even when skipping TLS auth, skip verify had to be explicitly set Required debugging with custom sarama client configuration Burrow supports SCRAM-SHA-512 and SCRAM-SHA-256 mechanisms. Make sure to match the mechanism used by MSK.\n4.2 Prometheus Collects metrics from Kafka and Burrow Config based on prometheus.tmpl.yaml Uses targets.tmpl.json to gather JMX and Node Exporter metrics 4.3 Docker Compose Launches Burrow, Prometheus, and Thanos Sidecar containers Ensures smooth inter-container communication 4.4 Makefile make render: Renders config files with current environment variables into generated/ directory 4.5 Environment Variable Management Create a .env file in the same directory as docker-compose.yaml, for example:\nPROM_CLUSTER={your-cluster-name} PROMETHEUS_PORT=9090 BURROW_PORT=8000 ZOOKEEPER_HOST_1={zookeeper1_endpoint} ZOOKEEPER_HOST_2={zookeeper2_endpoint} ZOOKEEPER_HOST_3={zookeeper3_endpoint} BROKER_HOST_1={broker1_endpoint} BROKER_HOST_2={broker2_endpoint} BROKER_HOST_3={broker3_endpoint} BURROW_USERNAME={user} BURROW_PASSWORD={password} 5. Setup and Launch 5.1 Clone the Project git clone https://github.com/dings-things/msk-monitoring-docker-compose.git cd msk-monitoring-docker-compose 5.2 Configure Environment Variables Create a .env file.\n5.3 Run Deployment Script chmod +x deploy.sh ./deploy.sh 5.4 Manual Startup (optional) make render docker compose up -d Deployment Tips For detailed usage, see the GitHub repo.\nIn production environments, you can use GitLab Snippets to manage environment variables dynamically via API.\nBuilding the Dashboard Here are the key metrics you should monitor:\nStatus per Topic/Partition: Detect anomalies per partition Use burrow_kafka_topic_partition_status Disk Usage: Alert when nearing disk limits Use node_filesystem_avail_bytes vs. size_bytes for disk utilization CPU Usage: Alert on CPU threshold breaches (may require partition scaling) Use node_cpu_seconds_total to measure user vs idle CPU Consumer Group Health: Overall health of consumers (apps) Use burrow_kafka_consumer_status Group/Topic Lag: Lag per topic/group Use burrow_kafka_consumer_partition_lag Lag per Partition: Granular lag analysis Use tabular view of burrow_kafka_consumer_partition_lag Current Offset: Latest committed offset Use burrow_kafka_consumer_status Final Architecture References Burrow Official Docs Prometheus Documentation Thanos for Prometheus Scaling Operating AWS MSK Connect Effectively MSK Monitoring at Yanolja ","permalink":"https://dingyu.dev/en/posts/dance-with-burrow/","summary":"Still monitoring Kafka using only low-level Input/Output Bytes? Let’s build topic-level monitoring using Burrow, JMX, and Node Exporter.","title":"[EDA] Kafka (MSK) Monitoring with Burrow Prometheus And Thanos"},{"content":"Introduction In Event-Driven Architecture (EDA) that aims for loose coupling and high scalability, paradoxically, event schemas create a strong contract between Producers and Consumers. Why do we use schemas and Schema Registry in the first place? Purpose of Event Schema Defines the structure of data, standardizes message formats, and ensures data consistency between Producers and Consumers. Maintains compatibility between Producers and Consumers. Enables data validation. Consider the familiar REST API as an analogy!\nWhen services communicate, they agree on an interface — typically documented using OpenAPI or similar specifications — describing required input and expected output.\nThe same applies to event streams:\nWhen a Producer publishes a pre-defined event, the Consumer processes it based on the agreed schema.\nExample 1 — Suppose a schema was agreed as follows:\n{ \u0026#34;user_id\u0026#34;: number, \u0026#34;user_action\u0026#34;: \u0026#34;string\u0026#34; } However, the Producer mistakenly emits user_action as a code rather than a string:\n(This triggers the Consumer\u0026rsquo;s rage 🤢)\nJust like you wouldn\u0026rsquo;t insert records into a database without designing an ERD, schema design is mandatory in EDA.\nEvent Schema Formats Choosing a schema format? Here\u0026rsquo;s a comparison:\nFormat Advantages Disadvantages JSON (JavaScript Object Notation) Human-readable text format. Widely supported across languages. Large message size. No enforced schema, risking data integrity. Protobuf (Protocol Buffers) Compact binary format by Google. Strong typing with enforced schemas. Faster parsing, smaller messages compared to JSON. Not human-readable. Requires predefined schemas. Smaller ecosystem than JSON. Avro Schema-based, compact binary format. Supports Schema Evolution without breaking existing programs. Less popular than JSON/Protobuf. Tooling/library support may be limited. Since event streams involve network transmission, data size matters.\nIf you\u0026rsquo;re used to writing API specs, JSON might feel natural — but should you really default to it?\nChecklist for Choosing JSON (\u0026ldquo;Die-Hard JSON Fan\u0026rdquo;) If you answer \u0026ldquo;yes\u0026rdquo; to all below, JSON could still be a good fit:\nIs the data size small? Can you tolerate serialization/deserialization overhead? Is strong type validation unnecessary? No plans to use Schema Registry? No expected major growth in data volume? Need for human-readable debugging? Advantages of Avro / Protobuf Strong Type Validation Serialization fails if fields do not match specified types (e.g., ENUM, float). High Serialization/Deserialization Performance Binary formats like Avro and Protobuf require no parsing, unlike text-based JSON. Schema Evolution Support New fields can be ignored by older consumers without issues. Benchmark: Even for small payloads, JSON lags behind Avro/Protobuf by more than 2x in performance.\nAnd the performance gap widens as payload size grows.\nImpact of Data Size on Performance Since JSON is stored as text instead of binary, it consumes more space:\nLarger Kafka volumes Degraded produce/consume performance Larger data costs more during ISR (in-sync replication) Larger payloads cause slower fetches during consumption Schema Registry Summarizing the core benefits of event schemas:\nConsistency Performance Compatibility A Schema Registry centrally manages and validates Kafka message schemas, ensuring compatibility between producers and consumers.\nSchema Evolution and Compatibility Example 2: Think of moving from v1 API to v2 API.\nKeep backward compatibility initially. Gradually migrate clients. Eventually deprecate the old API. In event streams, updating schemas means notifying Consumers about changes. But which should update first, Producer or Consumer?\nFortunately, Schema Registry solves this:\nProducer publishes events using the new v2 schema. Consumer detects schema changes and fetches the new version dynamically. Thus enabling smooth schema evolution without service disruptions.\nEfficient Event Management Is using a Schema Registry mandatory?\nNo. Choosing the right tool depends on your needs.\nSchema Awareness in Kafka Streams For Avro/Protobuf (binary formats), schemas are essential because raw binary data isn\u0026rsquo;t self-describing:\nEvent Size Considerations While Avro/Protobuf compress data well, embedding full schema info in each event would negate their size advantage:\nUsing schema IDs instead (with Schema Registry) minimizes event size while preserving compatibility.\nNote:\nIf both Producer and Consumer share identical .proto files, they can theoretically skip embedding schemas. But this approach has downsides: Tight coupling between producer/consumer. No dynamic schema updates. Requires redeploying both producer and consumer on schema changes. AWS Glue vs. Confluent Schema Registry Feature AWS Glue Schema Registry Confluent Schema Registry Schema Updates Adds as new version Adds as new version URL Stability ✅ (ARN-based) ✅ (REST API-based) Auto-use of Latest Version ❌ (Needs config) ✅ (Automatic) Kafka Compatibility ✅ (Works with AWS MSK) ✅ (Works with Confluent Kafka) Why Use Schema Registry? Maintains Data Consistency Ensures producer messages match consumer expectations. Prevents business logic errors. Supports Schema Evolution Add/change fields without breaking existing consumers. Centralized Schema Management No need for manual schema file syncing. Minimizes Kafka Message Size Sends lightweight schema IDs instead of full schemas. Provides Schema Validation Catches invalid payloads early. Enables Real-time Schema Updates Consumers fetch updated schemas dynamically. Configurable Compatibility Modes Prevents breaking changes. Versioned Schema History and Easy Rollbacks Retrieve or roll back to any historical schema version. References Confluent Oliveyoung Tech Blog Exploring Data Serialization in Apache Kafka ","permalink":"https://dingyu.dev/en/posts/schema-registry/","summary":"How can we ensure backward and forward compatibility for event schemas?","title":"[EDA] Schema Registry"},{"content":"class Me: def __init__(self): self.name = \u0026#34;Jung Woo Lee\u0026#34; self.born_year = 1996 self.MBTI = \u0026#34;ENTP\u0026#34; self.location = \u0026#34;Seoul, Korea\u0026#34; self.school = \u0026#34;Tsinghua University\u0026#34; self.major = \u0026#34;automation\u0026#34; self.interests = [\u0026#34;BE\u0026#34;, \u0026#34;DevOps\u0026#34;, \u0026#34;MLOps\u0026#34;, \u0026#34;Go\u0026#34;, \u0026#34;Kafka\u0026#34;, \u0026#34;TDD\u0026#34;, \u0026#34;Automation\u0026#34;] ","permalink":"https://dingyu.dev/en/about/","summary":"\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eMe\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"fm\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ename\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Jung Woo Lee\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eborn_year\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mi\"\u003e1996\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eMBTI\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;ENTP\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elocation\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Seoul, Korea\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eschool\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Tsinghua University\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emajor\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;automation\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003einterests\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;BE\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;DevOps\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;MLOps\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Go\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Kafka\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;TDD\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Automation\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e]\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"About"},{"content":"What is RPC? RPC stands for Remote Procedure Call. It\u0026rsquo;s an inter-process communication technology that allows functions or procedures to run in another address space without requiring separate remote control logic.\nIn essence, RPC abstracts the underlying connection and data transfer so that developers can invoke remote functions as if they were local.\nIn distributed services (especially MSA), RPC is commonly used for communication between clients and servers or between servers.\nAs applications are split into microservices, the days of simply calling functions locally in a monolith are gone. Now, services must communicate over the network.\nNetwork latency becomes the bottleneck, not your optimized code.\nHow RPC Works Define the interface using IDL (Interface Definition Language). The IDL definition is used to generate client stubs and server skeletons via a compiler (rpcgen). The client calls a stub method which marshals parameters. The stub sends the request in a form the server can understand. The server receives the call and executes the logic. The server sends back the response. Stub?\nA stub is an abstraction layer that lets clients call remote methods as if they were local. It handles marshaling/unmarshaling and networking logic internally.\nWhat is gRPC? gRPC is a modern RPC framework developed by Google. It uses Protocol Buffers for IDL and message serialization/deserialization. Clients can call server methods on remote machines as if they were local. RPC != gRPC. gRPC is a specific implementation of RPC, using HTTP/2 and Protocol Buffers internally.\nHTTP/1.1 vs HTTP/2 HTTP/1.1 Flow TCP handshake Request/response TCP termination Even with Keep-Alive, HTTP/1.1 can only handle one request per connection (sequentially).\nProblems Overhead from many TCP connections HOL Blocking: one slow request blocks the rest HTTP/2 Improvements Proxy + Keep-Alive Problems Proxies may drop Keep-Alive connections without notifying clients. Clients think the connection is still valid, leading to errors. HTTP/2 Fixes Servers send GOAWAY frames to indicate shutdown Proxies send PING to verify liveness Clients reconnect when needed Multiplexing Multiple requests over a single TCP connection Requests are independent and don’t block each other No More HOL Blocking Advantages and Disadvantages of gRPC Feature Pros Cons Performance Multiplexing, header compression, streaming Requires HTTP/2 support IDL Strong typing with .proto files ProtoBuf is not human-readable Communication Unary, Server/Client Streaming, Bidirectional Complex to debug Language Support Multi-language support (Go, Java, Python, etc.) Varies in maturity Code Generation Auto-generates stubs from .proto Requires additional build tooling Security Strong TLS support TLS setup can be tricky Load Balancing Client-side support available Typically used with Envoy or other proxies Streaming Built-in streaming APIs Harder to implement and maintain Transport Binary serialization = compact, fast Not easily debuggable like REST Browser Supported via gRPC-Web Requires gRPC-Web proxy layer More on gRPC performance, debugging, and implementation tips coming next.\n","permalink":"https://dingyu.dev/en/posts/grpc/","summary":"Whenever we hear about RPC communication (or maybe it’s just me?), we tend to assume it involves Protocol Buffers over HTTP/2 — that is, gRPC. But what exactly is RPC? Why is gRPC so fast? And when should we use it?","title":"[Protocol] Exploring RPC... and GRPC in Depth"},{"content":"Background Inconsistent project structures per service made code hard to follow. No shared conventions made common module/CI reuse difficult, reducing productivity. Onboarding was challenging, especially for Go newcomers. Our team has now matured in using Go and has built internal best practices. We want to establish conventions for better maintainability and clarity across services. Proposed Project Structure Sections marked with * are mandatory. This structure assumes MSA-style projects rather than large monoliths.\nExample Structure (Domain: match sampling) . ├── *docs │ ├── *swagger.yaml │ ├── sequence.md │ └── architecture.md ├── *cmd │ └── *main.go ├── pkg │ ├── file_parser.go │ └── time_convertor.go └── *internal ├── *handler │ ├── *v1/sampling_handler.go │ ├── v2/sampling_handler.go │ ├── server.go │ ├── health_handler.go │ ├── swagger_handler.go │ └── auth_middleware.go ├── data │ ├── mysqldb.go │ ├── redis.go │ ├── feature_event_producer.go │ ├── match_repository.go │ └── nass_api.go ├── *service │ ├── kda_sampler.go │ ├── match_sampling_usecase.go │ └── kda_sampler_test.go ├── logger.go ├── constants.go └── *config.go ├── *gitlab-ci.yml ├── *go.mod ├── *go.sum └── *README.md Section Required Description docs ✔ Project-level diagrams and specs cmd ✔ Entry point for DI \u0026amp; execution pkg optional Utility modules safe for external reuse internal ✔ Core domain logic, hidden from outside handler ✔ HTTP/gRPC/Kafka handlers (versioned) data optional Database, external API, Kafka interactions service ✔ Business logic per SRP, unit tested root files ✔ CI, readme, mod/sum files Constant Convention Use PascalCase for constants shared across packages. Use camelCase for internal/private constants. If a private constant must be exposed, wrap it via a public method. Data Model Convention Define models close to their use (not in shared folders). Use DTOs between layers, and convert as needed. For 2+ arguments, use structs. Keep validation logic in methods for readability and testability. // AS-IS if strings.HasPrefix(match.Version, \u0026#34;rc\u0026#34;) \u0026amp;\u0026amp; match.detail == \u0026#34;test\u0026#34; { ... } // TO-BE if match.IsTest() { ... } Test Conventions Testing business logic is mandatory, not optional. You should write test cases for already-defined errors in a concise yet detailed manner.\nDeterministic Asynchronous Unit Testing Avoid relying on time.Sleep or blindly logging after async execution without assertions.\nPrevent flaky tests by leveraging Dependency Injection (DI) and assert.Eventually.\n1. Injecting a Logger // NewQueue creates a Queue instance responsible for business logic func NewQueue( config Config, httpClient *http.Client, logger *zerolog.Logger, ) (queue Queue, err error) { // The queue will execute the thread executor when Start() is called. queue = Queue{ config: config, client: httpClient, logger: logger, quitChan: make(chan struct{}), } return } 2. Testing Output Test case for queue failure logging t.Run(\u0026#34;Logs failure correctly when queue processing fails\u0026#34;, func(t *testing.T) { // given var buffer bytes.Buffer ... inject logger with buffer as output // when ... execute async task event1, err := queue.Push([]byte(validJSON1)) assert.NoError(t, err) event2, err := queue.Push([]byte(validJSON2)) assert.NoError(t, err) // then assert.Eventually(t, func() bool { output := buffer.String() return strings.Contains(output, event1.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, event2.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, `\u0026#34;success\u0026#34;:false`) }, 1*time.Second, 10*time.Millisecond) }) Test case for queue success logging t.Run(\u0026#34;Logs success correctly when queue processes successfully\u0026#34;, func(t *testing.T) { // given var buffer bytes.Buffer ... inject logger with buffer as output // when ... execute async task event1, err := queue.Push([]byte(validJSON1)) assert.NoError(t, err) event2, err := queue.Push([]byte(validJSON2)) assert.NoError(t, err) // then assert.Eventually(t, func() bool { output := buffer.String() return strings.Contains(output, event1.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, event2.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, `\u0026#34;success\u0026#34;:true`) }, 1*time.Second, 10*time.Millisecond) }) ","permalink":"https://dingyu.dev/en/posts/go-convention/","summary":"Efficient Go Project Structure Guide","title":"[Go] Go Convention"},{"content":"Why Loki? In backend applications, we often used Elasticsearch queries to investigate user issues and visualized dashboards using Kibana for operational teams.\nIf asked, \u0026ldquo;Why switch to Loki?\u0026rdquo; the answer is simple: \u0026ldquo;To reduce cost!\u0026rdquo;\nCategory Loki 🟢 Elasticsearch 🔴 Storage Model Stores metadata only; raw logs in object storage Indexes and stores all logs Search Method Label-based search Full-text search Indexing Cost Low High (CPU/memory intensive) Storage Cost Cheap (S3/GCS object storage) Expensive (dedicated nodes needed) Performance Efficient for massive log storage Fast for query Scalability Easy to scale with simple configuration Cluster scaling is more complex Ops Overhead Low (no need to manage cluster) High (cluster management required) Use Case Simple log storage and retrieval Complex analysis and search Pre-Migration Checklist No need to expose dashboards to external stakeholders? Do you only need basic log viewing during incidents? Notes When Integrating Loki When labels are too diverse or contain too many unique values, Loki\u0026rsquo;s indexing becomes inefficient.\nExample: assigning fields like user_id or timestamp as labels can cause a rapid index size increase. Consequences:\nSlow query performance High memory usage Increased storage cost Use Fixed, Low-Cardinality Labels To keep cardinality low:\nUse static or limited-range values as labels Examples: Good 😊: region=\u0026quot;us-east-1\u0026quot;, app=\u0026quot;payment-service\u0026quot; Bad 😥: user_id=\u0026quot;12345\u0026quot;, request_id=\u0026quot;abcd-efgh\u0026quot; Only Use Labels Relevant to Filtering Design labels based on how you plan to filter/search logs in dashboards:\nUse labels only for data that will be filtered or analyzed Avoid labels used solely for debugging Examples: Good 😊: For tracking TPS/latency, design labels around logs that appear once per request → e.g. func=\u0026quot;LogMiddleware.Log\u0026quot; Bad 😥: Using latency itself as a label Separate Log Message from Metadata Labels should serve as tags. Dynamic values should go inside the log message.\nExamples: Good 😊: label:func=\u0026quot;RestrictionsService\u0026quot;, log: member_id=\u0026quot;12341512321\u0026quot;, message=\u0026quot;restricted member\u0026quot; Bad 😥: label:member_id=\u0026quot;12341512321\u0026quot;, log: message=\u0026quot;restricted member\u0026quot;, func=\u0026quot;RestrictionsService\u0026quot; Limit the Number of Labels Too many labels = high cardinality → indexing inefficiency.\nLoki\u0026rsquo;s official recommendation: keep labels under 20. Limit Unique Values per Label Loki recommends fewer than 1,000 unique values per label.\nAcceptable: status=\u0026quot;200\u0026quot;, status=\u0026quot;500\u0026quot; Avoid: user_id=\u0026quot;12345\u0026quot;, session_id=\u0026quot;abcd-efgh\u0026quot; Examples: Good 😊: env=\u0026quot;production\u0026quot;, service=\u0026quot;payments\u0026quot;, region=\u0026quot;us-east-1\u0026quot; Bad 😥: user_id=\u0026quot;12345\u0026quot;, request_id=\u0026quot;xyz-789\u0026quot; Tune Chunk Size and Retention Loki stores logs in chunks and flushes them to object storage periodically.\nToo small chunks → low performance Too large chunks → slow search Recommended: chunk_encoding: gzip chunk_target_size: 1MB~2MB (adjust as needed) Building the Dashboard 1. Set Up Variables While filtering with Variables adds complexity, it’s a gift to your future self.\nApplication logs are usually queried using raw log, so choose frequently used filter labels as variables. Also add text box filters to refine the filtered results.\n2. Build Raw Logs Panel Start with a global log viewer using Logs Visualization.\nExample:\n{_type=\u0026#34;loki\u0026#34;} |= `$filter` 3. Filter Error Logs by Log Level From raw log, duplicate and filter only error level logs.\nSome business logic might bypass error tagging—make sure log levels are properly set.\nExample:\n{level=\u0026#34;error\u0026#34;} 4. Track TPS with Quantiles Precision = speed. Use P50, P99 as standard metrics with Gauge Visualization using quantile_over_time.\nNote:\nUnlike Prometheus TSDB, Loki doesn\u0026rsquo;t compute percentiles efficiently High volume log ranges may degrade performance quantile_over_time is memory intensive and expensive at large time ranges Example:\nquantile_over_time(0.50, {func=\u0026#34;LogMiddleware.Log\u0026#34;} |= `$filter` | json | unwrap latency [$__range]) by (func) 5. Build Distributions by Label Use Piechart to understand overall log distribution.\nLabels must be used, so apply this only to unique label values\nExample:\nsum by(method) (count_over_time({func=\u0026#34;LogMiddleware.Log\u0026#34;} |= `$filter` [$__auto])) 6. Use Table to Visualize and Filter Distributions Table is great for at-a-glance inspection. You can make labels clickable for filtering.\nExample:\nsum by(method) (count_over_time({func=\u0026#34;LogMiddleware.Log\u0026#34;} |= `$filter` [$__auto])) Final Dashboard References 6 easy ways to improve your log dashboards with Grafana and Grafana Loki ","permalink":"https://dingyu.dev/en/posts/es-to-loki/","summary":"Is heavy-duty Elasticsearch necessary just for basic application monitoring? This post explores the transition to Loki and the lessons learned.","title":"[LGTM] Elasticsearch to Loki Migration Story"},{"content":" REFS Rules Based Stream Processing with Apache Flink\u0026rsquo;s Broadcast Pattern Advanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System Build a dynamic rules engine with Amazon Managed Service for Apache Flink Research Background We needed a case study on building a frequency-based detection policy using filtering conditions within a Window Time. Research was necessary on how to handle all policy processing within a single job code. Allocating instances per policy—even when using container virtualization—is resource inefficient. Policies are managed by admins and should not require job redeployment upon updates. Prerequisites Dynamic Partitioning Kafka uses the event key to hash and modulo into partitions.\nIn frameworks like Kafka Streams and Flink, using a non-key field for group by (keyBy) causes reshuffling, which involves cross-network data movement among Task Managers—posing a significant overhead.\nTo solve this, ensure that transactions with the same grouping key are handled by the same subtask.\nExample: Pre-partitioning by target key (e.g., ID or IP) via separate Kafka topics.\nTerminology (Abbreviated explanation of JobManager, TaskManager, SubTask, Broadcast, etc.)\nImplementation Strategy Merge action events with active policy events (via CDC from rule DB) and publish to a topic. If there’s 1 action and N active policies → publish N merged events. Use DynamicKeyFunction to dynamically partition source stream by group-by condition. Handles reshuffling dynamically without job redeployment. Existing keyBy still processed by current TaskSlots. In DynamicEvaluationFunction, evaluate whether each event satisfies a rule → emit restriction event if it does. Broadcast State Broadcast one stream to all parallel subtasks so they can share consistent configuration/rules.\nTypical use case: low-throughput control/config stream broadcasted to high-throughput action stream.\nBroadcast Architecture Source stream: Payment events Broadcast stream: Policy rules (with infinite retention) Merge both → evaluate Dynamic Data Partitioning Create a system that can add/remove rules at runtime without redeploying jobs.\nStatic vs. Dynamic Keys Type Static Key Dynamic Key Definition Pre-defined field Runtime-decided field Flexibility Low High Implementation Simple Complex (rule parsing required) Performance Optimized Slight overhead Example: If rules are grouped by id, all relevant events will go to the same subtask, even if the logic per rule differs.\nPolicies can share subtasks if their groupingKey is the same.\nRule Broadcasting Use a broadcast source (e.g., from a rule DB CDC topic) to continuously update the active rules.\nEach time a rule is added/modified, it is inserted into the broadcast state.\nIf rule.disabled = true, it is removed.\nCustom Window Processing Flink offers multiple window types: Tumbling, Sliding, Session.\nBut\u0026hellip; each has limitations for fraud detection.\nTumbling: May miss events at window boundaries. Sliding: Has inherent delay and overlapping evaluations. → Solution: Implement Custom Windowing using state and timestamps.\nEvents stored as:\nMapState\u0026lt;Long, Set\u0026lt;PaymentEvent\u0026gt;\u0026gt; windowState; Since the state backend is a key-value store, it doesn\u0026rsquo;t support list types directly. This means we need to iterate over all timestamps in the map to find valid entries\u0026hellip; More research is needed here, but since we’re only iterating timestamps (not full events), memory impact may be minimal — though CPU usage could be a concern depending on loop cost.\nConsiderations on Event Retention (TTL) How should we determine the retention period, i.e., the Time-To-Live (TTL) for each event?\nIn DynamicEvaluationFunction(), it is possible to receive payment events with the same key scope but evaluate them under different rules with different time windows.\nTherefore, at the time a rule is consumed from the Rule Stream (Broadcast Stream), we must update and store the longest rule duration for that key.\nExample: UpdateWidestWindow @Override public void processBroadcastElement(Rule rule, Context ctx, Collector\u0026lt;Alert\u0026gt; out) { ... updateWidestWindowRule(rule, broadcastState); } private void updateWidestWindowRule(Rule rule, BroadcastState\u0026lt;Integer, Rule\u0026gt; broadcastState) { Rule widestWindowRule = broadcastState.get(WIDEST_RULE_KEY); if (widestWindowRule == null) { broadcastState.put(WIDEST_RULE_KEY, rule); return; } if (widestWindowRule.getWindowMillis() \u0026lt; rule.getWindowMillis()) { broadcastState.put(WIDEST_RULE_KEY, rule); } } In summary, Dynamic Evaluation uses the rule with the longest duration to determine the TTL for the event.\nSince the state backend is a key-value store, it doesn\u0026rsquo;t support list types directly.\nThis means we need to iterate over all timestamps in the map to find valid entries…\nMore research is needed here, but since we’re only iterating timestamps (not full events), memory impact may be minimal — though CPU usage could be a concern depending on loop cost.\n","permalink":"https://dingyu.dev/en/posts/flink-dynamic-job/","summary":"Do we really need to redeploy the job every time the policy changes? A deep dive into executing Flink jobs dynamically.","title":"[EDA] Flink Dynamic Job Case Study"},{"content":"MSA Requirements for a Go App Configuration management, graceful shutdown Testable code API specifications Logging Profiling, error monitoring, metrics, API tracing Tiny Main Abstraction Instead of using separate .env files, environment variables are read directly from the OS. A question raised: isn\u0026rsquo;t setting every OS argument as a flag wasteful? Graceful Shutdown Testable Code The speaker waits for the server startup with long polling before running tests—not the cleanest method. Since the speaker is focused on vanilla Go, it seems they deliberately chose not to use the httptest package. Health Check Version is set using ldflags, which could be automatically tagged during release. Storing server start time is preferred over tracking total uptime. Documentation is a Must As Gophers say, \u0026ldquo;GoDoc isn\u0026rsquo;t optional—it’s essential.\u0026rdquo;\nWhile the speaker didn\u0026rsquo;t use godoc, they emphasized exposing OpenAPI specs. go:embed embeds OpenAPI files into the binary at build time. Build fails if the file is missing. Swagger endpoints expose the embedded documentation. Maintaining Swagger alongside code updates may not be sustainable. Using swaggo might be more intuitive.\nLogging is a Must JSON logging with slog is adopted—a must for modern apps.\nFollowing the 12-factor app philosophy, logs are written to stdout. This reduces File I/O costs.\nFluentbit handles post-processing with multiple outputs:\nSentry: Error tracking Jaeger: API tracing Elasticsearch: Log storage for Kibana Decorate Decorates the response writer to track HTTP status codes and byte sizes. Reflections This approach contrasts with rigid project structures. The minimal Go style without strict clean architecture was convincing. Some trade-offs are inevitable. Even when using established frameworks, clean and concise code is still achievable. Fluentbit should be adopted company-wide to reduce log coupling in the app. Company-wide tracing adoption (e.g., Jaeger, OpenTelemetry) is a must, potentially mandated by CTOs. Applying to Production // HealthHandler : Server health status handler type HealthHandler struct { version string startTime time.Time } // NewHealthHandler : Creates a new HealthHandler func NewHealthHandler(version string) HealthHandler { return HealthHandler{ version: version, startTime: time.Now(), } } // Check : Returns server status and build metadata func (h HealthHandler) Check(ctx *gin.Context) { type responseBody struct { Version string `json:\u0026#34;version\u0026#34;` Uptime string `json:\u0026#34;up_time\u0026#34;` LastCommitHash string `json:\u0026#34;last_commit_hash\u0026#34;` LastCommitTime time.Time `json:\u0026#34;last_commit_time\u0026#34;` DirtyBuild bool `json:\u0026#34;dirty_build\u0026#34;` } var ( lastCommitHash string lastCommitTime time.Time dirtyBuild bool ) { buildInfo, _ := debug.ReadBuildInfo() for _, kv := range buildInfo.Settings { if kv.Value == \u0026#34;\u0026#34; { continue } switch kv.Key { case \u0026#34;vcs.revision\u0026#34;: lastCommitHash = kv.Value case \u0026#34;vcs.time\u0026#34;: lastCommitTime, _ = time.Parse(time.RFC3339, kv.Value) case \u0026#34;vcs.modified\u0026#34;: dirtyBuild = kv.Value == \u0026#34;true\u0026#34; } } } up := time.Now() ctx.JSON(http.StatusOK, responseBody{ Version: h.version, Uptime: up.Sub(h.startTime).String(), LastCommitHash: lastCommitHash, LastCommitTime: lastCommitTime, DirtyBuild: dirtyBuild, }) } ","permalink":"https://dingyu.dev/en/posts/gopher-con-2024-minimalistic-go/","summary":"No more unnecessary frameworks—pure Go is the ultimate form of tuning. Learn how to build minimalistic applications with vanilla Go.","title":"[Go] Gophercon 2024 - Building Minimalistic Backend Microservice in Go"},{"content":"[Go] Gophercon 2024 - Go Project Guide A-Z Small Projects Low traffic → Limited user base Simple functionality → Undefined user states in new services Need for rapid development → Early feature experimentation required for hypothesis validation For a Simple CLI Tool For a Simple API Server Take a minimalistic approach using standard libraries to build and expand APIs incrementally.\nFeature-Oriented Project Patterns Using Handler Struct (Class-based DI) Pass required parameters for each method via function inputs:\nInjecting Dependencies via HTTP HandlerFunc Use closures internally to capture dependencies. This design is clean, allows for scoped dependency injection, and is highly testable.\nType Advantages When to Use Handler - Easier management of complex state- Clearly structured dependencies- Easy to extend- Good for interface implementation- Helps organize complex logic Complex business logic, many dependencies, stateful apps HandlerFunc - Quick to write- Simple DI with closures- Functional composition- Ideal for prototyping- Easy mocking for testing Simple handlers, small apps, microservices Code Pattern for Enterprise-Scale Services Apply a Layered Architecture:\nLayer Breakdown Presenter = Converts between Domain Model and API Model Handler = Serves API Models Handles HTTP/gRPC requests and responses Clearly separates presentation from application logic Usecase = Executes business logic Depends on Services or Repositories Avoids using API or DB-specific models, focusing on domain logic (Service) = Implements complex business logic Extracted when Usecases become complex or involve multiple dependencies Usecases orchestrate; Services execute logic (similar to xxxExecutor pattern) Repository = Manages CRUD for domain models Encapsulates data access logic Inverts dependency: Repositories return domain models and depend on domain layer (Recorder) = Manages DB-specific models Used for handling NoSQL/RDBMS-specific logic Enables DB migrations by swapping out recorder DI Test Code per Layer Use the mocking library counterfeiter.\nFeature Go Mockery Counterfeiter Mock Creation Easy auto-generation with args Fake objects allow for advanced simulation Setup Ease Simple and intuitive Ideal for complex test scenarios Test Maintainability Best for simple interfaces Long-term test stability with fake objects Best Use Case Isolated module testing Complex logic and varied response scenarios Fake mocks are usually stored within the same layer they belong to.\nClosing Thoughts While Go\u0026rsquo;s ecosystem has experimented with many structural approaches, no clear standard has emerged. This pattern seems highly practical. Concern: Does a layered architecture cause unnecessary data structure conversions? (e.g., Presentation, Business, Data layers could each use separate structs for a single API) Service vs. Usecase? Seems to hinge on whether multiple services are involved. On APM: What about cost? → APM isn\u0026rsquo;t applied globally. It is only used for business-critical logic. ","permalink":"https://dingyu.dev/en/posts/gopher-con-2024-go-project-guide/","summary":"How to structure projects in Go, a language without rigid frameworks like Spring. This post introduces practical patterns for feature-driven development and enterprise-level application design.","title":"[Go] Gophercon 2024 - Go Project Guide from A to Z"},{"content":"Kubernetes API An interface to interact with the Kubernetes platform. Used by users, administrators, and applications to manage clusters and resources. Enables a series of operations such as listing or creating resources. Supports application deployment and status monitoring. Provided as an HTTP API, compatible with various languages and tools. https://\u0026lt;APISERVER\u0026gt;/\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt;/\u0026lt;NAME\u0026gt;\nAPI SERVER: The address of the API server GROUP: /api: Core API group (e.g., Pod, Service) /apis/*: Extended API group for deployment and management features VERSION: e.g., v1 (stable), v1beta, v1alpha1 RESOURCE: The type of resource (e.g., /pods, /services) \u0026lt;NAMESPACE\u0026gt;/\u0026lt;RESOURCE\u0026gt;: Namespace-specific queries NAME: The specific name of the target resource client-go Library Abstracts Kubernetes API calls into convenient Go functions.\nAccess both core and extended resource groups.\nSupports initialization inside/outside the cluster.\nProvides Informers to cache resource changes.\nQuerying Pods(\u0026quot;\u0026quot;) allows access to all namespaces (within permission scope).\nWhen Updating a Deployment Concurrently The first request that reaches the server is accepted, others fail. Similar to MVCC: version conflict detection prevents lost updates. Simultaneous Image Change from Different Servers One request will succeed; the rest will fail with a Conflict error. Optimistic Concurrency Step 1 Step 2 Step 3 Submit update with resource version Apply if version matches stored version Fail on version mismatch Use retry.RetryOnConflict() to automatically retry on version conflicts.\nControllers and Operators in Kubernetes Controller:\nManages native Kubernetes resources (Pods, Deployments, Services). Operator:\nManages complex state via custom resources. Automates application deployment and management. kubebuilder Framework Go-based framework to develop Kubernetes controllers/operators. Generates boilerplate for CRD management and reconciliation. Helps focus only on core business logic. Alternative to client-go (which is more complex). Component Overview:\nProcess: Application configuration/init\nManager: API server communication, event caching, metrics\nWebhook: Initialization and validation logic\nController: Filters resource events and triggers Reconciler\nReconciler: Implements logic for managing custom resources\nSPEC: Desired state definition\nSTATUS: Current state reporting\nController Option Configuration Settings for the Reconcile() function triggered on resource changes.\nIncludes filters, performance settings, resource scopes.\nGeneration field can be used to only trigger reconciliation on spec changes.\nConfigure max concurrent reconciliations via controller options.\nWhen is Reconcile() Triggered? On operator startup (traverses all resources) On resource updates Default: any field change Customizable to spec-only changes Takeaways While CPU and memory-based autoscaling is standard, customizing based on application nature can improve performance. e.g., for queue-based workers, scale via queue length. Conflict errors during concurrent deployment can be mitigated using custom controllers. Instead of manually triggering deployment via Rundeck, a DaemonSet-based operator could handle automation (with caution). ","permalink":"https://dingyu.dev/en/posts/gopher-con-2024-kubernetes-programing/","summary":"How to use the Kubernetes API with Go… and deploy applications using Operators","title":"[Go] Gophercon 2024 - Kubernetes Platform Programming"},{"content":"Background Service server refers to a server performing business-specific operations.\n[AS-IS] The service server was publishing individual HTTP events for each occurrence.\nAlthough asynchronous HTTP requests were used, the batch loader\u0026rsquo;s response time increased linearly with the number of HTTP requests.\nProblems Excessive Network Traffic: Every event triggered an HTTP request, overwhelming the network as event frequency increased. Increased Latency: More requests led to queuing and response delays. Lack of Scalability: Each service implemented batch loading differently. Moving to event streaming would require repetitive changes across servers. [TO-BE] Solutions Reduce Network Traffic: Buffer events internally and batch-send them based on buffer limit or interval. Reduce Latency: Gathered events are sent at fixed intervals, solving the queuing delay. Increase Scalability: Use a shared library to minimize repetitive changes and maintain loose coupling with the batch loader server. Solution A shared library was designed, called \u0026ldquo;Batch Processor\u0026rdquo;, to buffer and batch-send events.\nRequirements Minimize IO-bound tasks. Control goroutine lifecycle explicitly. Option 1: Worker Pool Multiple workers buffer internally and synchronously send batched events.\nPros No deep copy overhead. Performance tuning possible by adjusting worker pool size. Cons Up to N HTTP requests every interval (where N = number of workers). Tuning required to find the optimal \u0026ldquo;magic number\u0026rdquo; of workers. Option 2: Single Worker + Async HTTP Pros Only one HTTP request per interval. No worker tuning necessary. Cons Potential CPU and memory load from deep copies. Memory overhead may trigger GC, leading to \u0026ldquo;Stop the World\u0026rdquo; delays. Option 2 was selected due to superior usability despite potential deep copy overheads.\nProfiling Goals Throughput: Handle 2000 RPS for 1 minute. Memory Usage: Measure overhead from deepCopy(). GC Overhead: Check GC impact during memory copy. func deepCopy[T any](src []T) []T { if src == nil { return nil } dst := make([]T, len(src)) copy(dst, src) return dst } Method Compare 10/100 Worker Pool + Sync IO vs 1 Worker + Async IO.\nCPU profiling enabled. Count processed events by logging inside the API send function. func (q *BatchProcessor) send(payload []byte, traceIDs []string) { response, err := q.client.Do(request) q.logger.Info().Int(\u0026#34;count\u0026#34;, len(traceIDs)).Send() } CPU Profile selectgo: Go runtime\u0026rsquo;s internal event selection from multiple channels.\n100 Worker Pool + Sync IO runWithSync() analysis: Overhead from channel locking (sellock) and scheduling (acquireSudog) 85%. 10 Worker Pool + Sync IO Overhead reduced to 66%. 1 Worker + Async IO deepCopy() overhead (runtime.memmove) only 10ms. Runtime and deep copy overhead 50%. Heap Profile Focused on deep copy impact during execution.\nWorker Pool No measurable deep copy overhead. 7.7MB out of 8.2MB used for HTTP requests. Single Worker + Async IO 11.4MB out of 12.21MB used for HTTP requests. Deep copy overhead is 150kB (1.22%), negligible. Test Results Setup Throughput (per minute) CPU Overhead Memory Overhead 10 Worker 83,663 66% 0% 100 Worker 84,042 85% 0% 1 Worker + Async IO 119,720 50% 1.22% Summary Worker Pool introduces significant synchronization overhead. Increasing workers does not linearly increase throughput. Async IO is significantly more efficient for IO-bound tasks. Worker Pools are better for CPU-bound tasks or when strict request ordering is needed. pprof integration:\nFor pprof-based GC tuning, refer to pprof tuning article.\n","permalink":"https://dingyu.dev/en/posts/worker-pool-async/","summary":"Is it better to distribute tasks across multiple workers or handle each task asynchronously? Let’s find out through performance profiling.","title":"[Go] Profiling Worker Pool vs. Async Processing"},{"content":" Source Code: https://github.com/dings-things/coffee_pal\nPurpose Great cultures make great organizations — not just getting things done, but doing them well. Good culture enhances productivity and quality alike.\nI created “CoffeePal” with the idea of helping people stuck on problems or simply wanting a quick small talk boost. The name reflects its purpose: your “coffee pal” during the workday.\nBefore Development The app needed to be easy and fast, seamlessly matching users via our internal Slack tool.\nTo understand the interaction flow, I started with Slack Workflow.\nCreate a Trigger Point\nFor testing, I used a :coffee: emoji reaction to initiate the workflow.\nActivate Workflow\nA message is sent to the reacting user to confirm whether they want to start a coffee chat.\nCollect User Information via Form\nFor personalization, users can optionally input their MBTI and birthday.\nSet Target, Time, and Topic\nDefine the coffee partner, schedule, and discussion topic ahead of time.\nSend Invitation Message to Partner\nUsing the collected data, send an invitation message to the designated partner.\nI used Slack Workflow for the first time and found the UI clean and intuitive.\nHowever, it felt more like a one-way communication than true interaction. So, I decided to expand it into a full Slack app.\nIt felt like being force-fed food I didn’t want\u0026hellip;\nRequirements Match users based on pre-saved profile data\nTrigger matching after personal data is submitted Secure compute resources to handle app interactions\nInternal Server Firewall complexity Fast response via socket communication AWS Lambda Free under Free Tier Serverless Can connect with API Gateway Consider future cost after Free Tier Intuitive UI for users at a glance\nUse Slack Block Kit Implementation Using Lambda Create Slack App Set up Lambda Configure Socket Mode Step 1: Create Slack App Go to Slack API and create a new app\nStep 2: Click From scratch Step 3: Choose workspace and name Step 4: Set bot for user interaction Step 5: Enable events Lambda Step 1: Go to Lambda Console Lambda Step 2: Configure function, runtime, architecture Lambda Step 3: Set up API Gateway API Gateway routes HTTP requests to Lambda — essential to integrate Slack Events API with Lambda.\nBut then I asked myself\u0026hellip; If we already have a server, why use Lambda? Wouldn’t not using it be wasteful?\nSo I decided to use our spare dev server and implement it via socket mode.\nSocket Mode Configuration What is Socket Mode?\nInstead of sending payloads to public HTTP endpoints, Slack uses a WebSocket connection to deliver app interactions and events.\nAvoiding public endpoints means better security, and you don\u0026rsquo;t need to subscribe to each event individually.\nStep 1: Generate App-Level Token Grant permissions to a global token for Slack app control inside the server.\nStep 2: Set Token Scope channels:read: Get user info from channels chat:write: Post messages to channels chat:write.public: Write to channels even if not invited groups:read: Access private channel info groups:write: Write in private channels im:write: Send DMs mpim:write: Send group DMs reminders:write: Set reminders Development Slack Bolt A framework for building Slack apps, supporting languages like Node.js, Python, and JavaScript.\nWhile you might consider skipping it… using Bolt avoids a lot of redundant work. Skipping it results in slower development and difficult maintenance.\nWithout Bolt: from slack_sdk import WebClient from slack_sdk.errors import SlackApiError import os, json, base64, urllib.parse def lambda_handler(event, context): bot_token = os.getenv(\u0026#39;BOT_KEY\u0026#39;) client = WebClient(token=bot_token) if event.get(\u0026#39;isBase64Encoded\u0026#39;, False): decoded_body = base64.b64decode(event[\u0026#39;body\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;) event_body = urllib.parse.unquote_plus(decoded_body)[8:] else: event_body = event[\u0026#39;body\u0026#39;] try: event_data = json.loads(event_body) except json.JSONDecodeError: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;message\u0026#39;: \u0026#39;Invalid request\u0026#39;}) } event_type = event_data.get(\u0026#39;event\u0026#39;, {}).get(\u0026#39;type\u0026#39;) # handle event_type... With Bolt: app = App(token=settings.SLACK_BOT_TOKEN) @app.event(\u0026#34;app_home_opened\u0026#34;) def handle_home(event, client: WebClient = None): ... @app.action(\u0026#34;suggest_coffee_chat_button\u0026#34;) def handle_chat_button(ack, body, client: WebClient = None): ... Bolt simplifies:\nAuth Routing by event/action/view Middleware chaining Block Kit Block Kit is at the heart of Slack app UI design.\nEach visual unit is a \u0026ldquo;block\u0026rdquo;. You compose a full view by stacking them.\nHOME_VIEW = { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;blocks\u0026#34;: [ ... ] } HOME layout Horizontal Section Vertical Section Input Slack API is stateless. Every request is isolated, so inputs are critical for maintaining interaction context.\nEven if the server disconnects, the state can be restored based on input fields.\nRandom Coffee Chat example Improvements Use Slack DataStore for stateful user data with TTLs Integrate HR info (rank/age/role) to personalize matches Improve UI for clarity and friendliness Add CI/CD and infra for production usage Final Thoughts As a developer, I resonate with the Ubuntu philosophy:\n“I am because we are.”\nFostering collaboration is just as important as individual contribution.\nThat’s why I built CoffeePal ☕\nIf you hit technical roadblocks or need an ice-breaker for teamwork, consider building your own Slack bot!\n","permalink":"https://dingyu.dev/en/posts/coffee-pal/","summary":"I’m a developer, chatting on Slack with the person next to me… Let’s become friends!","title":"[DX] Building a Slack Bot for Internal Coffee Chats"},{"content":"Applying pprof import \u0026#34;github.com/gin-contrib/pprof\u0026#34; // initialize server var ( router *gin.Engine ) { router = gin.New() // register pprof routes pprof.Register(router, APIPrefix+\u0026#34;/debug/pprof\u0026#34;) } Script echo \u0026#34;User API v1.2.3\u0026#34; # start profiling curl https://{endpoint}/debug/pprof/trace?seconds=10 --output v1.2.3-trace.out \u0026amp; curl https://{endpoint}/debug/pprof/heap?seconds=10 --output v1.2.3-heap.prof \u0026amp; curl https://{endpoint}/debug/pprof/profile?seconds=10 --output v1.2.3-cpu.prof \u0026amp; # Run load test while profiling bombardier -t 1s -l -c 30 -d 10s \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -m GET https://{endpoint}/v1/users/51387659 wait # view pprof output locally go tool trace -http 127.0.0.1:9094 v1.2.3-trace.out \u0026amp; go tool pprof -http 127.0.0.1:9095 v1.2.3-heap.prof \u0026amp; go tool pprof -http 127.0.0.1:9096 v1.2.3-cpu.prof \u0026amp; Note: Bash \u0026amp; runs commands asynchronously.\npprof Usage Steps Start the application with pprof routes registered. If the app isn’t under load, run a load test on a target API. Use curl to trigger profiling. If you’re behind a Load Balancer, ensure profiling completes within the LB timeout.\nHeap Profiling Metrics inuse_objects: number of currently used objects inuse_space: amount of memory currently in use alloc_objects: total number of objects ever allocated alloc_space: total memory ever allocated Notes on Inuse vs Alloc Same Inuse/Alloc (e.g. 1GB/1GB): object was allocated once and reused — no GC triggered, CPU efficient. Alloc much larger than Inuse (e.g. 1TB/1GB): indicates high churn and frequent GC — may cause Stop-The-World (STW) pauses. Tuning Opportunities Flame Graph Flame Graphs help distinguish between user-defined logic and framework/internal code like ServeHTTP.\nThe wider the bar, the more allocations are made.\nContrary to expectations, Redis accounted for only 7.9% of allocations. Setting context values consumed 13.9% of heap allocations. Each context Set allocates a new map if the key doesn\u0026rsquo;t exist. Since no capacity is preallocated, this results in additional memory usage.\nIf the overhead for grouping logs by request ID is too high, consider removing it.\nAllocation Graph Larger rectangles = higher memory usage. Prioritize tuning large allocations with high % usage. Zerolog Overhead Despite using a shared Logger instance, allocations still occurred due to the use of With().Str().Logger() on every request.\nvar logEntry zerolog.Logger = h.logger.With(). Str(\u0026#34;trace_id\u0026#34;, ctx.GetString(RequestIDContextKey)).Logger() Although cleaner for readability, this approach allocates new memory on each invocation.\nSolution: Accept the readability tradeoff or switch logging frameworks.\nHowever, other loggers may allocate even more due to interface marshalling. Always benchmark alternatives.\nGC Tuning Goal: minimize STW (Stop-The-World) latency caused by excessive GC.\nUsing GOMEMLIMIT GOGC uses heap growth rate to trigger GC. Default GOGC = 100 → GC triggers when heap doubles. Tuning Based on Program Behavior Low GOGC → GC runs too often, especially early on when heap is small. High GOGC → Risk of OOM. E.g., with 40GB heap and GOGC=50, GC only runs after heap reaches 60GB. GOMEMLIMIT caps memory usage before GC triggers. Set it below peak usage to reduce GC frequency.\nGOMEMLIMIT is a soft limit — Go may allocate slightly more than configured.\nSuggested Workflow:\nStart with default GOGC. Measure memory usage. Set GOMEMLIMIT to ~80% of peak. Monitor GC behavior. Benchmarking Production Code Use go test -benchmem to evaluate memory allocations.\ngo test -bench=. -benchmem Example benchmark of StructuredLogger:\nBenchmarkStructuredZeroLoggerMessage-12 596910 1724 ns/op 1000 B/op 30 allocs/op BenchmarkStructuredZeroLoggerMessageWithFields-12 285715 3926 ns/op 1633 B/op 60 allocs/op BenchmarkStructuredZeroLoggerMessageWithFieldsWithContext-12 222223 5346 ns/op 3418 B/op 68 allocs/op BenchmarkStandardZeroLoggerMessage-12 11927823 90.17 ns/op 0 B/op 0 allocs/op BenchmarkStandardZeroLoggerMessageWithDeterminedFields-12 5649648 217.6 ns/op 0 B/op 0 allocs/op BenchmarkStandardZeroLoggerWithFields-12 300001 3894 ns/op 1553 B/op 59 allocs/op Even with ZeroLogger, repeated use of WithFields leads to significant allocations.\nUse Case Driven Pointer Usage While pointers are efficient due to passing references instead of values, Go may allocate them on the heap, unlike value types which may reside on the stack.\nWhen to Use Pointers Large structs: avoid copying Mutability: need to modify original value Consistency: stick to pointer receivers for uniformity Nullable fields: to distinguish between zero value and nil Ref: https://articles.wesionary.team/use-case-of-pointers-in-go-w-practical-example-heap-stack-pointer-receiver-60b8950473da\nPreallocate Slice Capacity Without preallocating, append() causes reallocation on each operation.\nGo can optimize stack allocation for small-sized slices:\nMaxStackVarSize: up to 10MB for explicitly declared vars MaxImplicitStackVarSize: up to 64KB for implicit vars MaxSmallArraySize: arrays \u0026lt;= 256 bytes are stack-allocated Ref: https://go.dev/src/cmd/compile/internal/ir/cfg.go\n","permalink":"https://dingyu.dev/en/posts/go-pprof-gc/","summary":"Ever heard of \u0026ldquo;many a mickle makes a muckle\u0026rdquo;? By identifying bottlenecks and minimizing memory allocations, you can reduce GC overhead and build highly optimized applications!","title":"[Go] Tuning GC with pprof"},{"content":"Background Developers were manually monitoring logs for incident response. However, there were limitations with log monitoring: Elasticsearch field mapping errors due to field type mismatches causing log event drops. If system logs were required (e.g., for stack traces), developers had to request them separately from the system engineers (SE), resulting in operational overhead. Goal Establish a common Sentry integration standard for Go-based projects. Provide feature specifications and guidelines for Sentry usage. Build a stable service foundation through faster incident response after Sentry adoption. This document assumes that a Sentry account and a Sentry project are already set up.\nAbout Sentry Capture Exception Automatically detects and records exceptions and errors in applications. Sentry provides stack traces and environment information at the time of error occurrence, making it easier to identify root causes.\nTransaction Monitoring Monitors application performance by tracking transactions such as web requests or API calls, providing response times, success rates, and error rates.\nTracing Tracks transactions in detail, analyzing service-to-service calls and database queries to identify bottlenecks.\nAlerts Manages issues and notifies via email, Slack, etc., when problems arise. Developers can be assigned to issues and collaborate on resolutions.\nRelease Tracking Tracks application versions and analyzes the impact of new releases on error rates, helping detect problems introduced by recent deployments.\nSentry Alerts Set up Sentry alerts integrated with collaboration tools for easy developer response.\nSetup Step-by-Step Dashboard \u0026gt; Alerts \u0026gt; Create Alert\nTrigger Setup\nIssues: Trigger alerts based on stacktrace patterns (e.g., API HTTP status code-based, service category-based). Number of Errors: Trigger alerts based on error occurrence counts. Users Experiencing Errors: Trigger alerts when a threshold number of users experience errors (e.g., 100 users failing to log in). Detailed Conditions\nWHEN: Define when the alert should trigger (any vs. all conditions). IF: Specify detailed conditions like tags, frequency, or categories. Action Selection\nSend notifications to Email, Slack, Teams, etc. Development Client Options DSN: Unique string identifying the Sentry project. Environment: Specify the runtime environment (e.g., production, staging, development). Release: Track the application version where the error occurred. SampleRate: Control the proportion of reported events (e.g., 0.1 means 10% sampling). TracesSampleRate: Similar to SampleRate but for performance data. BeforeSend: Callback to modify/filter events before reporting. AttachStacktrace: Include stack traces in logs. ServerName: Specify server name for better error grouping. Integrations: Set additional integrations with frameworks and libraries. Transport: Customize event transport, including timeout settings. Initialize Best practice is to call sentry.Init() at the application\u0026rsquo;s entry point (main.go).\nerr := sentry.Init( sentry.ClientOptions{ Dsn: config.Sentry.DSN, SampleRate: config.Sentry.SampleRate, EnableTracing: config.Sentry.EnableTrace, Debug: config.Sentry.Debug, TracesSampleRate: config.Sentry.TracesSampleRate, Environment: config.Sentry.Environment, AttachStacktrace: true, Transport: \u0026amp;sentry.HTTPSyncTransport{ Timeout: config.Sentry.Timeout, }, }, ) Capture Exception Capture and track exceptions automatically:\nhub := sentry.GetHubFromContext(ctx) if hub == nil { hub = sentry.CurrentHub().Clone() ctx = sentry.SetHubOnContext(ctx, hub) } hub.CaptureException(err) Stack Trace Differences: Go errors vs pkg/errors errors package: Does not capture stack traces. github.com/pkg/errors: Includes stack traces automatically. Test example:\nreturn pkgErr.Wrap(PkgErr1, PkgErr2.Error()) Scope() Scope allows attaching contextual metadata (e.g., request parameters) to events.\nhub := sentry.GetHubFromContext(r.Context()) if hub == nil { hub = sentry.CurrentHub().Clone() r = r.WithContext(sentry.SetHubOnContext(r.Context(), hub)) } hub.Scope().SetRequest(r) Sentry Advancement SentryInitializer Manages singleton initialization of the Sentry client:\ntype SentryInitializer struct { conf *Config enabled bool mutex sync.RWMutex } func (si *SentryInitializer) Init() error { ... } func (si *SentryInitializer) Enabled() bool { ... } Manages concurrent access safely with mutex.\nErrorCapturer Captures exceptions through the Sentry Hub:\ntype ErrorCapturer interface { CaptureError(ctx context.Context, err error) } Handles Hub setup and error capture internally.\nRecoverMiddleware Handles panics and automatically logs to Sentry:\ndefer func() { if err := recover(); err != nil { hub.RecoverWithContext(r.Context(), err) // Log stack trace } }() Even in panic scenarios, proper logging and Sentry reporting is ensured.\nDespite its cost, Sentry offers great convenience for both APM and error capture management!\n","permalink":"https://dingyu.dev/en/posts/sentry/","summary":"A tool that supports both APM and error traceability?","title":"[Third-Party] Integrating Sentry in Go"},{"content":"Background After deploying on Kubernetes (k8s), intermittent 502 / 504 errors were observed during load testing. Pods were terminated before completing existing requests, causing 504 Gateway Timeout errors. New Pods were launched while old Pods were terminated, causing 502 Bad Gateway errors. Although rolling updates were happening, it was confirmed that without proper Readiness Probe settings, downtime could occur.\nSetup Install Load Testing Tools bombardier: A simple and easy-to-use Go CLI load testing tool. vegeta: A flexible load tester that allows scripting and provides detailed status code responses. Installation steps are omitted here.\nReadiness Probe A mechanism used to determine whether a Pod is ready to handle traffic.\nEven after a container starts, it might not be ready to handle external traffic until certain initialization tasks are completed.\nTraffic Routing Control:\nKubernetes does not route traffic to a Pod until its Readiness Probe succeeds. Ensures Pods handle requests only after they\u0026rsquo;re fully initialized. Pod Status Management:\nUntil the probe passes, Kubernetes removes the Pod from the service endpoint. Without a Readiness Probe, 502 errors can occur.\nIf a container receives traffic immediately after startup — before the server is fully initialized — it may respond with a 502 Bad Gateway.\nChanges to deployment.yml ... readinessProbe: httpGet: port: 8080 path: /alive scheme: HTTP initialDelaySeconds: 30 periodSeconds: 30 ... Create a /alive endpoint and configure the probe to accept traffic only after receiving HTTP 200.\nTest bombardier -c 200 -d 3m -l https://{endpoint} Results:\n5XX errors still occur. lifecycle \u0026amp; preStop What is lifecycle? Kubernetes lifecycle hooks allow execution of commands during certain container states (similar to AOP):\npostStart: Executed immediately after a container starts. preStop: Executed just before container termination. preStop Hook\nUsed to safely detach Pods from services before termination. Helps complete cleanup tasks like closing connections or saving files. Allows graceful shutdown by adding a delay. Even with Readiness Probe configured, without lifecycle settings, intermittent 502 errors could still happen.\nPod termination (SIGTERM) and service deregistration are asynchronous. Pod might still receive traffic but can\u0026rsquo;t serve requests properly. Thus, setting up:\nService Detach → Handle remaining requests → Terminate Pod Achieves a true graceful shutdown.\nChanges to deployment.yml ... lifecycle: preStop: exec: command: - /bin/sh - -c - sleep 40 # Wait for 40s after detaching from service ... Flow:\nKubernetes sends SIGTERM to Pod. preStop hook (sleep 40) is executed. terminationGracePeriodSeconds countdown starts. Pod terminates after hook and grace period. Test bombardier -c 200 -d 3m -l https://{endpoint} Results:\nReduced but not eliminated 5XX errors. terminationGracePeriodSeconds Pod Shutdown Scenario:\nKubernetes sends SIGTERM. Application can finalize connections and clean up. Grace Period:\nDefined by terminationGracePeriodSeconds. Kubernetes waits for clean termination. Default is 30 seconds. SIGKILL:\nIf the Pod is still alive after the grace period, Kubernetes forcefully kills it. Problem:\npreStop sleep is 40 seconds. Default grace period is 30 seconds. Kubernetes sends SIGKILL before graceful shutdown completes. Changes to deployment.yml ... terminationGracePeriodSeconds: 50 ... Important: Check ALB timeout settings if you\u0026rsquo;re using AWS Ingress!\nIf terminationGracePeriodSeconds exceeds ALB timeout, 504 Gateway Timeout errors may occur.\nRecommended:\nlifecycle.preStop (40s) \u0026lt; terminationGracePeriodSeconds (50s) \u0026lt; ALB Timeout (60s) Test bombardier -c 200 -d 3m -l https://{endpoint} Results:\nNo 5XX errors observed! References Kakao Tech Blog - Zero Downtime Deployment on Kubernetes Kubernetes Official Docs - Pod Lifecycle ","permalink":"https://dingyu.dev/en/posts/k8s-zero-downtime/","summary":"How can we prevent elusive 502 and 504 errors during rolling updates in Kubernetes?","title":"[Infra] Zero-Downtime Kubernetes Deployment Guide"},{"content":"AWS Architected Best Practice Well-Architected Can you confidently answer these questions when reviewing your team’s systems or applications? Sometimes you design a system, but you’re unsure if it’s well-designed — that’s where this knowledge comes in handy.\nWhat defines a good design? Core elements Benefits General design principles Cost Performance CPU/memory configuration based on RPS (Requests Per Second) Operational Excellence Security Reliability Performance efficiency Cost optimization Sustainability Design Principles Faster development and deployment Risk mitigation or reduction Is it safe to open port 22 on a server? Should we rely on a Bash script or online guides to open well-known ports? With AWS Systems Manager Session Manager, you can access servers without opening port 22 SCALE UP / SCALE OUT SCALE UP: Vertical scaling — upgrading hardware specs like CPU/memory (UP/DOWN) SCALE OUT: Horizontal scaling — adding more servers with the same specifications (IN/OUT) Best Practices for EC2 Infrastructure Perspective Estimate capacity first → this leads to cost estimation Automate response to security events: trigger automatic actions based on event or condition-based alerts Operational Excellence Core focus: How the organization supports its business objectives Effectively running workloads, gaining operational insights, and continuously improving processes and procedures to deliver business value Design Principles Over-provisioning = waste (based on peak traffic estimates) Under-provisioning = overload risk Test systems at production scale Use identical environments for testing to ensure stability Since it\u0026rsquo;s cloud-based, you can terminate unused resources (e.g., Blue/Green deployment) Architecture experimentation becomes easier with automation Enabling Innovative Architectures MSA (Microservices) ↔ Monolith Use PoCs to identify better migration strategies — don’t get stuck Data-driven architecture Don’t rely on gut feeling; base decisions on data Improvements Through Real-World Testing Failing to prepare for failure can lead to expensive recoveries Human resources should be included as part of the workload Infrastructure as Code (IaC) Manage infrastructure using code\nMake small, reversible changes frequently Like Merge Requests Build CI/CD workflows using CodePipeline Canary Deployment: Deploy to a subset of servers, test and monitor, then roll out to the rest Re-defining Operational Procedures Frequently Opening SSH = opening port 22 Opening well-known ports increases the risk of hacking attempts AWS Session Manager allows browser-based management You can perform prompt-based operations just like with SSH Failure Prediction and Response Build failure-tolerant architectures Perform health checks before routing traffic If a health check fails, isolate the traffic → prevent failure propagation e.g., Case Study: Any Company\n[Issue]\nWhen Availability Zone A goes down, all services go down\n→ Split services across multiple Availability Zones\nInstalling a database directly on EC2 increases the management burden\n→ Use redundancy and dedicated instances to ensure high availability\n[Best Practice Review]\nMost operations were performed manually Product catalog application needed a highly available architecture Security was the top priority Database Replication Use Active/Standby setup Data is synchronized in real-time RPO (Recovery Point Objective): How often data is backed up — RPO can be zero Reliability Key Elements Recover from infrastructure or service failures Dynamically acquire computing resources based on demand Mitigate interruptions due to misconfigurations or temporary network issues Auto Recovery From Failures Use Rolling, Canary, or Blue-Green deployment strategies Configure Auto Scaling with min:max:desired capacity settings Horizontal Scaling Scale across multiple Availability Zones Elastic Load Balancer adds capacity automatically when traffic increases (Auto Scaling) Security Security involves protecting your data, systems, and assets using cloud technology and improving your overall security posture.\nMultiple teams sharing a single account\nUse separate AWS accounts for each function\nApply Security at Every Layer Strong Identity and Access Management Like Git commits, changes can be tracked similarly to ChangeLogs — know when and how changes occurred Protect Data In Transit and At Rest Encryption: Amazon Macie For key management, use AWS KMS for common resources, and AWS CloudHSM for team-specific keys Cost Optimization Continuously monitor usage and operate systems that deliver business value at the lowest possible cost Pay-as-you-go architecture Minimize resources for development/test environments Efficient Cost Management Measure value as workloads evolve Consider switching to serverless architectures Use tags for cost tracking: Identify where usage or costs are increasing ","permalink":"https://dingyu.dev/en/posts/aws-well-architected/","summary":"The more you know, the more value you get from AWS. It can be a money pit, but also a highly efficient tool depending on how you use it. Let\u0026rsquo;s explore the best practices for AWS infrastructure design.","title":"[Infra] AWS Well Architected"},{"content":"Purpose For typical RDB transactions, we rely on isolation levels, rollback, and commit mechanisms.\nBut what about Redis?\nRedis does not offer clear options to ensure data consistency and atomicity out-of-the-box.\nThis document records the approach to isolate a transaction and achieve All-or-Nothing atomicity in Redis.\nOptions Redis TxPipeline Lua Script Redis TxPipeline When handling multiple commands in Redis, Pipeline is a natural choice.\nPipeline allows sending multiple commands to the Redis server in a batch and receiving multiple responses at once.\nHowever, standard Pipeline does NOT guarantee transactional integrity.\nDuring execution, data might still be modified by other commands, resulting in inconsistencies.\nEdge Cases Network Latency: Command and response order may mismatch due to network delay. Multi-threaded Environment: Redis allows concurrent client requests, making command execution order non-deterministic. Redis Server Configuration: Replication settings (e.g., slaveof) can impact consistency. TxPipeline solves these issues.\nPros Transactional Guarantee: Commands are treated as one atomic transaction. Performance Boost: Reduces network overhead by batching commands. Atomicity: All commands succeed or fail together (but no rollback). Cons Memory Usage: Large transactions consume significant memory. Complexity: Careful management is needed, especially when mixing with regular pipelines. Test Using WATCH ensures monitored keys are checked for changes. If any change occurs, the transaction fails.\nExample Transaction:\nMULTI SET key1 value1 SET key2 value2 SET key3 value3 EXEC If an error occurs at SET key2:\nMemory Exhaustion Case: OOM command not allowed when used memory \u0026gt; \u0026#39;maxmemory\u0026#39; Wrong Type Error: WRONGTYPE Operation against a key holding the wrong kind of value The most common realistic failure is client disconnection.\nEven if you attempt rollback manually after disconnection, it would likely fail — thus TxPipeline guarantees consistency but not perfect atomicity.\nLua Script Execute multiple Redis commands atomically inside a Lua script.\nEdge Cases Network Failure During Script Upload: Script might never reach the server. Network Failure During Response Reception: Script executes but the client may not receive the result. Pros Lightweight and Fast: Lua scripts execute efficiently. Built-in Scripting: Extends Redis functionality (like raw SQL vs ORM). Readable Syntax: Clean, easy-to-understand language. Cons Smaller Ecosystem: Fewer libraries and community support. Limited Data Types: Integer/floating point distinctions are loose. Strict Syntax: Steeper learning curve for beginners. Threading Limitations: Lua is fundamentally single-threaded. Test Lua scripts also do NOT support rollback.\nWorst-case scenario remains: network disconnection during execution.\nExample: Insert key1 ~ key5, simulate error at key3.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; ) var ctx = context.Background() func main() { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: \u0026#34;localhost:6379\u0026#34;, }) luaScript := ` for i = 1, #KEYS do if KEYS[i] == \u0026#39;key3\u0026#39; then error(\u0026#39;Error on setting key3\u0026#39;) else redis.call(\u0026#39;SET\u0026#39;, KEYS[i], ARGV[i]) end end ` keys := []string{\u0026#34;key1\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;key3\u0026#34;, \u0026#34;key4\u0026#34;, \u0026#34;key5\u0026#34;} values := []interface{}{\u0026#34;value1\u0026#34;, \u0026#34;value2\u0026#34;, \u0026#34;value3\u0026#34;, \u0026#34;value4\u0026#34;, \u0026#34;value5\u0026#34;} err := rdb.Eval(ctx, luaScript, keys, values...).Err() if err != nil { fmt.Printf(\u0026#34;Lua script execution error: %v\\n\u0026#34;, err) return } fmt.Println(\u0026#34;Lua script executed successfully\u0026#34;) } [Result]\nSummary Pros \u0026amp; Cons TxPipeline Lua Script Key Features - Standard Redis syntax- Ensures data integrity- Optimistic lock (WATCH) prevents conflict - Requires Lua scripting- Ensures data integrity- Executes script as a single unit Transaction Rollback Not supported Not supported Drawbacks - Slightly lower performance than Lua- Complexity increases with large transactions - Management overhead for Lua scripts- Learning curve Both options guarantee data integrity, but neither supports rollback after failure. Validation before insertion reduces chances of runtime errors. Worst-case scenario: Redis client connection drops during transaction. Cannot roll back manually if disconnected. Consider Redis Sentinel if high availability edge cases need to be addressed. Benchmark Testing 1,000 key insertions into Redis:\nTest Item Test Count Avg Execution Time (ms) Lua Script 1424 0.83 TxPipeline 460 2.56 Pipeline 506 2.34 Performance differences are not critical. TxPipeline or Lua Script is recommended to ensure data consistency. ","permalink":"https://dingyu.dev/en/posts/redis-transaction/","summary":"How does Redis guarantee transactionality without traditional locks?","title":"[DB] Redis Transaction"},{"content":"About FastAPI Convention What is FastAPI? Advantages of FastAPI Dependency Injection Automatic Documentation Asynchronous Support Pydantic Model With the rise of cloud services, MSA (Microservice Architecture) has gained attention, leading to changes in server ecosystems. Now, stateless architectures using lightweight RESTful APIs have become mainstream, and FastAPI is optimized for such small, modular services.\nDependency: (If A must run after B, they are dependent) Depends is the core function that lowers coupling for tasks like authentication and DB connections. Auto Documentation: Documentation often feels like homework to developers. Unlike other frameworks, FastAPI automatically generates ReDoc and OpenAPI docs without additional dependencies, increasing developer productivity. Async Support: While Python is synchronous by default and its GIL (Global Interpreter Lock) discourages threading, FastAPI supports async operations out of the box. Pydantic: FastAPI loves Pydantic. It provides great features like serialization, type validation, and path variable parsing. Characteristics The biggest issue is the lack of a de facto standard. Unlike long-standing frameworks, FastAPI is relatively young and emphasizes flexibility and lightweight design, which naturally results in the absence of a consistent coding standard. While this freedom can be positive, it may also be seen as a lack of structure.\nPurpose Due to the absence of class-based structures and consistent design patterns, collaboration within teams often feels chaotic, even among teammates. The goal is to establish our own convention to ensure intuitive and maintainable project architecture.\nClass-Based Convention Current Issues Utility-dependent methods that lack structure Classes take on too many responsibilities (low cohesion) Non-intuitive structure Business logic is scattered, making code hard to read No consistent convention for Dataclass, Pydantic Models, etc. Requirements Project structure must be consistent and intuitive Each class should have a single responsibility Organize packages by business layer Convention Based on Requirements 1. Consistent and Intuitive Project Structure [FastAPI\u0026rsquo;s Suggested Project Structure]\n. ├── app │ ├── __init__.py │ ├── main.py │ ├── dependencies.py │ └── routers │ │ ├── __init__.py │ │ ├── items.py │ │ └── users.py │ └── internal │ ├── __init__.py │ └── admin.py [Proposed Project Structure]\nfastapi-project ├── app │ ├── worker │ │ ├── enums.py │ │ ├── models.py │ │ ├── dependencies.py │ │ ├── constants.py │ │ ├── exceptions.py │ │ └── utils.py │ ├── configs │ │ ├── config.py │ │ └── log_config.py │ ├── models.py │ ├── utils.py │ ├── exceptions.py │ ├── database.py │ └── main.py ├── aws │ ├── client.py │ ├── models.py │ ├── constants.py │ ├── exceptions.py │ └── utils.py ├── tests/ │ ├── domain │ └── aws ├── templates/ │ └── index.html ├── requirements │ ├── dev.txt │ ├── stg.txt │ └── prod.txt ├── .env └── .gitignore The root of all domain directories is app.\nmain.py initializes the FastAPI app (like src/ in other projects) controller: Module endpoints enums: Enum definitions models: Pydantic models entities: Entity models service: Business logic dependencies: Validation and injection logic constants: Internal constants config: Configuration files exceptions: Custom exceptions If multiple methods share similar concerns, group them into a dedicated package.\nExternal packages are maintained outside the app directory since they aren\u0026rsquo;t app-dependent.\n2. Each Class Should Have a Single Responsibility The author previously misunderstood cohesion. Grouping many methods with related but broad concerns into one class does not improve cohesion—it dilutes it.\nThis follows the SRP (Single Responsibility Principle) from SOLID design.\nCommon examples of so-called \u0026ldquo;GOD\u0026rdquo; classes include: XXXService, XXXClient, XXXHandler, XXXWorker\nEven I fell into this trap, grouping anything remotely related under one service. But this leads to poor readability and becomes a nightmare during unit testing.\nFor example:\nImplement a feature to write user logs into a txt file\n[Bad Example]\nclass UserService: def write_log_file(self, user_id: str) -\u0026gt; None [SRP Applied]\nclass UserLogWriter: def __init__(self, user: User): self.user = user def write(self) -\u0026gt; None While this is a simplified example, when services accumulate various unrelated features, it hurts readability and maintainability.\nTo be more Pythonic and adhere to OOP principles, I also refactored the router to be class-based.\nIt might seem unnecessary, but I aimed to manage controllers as containers using inheritance to reduce boilerplate.\nExample: [BaseController] [HelloController] 3. Organize by Business Layer If you’re building a CRUD app for a User domain, the structure might look like:\nfastapi-project ├── app │ ├── user_manager │ │ ├── user_getter.py │ │ ├── user_updater.py │ │ ├── user_creator.py │ │ ├── enums.py │ │ ├── models.py │ │ ├── entities.py │ │ ├── user_database.py │ │ ├── dependencies.py │ │ ├── constants.py │ │ ├── exceptions.py │ │ └── utils.py Want to retrieve a User entity from the DB? Just use UserGetter.get()—clean and predictable.\nEven if you later add a Manager layer using the Facade pattern, this structure still holds.\nWhat if model class names collide? Yes, of course. Entity and DTO classes often share names, so we separate them using namespaces:\nimport app.user_manager.entities as entity import app.user_manager.models as dto user_dto = dto.User user_entity = entity.User Design is more important than implementation. Even at the implementation level, clear conventions allow consistent class, sequence, and module diagrams to be built.\nThis convention isn\u0026rsquo;t a universal truth. But it\u0026rsquo;s a solid example of how one could be done.\n","permalink":"https://dingyu.dev/en/posts/fastapi-convention/","summary":"A Class-based FastAPI Structure Guide","title":"[Python] FastAPI Convention"},{"content":"Before We Begin Purpose Automating login steps is one of the most common challenges in data scraping from the web. Some portal sites implement Google Recaptcha to block Selenium-based automation, and this post aims to bypass that. A secondary goal is to gain a deeper understanding of how ChromeDriver works under Selenium. While this project uses a Chrome extension, many paid Recaptcha solvers rely on real humans to solve captchas. This post investigates why that is the case. Basic Setup 1. Setting up ChromeDriver Check the version of Chrome browser you are using: Settings → About Chrome Download the ChromeDriver version that matches your browser: https://chromedriver.chromium.org/downloads -\u0026gt; Download the driver version that supports your current Chrome version Or add as a dependency / NuGet package Automatically matching the ChromeDriver version with the installed browser version via code will be added in future enhancements.\n2. Download Buster: Recaptcha Solver Buster is a Chrome extension that solves Google\u0026rsquo;s Recaptcha using voice recognition and inputs the result automatically. Search \u0026ldquo;buster recaptcha\u0026rdquo; on Google and install the extension from the Chrome Web Store. You can configure the Speech service (Wit.ai by default and free to use). Google Speech-to-Text is also available and was reported in some articles to achieve a 97% pass rate. However, since Recaptcha is a Google service, it is likely that the same dataset was used to train the AI, and this speech service is paid, so we’ll skip it.\nUsing Wit.ai, Recaptcha could be solved in up to 8 retries. This will be an important issue explained further below.\n3. Extension Setup Selenium\u0026rsquo;s ChromeDriver() class provides an option called addExtension() for adding extensions.\naddExtension(String extensionPath) accepts the path to the .crx extension package file.\nIn Chrome, go to More Tools → Extensions or visit chrome://extensions Enable Developer Mode and remember the ID of the extension you want to use Unzip the extension and place it where it can be loaded by ChromeDriver (e.g., folder like 1.3.1_0) Typically located in Users-{Username}-AppData-Local-Google-Default-Extensions — find the folder using the extension ID. ! If AppData is hidden, manually enter \\AppData in the File Explorer path bar !\nAnalyzing Recaptcha on the Webpage via Browser Before writing any code, analyzing the webpage structure is crucial.\nWe\u0026rsquo;ll test with this URL: https://patrickhlauke.github.io/recaptcha/ — it always loads Recaptcha.\nPacket Monitoring with Fiddler If Recaptcha is enabled during login, the login page will return something like ex:needRecaptcha in the response. If needRecaptcha = true, Recaptcha solving becomes mandatory. A successful login POST request must include the Recaptcha token. Services offering Recaptcha API solve the captcha and return this token. Changes in the Browser First, focus on the top Recaptcha checkbox widget:\nIt includes a data-sitekey attribute — a unique identifier per site (site key) Recaptcha solve APIs use this sitekey to simulate solving and return tokens. Inside this div is an iframe, accessible using Selenium’s SwitchTo().Frame() From analysis, we found that Recaptcha success can be detected by checking if aria-checked=\u0026quot;false\u0026quot; becomes true This change is used as a checkpoint for scraping automation: This iframe also holds the Recaptcha token once solved Second, another iframe has the title recaptcha challenge expires in two minutes\nIf it’s your first captcha attempt, it might pass instantly. After repeated attempts, image/audio challenges appear here This is also where the Buster button lives Wit.ai often fails on the first few tries. You’ll need to hit the reload and Buster buttons until successful.\n! Google detects repetitive actions and may completely block Recaptcha for your IP. Hence, this is an experimental-only project !\nTo Code (C# Base) We design the flow based on the previous analysis:\nVisit login page → Check needRecaptcha = true? → Control browser with Selenium → Use Buster and reload button until checkbox is ticked → Extract token → Send POST login request\nWe’ll use MSTest as the testing framework.\nInstall Selenium.WebDriver via NuGet package\nCreate ChromeDriver\nChromeOptions _options = new ChromeOptions(); _options.AddExtension(@\u0026#34;...path_to_.crx\u0026#34;); ChromeDriver _driver = new ChromeDriver(_options); If using standalone ChromeDriver:\nChromeOptions _options = new ChromeOptions(); _options.AddExtension(@\u0026#34;...path_to_.crx\u0026#34;); ChromeDriver _driver = new ChromeDriver(@\u0026#34;...path_to_chromedriver_folder\u0026#34;, _options); Extra: Use Chrome Profile for ChromeDriver If you can access a non-sensitive user profile, you can reuse it:\nClick profile picture in Chrome Continue without sign-in Create new profile Profile path: Users-{User}-AppData-Local-Google-Profile{X} Download Buster extension into that profile Set profile via ChromeOptions: ChromeOptions _options = new ChromeOptions(); _options.AddArgument(\u0026#34;--user-data-dir=\u0026#34; + @\u0026#34;path_to_profile\u0026#34;); _options.AddArgument(\u0026#34;--profile-directory=ProfileNumber\u0026#34;); ChromeDriver _driver = new ChromeDriver(_options); Warning: If Chrome is already running with this profile, Selenium will fail to open the browser\nUtility methods to check Recaptcha state: public static bool IsChecked(ChromeDriver driver) { return driver.FindElement(By.Id(\u0026#34;recaptcha-anchor\u0026#34;)).GetAttribute(\u0026#34;aria-checked\u0026#34;) == \u0026#34;true\u0026#34;; } public static bool IsExistByCss(ChromeDriver driver, string cssQuery) { try { driver.FindElement(By.CssSelector(cssQuery)); } catch { return false; } return true; } Check if captcha exists: if (IsExistByCss(_driver, \u0026#34;iframe[title=\\\u0026#34;reCAPTCHA\\\u0026#34;]\u0026#34;)) Find two Recaptcha-related iframes: IWebElement first = _driver.FindElement(By.CssSelector(\u0026#34;iframe[title=\\\u0026#34;reCAPTCHA\\\u0026#34;]\u0026#34;)); IWebElement second = _driver.FindElement(By.CssSelector(\u0026#34;iframe[title=\\\u0026#34;reCAPTCHA challenge expires in two minutes\\\u0026#34;]\u0026#34;)); Switch to first iframe and click the checkbox: _driver.SwitchTo().Frame(first); var checkBox = _driver.FindElement(By.ClassName(\u0026#34;recaptcha-checkbox\u0026#34;)); _driver.ExecuteScript(\u0026#34;arguments[0].click()\u0026#34;, checkBox); Check if it passed: if (!IsChecked(_driver)) Retry with second iframe and click Buster button: _driver.SwitchTo().DefaultContent(); _driver.SwitchTo().Frame(second); _driver.Manage().Timeouts().ImplicitWait = TimeSpan.FromSeconds(200); var busterHolder = _driver.FindElement(By.ClassName(\u0026#34;help-button-holder\u0026#34;)); busterHolder.Click(); If not passed, refresh and try again: if (!IsChecked(_driver)) { _driver.SwitchTo().DefaultContent(); _driver.SwitchTo().Frame(second); _driver.FindElement(By.ClassName(\u0026#34;rc-button-image\u0026#34;)).Click(); _driver.FindElement(By.ClassName(\u0026#34;help-button-holder\u0026#34;)).Click(); } else { isChecked = true; break; } Extract token: if (isChecked) { _driver.SwitchTo().DefaultContent(); _driver.SwitchTo().Frame(second); _token = _driver.FindElement(By.Id(\u0026#34;recaptcha-token\u0026#34;)).GetAttribute(\u0026#34;value\u0026#34;); return _token; } Improvements Through this project, I came to appreciate Google\u0026rsquo;s anti-bot intelligence.\nIf I had used the paid Google Speech-to-Text, the success rate would’ve been higher even under heavier traffic. However, since we used free tools, Recaptcha eventually flagged the automation and blocked us.\nRecaptcha v3 is known to detect cursor movement and behavior patterns — even if this project is enhanced, v3 may still detect it.\nThat’s why many Recaptcha solving APIs hire real humans to manually solve captchas and return the token.\nKey improvement areas:\nChromeDriver version must be manually matched with Chrome browser version Heavy traffic and repeated failures lead to Recaptcha ban Login must complete within 2 minutes of solving the captcha ","permalink":"https://dingyu.dev/en/posts/crawling-selenium-solver/","summary":"Nothing can stop me! Having trouble with Recaptcha while crawling? Have you tried Buster?","title":"[Crawler] Recaptcha Solver"}]
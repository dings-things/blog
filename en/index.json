[{"content":"Why This Setup? Debugging on development servers was cumbersome due to lack of access/log visibility. Kafka in our environment enforces SASL authentication. We needed to test with different SASL mechanisms (SCRAM-256 and 512) without changing application code. Our Event Dispatcher consumes from one Kafka and produces to another—all on the same cluster but under different authentication methods. For availability testing (e.g., ISR=2), a 3-node Kafka cluster is used. Features Kafka cluster with SCRAM-SHA-256 and SCRAM-SHA-512 authentication Zookeeper-based Kafka cluster with 3 brokers Kafka UI for producing and inspecting topics Event Dispatcher that uses SCRAM-256 for consumer and SCRAM-512 for producer Why Bitnami Kafka? Bitnami’s image provides flexible environment-based configuration:\nSASL users auto-registration: KAFKA_CLIENT_USERS=user256,user512 KAFKA_CLIENT_PASSWORDS=pass256,pass512 SASL protocol setup without editing server.properties. Deployment \u0026amp; Teardown docker compose --env-file .env up --build docker compose down -v .env Example: 256_SASL_USER=user256 256_SASL_PASSWORD=pass256 512_SASL_USER=user512 512_SASL_PASSWORD=pass512 KAFKA_BROKER_0_PORT=9092 KAFKA_BROKER_1_PORT=9093 KAFKA_BROKER_2_PORT=9094 Sequence Overview Zookeeper starts, no user state yet Kafka Brokers register SCRAM users via env Controller elected, brokers stabilize Kafka UI connects using SCRAM-512 for admin operations Event Dispatcher: Consumes with SCRAM-256 Produces with SCRAM-512 Common Issues \u0026amp; Fixes 1. Misconfigured ADVERTISED_LISTENERS Symptom: UI and clients can\u0026rsquo;t connect to Kafka Fix: Use host.docker.internal not localhost\nKAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://host.docker.internal:9092 2. Mixed SCRAM mechanism auth failures Symptom: UI works, dispatcher fails with EOF or Failed to authenticate Fix: Double-check user/mechanism pairs and registration\n3. Kafka UI produce failure Symptom: UI can\u0026rsquo;t send messages Fix: Ensure correct JAAS SASL configs passed into UI\n4. Cluster ID mismatch Symptom: Broker startup fails after a restart Fix: Teardown volumes fully:\ndocker compose down -v 5. Producer/Consumer config missing SASL Symptom: no brokers to talk to error Fix: Ensure SASL mechanism and credentials are passed correctly into the Kafka config\nFinal Thoughts Building a SASL-based Kafka setup locally is extremely helpful for validating production-like authentication scenarios. Bitnami’s Kafka image simplifies user registration and protocol setup. Combined with Kafka UI and an app like Event Dispatcher, you can simulate consumer-producer flows entirely within Docker.\nHaving both SCRAM-256 and SCRAM-512 supported in the same cluster without duplicating infrastructure is a game changer.\n","permalink":"https://dingyu.dev/en/posts/local-sasl-kafka/","summary":"E2E testing is critical for validating production-ready applications. But how do you simulate a secure Kafka environment locally—especially with SASL authentication? This post demonstrates how to spin up a Kafka cluster with both SCRAM-SHA-256 and SCRAM-SHA-512 using Docker Compose.","title":"[EDA] Running a Local Kafka Cluster with SASL SCRAM Authentication (Docker Compose)"},{"content":"Background One of the biggest trade-offs between asynchronous and synchronous programming is that while performance may be improved, it becomes harder to trace.\nThe same is true for event-driven architecture (EDA) using Kafka. It is not easy to track the flow of asynchronously processed data in real-time. In particular, issues often go unnoticed until after they occur, making Kafka monitoring increasingly critical.\nIn event-driven systems, performance monitoring is crucial. In REST API-based architectures, traffic is typically handled using Kubernetes Horizontal Pod Autoscaler (HPA) based on CPU/memory usage and request rates. However, in Kafka-based architectures, the key performance factors are the number of partitions and the processing speed of consumers.\nWhat if there are too few partitions or slow consumers causing lag? → Developers must manually analyze performance and perform partition scaling or consumer scale-out.\nTo detect and address such issues in advance, a Kafka monitoring system was built.\nWe considered three different options for monitoring Kafka performance:\nDesign Comparison of Kafka Monitoring Solutions AWS CloudWatch Using AWS CloudWatch for Kafka monitoring allows metric collection at the PER_TOPIC_PER_PARTITION level.\nKey Monitoring Metrics Metric (AWS/MSK) Description EstimatedTimeLag Time lag (in seconds) of the partition offset after the consumer group reads data OffsetLag Number of offset lag in the partition after consumption by the consumer group Item Description ✅ Easy setup Integrated with AWS MSK by default ✅ Supports CloudWatch Alarm + SNS Simple alert setup available ❌ Viewable only on AWS Console Hard to integrate with external monitoring tools ❌ Cost concerns Additional fees for topic-level monitoring Helm-based Monitoring on EKS (Internal solution, failed attempt) We attempted Kafka monitoring using an internal Helm chart, but it failed due to MSK and EKS residing in different regions.\nItem Description ✅ Internal system integration Smooth integration with in-house systems ❌ MSK and EKS are in different regions Integration not possible ❌ Cost if EKS is redeployed in MSK region Additional expenses may occur Ultimately, this approach was abandoned.\nEC2-based Docker Compose Monitoring (Final Choice) Eventually, we opted to deploy EC2 in the same VPC as MSK and build the Kafka monitoring stack manually.\nUsed JMX Exporter \u0026amp; Node Exporter for Kafka metric collection Used Burrow to monitor consumer lag Enabled long-term monitoring via Thanos and Prometheus Item Description ✅ Cost-efficient Can run on low-cost T-series EC2 instances ✅ Scalable Easily customizable and extensible ✅ Fine-grained Kafka monitoring Enables detailed tracking via Burrow ❌ Initial setup burden Manual configuration with potential trial-and-error ❌ Lack of Burrow \u0026amp; Thanos ops experience Required team to learn and operate monitoring stack from scratch Surprisingly, starting from scratch became a benefit, so we decided to build EC2-based monitoring ourselves.\nArchitecture Overview MSK Terminology zookeeper: Stores Kafka metadata and manages Kafka states broker: The server/node where Kafka runs JMX Exporter: Exposes various metrics from Apache Kafka (brokers, producers, consumers) for monitoring via JMX Node Exporter: Exposes CPU and disk metrics Kafka Monitoring Architecture Evolution: Integrating Prometheus, Thanos, and Burrow Our monitoring architecture evolved from standalone Prometheus (V1), to Thanos integration (V2), to including Kafka consumer monitoring with Burrow (V3).\nEach version is compared below with its respective pros and cons.\nV1: Standalone Prometheus Metric Collection Used Prometheus to gather Kafka metrics and added both CloudWatch and Prometheus as data sources in Grafana for monitoring.\nArchitecture Diagram Pros Simple setup using only Prometheus Easy to compare metrics between Prometheus and CloudWatch in Grafana Cons If Prometheus goes down, the entire monitoring stack becomes unavailable → SPOF risk Prometheus stores all metrics in-memory (TSDB), increasing memory usage with metric volume Large TSDB size can degrade Prometheus performance V2: Prometheus + Thanos Sidecar for HA To ensure high availability, we ran two Prometheus instances and added Thanos Sidecars to connect them to a central Thanos Query + Store server.\nArchitecture Diagram Why Thanos? Avoid metric duplication: Thanos Query deduplicates metrics collected from multiple Prometheus instances Separate short-term (Prometheus) and long-term (Thanos Store) storage: Enables restoration from S3 Reduce TSDB size: Lowers memory usage and cost Pros HA support: Continues monitoring even if one Prometheus fails Long-term metric storage: Offload to S3/GCS Resource efficiency: Reduces Prometheus memory usage Cons Increased operational complexity: Requires Sidecar, Query, and Object Storage (S3) configuration Performance limits of Thanos Query: Heavy queries may impact performance V3: Burrow + Prometheus + Thanos Sidecar for Kafka Consumer Monitoring To cut costs and enhance Kafka Consumer Lag monitoring, we replaced CloudWatch with Burrow for metric collection.\nNow the monitoring stack is fully built with Burrow + Prometheus + Thanos, removing CloudWatch.\nArchitecture Diagram Burrow collects Kafka consumer metrics periodically, Prometheus scrapes those metrics, and Thanos Query gathers them from Sidecar.\nWhy Burrow? Cost savings: Eliminates CloudWatch fees Detailed Consumer Lag visibility: Real-time offset tracking for Kafka consumer groups ACL-aware monitoring: Burrow provides fine-grained lag info per topic and group, unlike CloudWatch Pros Reduces CloudWatch cost Detailed insights into Consumer Group lag and partition status Integrates well with existing Prometheus + Thanos stack Cons Setup overhead: Initial integration with Kafka clusters and ACL permissions needed Limited alerting in Burrow: Works best when combined with Prometheus Alertmanager or Grafana Alerta Version Comparison Summary Architecture Pros Cons Verdict V1: Prometheus only Easy and fast setup Lacks HA, high memory usage Inefficient due to SPOF V2: Prometheus + Thanos HA, long-term storage, memory optimization More complex setup Good for scalable systems V3: Burrow + Prometheus + Thanos Reduces cost, adds detailed monitoring, integrates well Needs setup, weak built-in alerts ✅ Final choice To monitor Kafka effectively, a combination of Burrow + Prometheus + Thanos was the optimal solution for comprehensive Kafka consumer lag visibility.\nImplementation Burrow Kafka Consumer Lag Monitoring with Burrow While Kafka clients can expose records-lag-max using the metrics() method, this only reflects the lag of the slowest partition, making it difficult to get a full picture of the consumer’s performance. Additionally, if the consumer stops, lag will no longer be measured, requiring an external monitoring system. A representative solution is LinkedIn’s Burrow.\nBurrow: Kafka Consumer Monitoring Reinvented\nConsumer A: Lag is consistently decreasing → Healthy Consumer B: Lag temporarily spiked but recovered → Healthy Consumer C: Lag remains constant → Healthy Consumer D: Lag temporarily increased but recovered → Healthy traffic pattern The Problem with Lag Thresholds Threshold-based detection is prone to false positives. For example, if a threshold of 250 is set, consumers B and D, which are behaving normally, could be incorrectly flagged as unhealthy.\n⚠️ You cannot determine the health of a Kafka consumer solely based on MaxLag!\nHow Burrow Solves This Burrow reads the internal Kafka topic where consumer offsets are committed and evaluates the state of each consumer independently. It’s not dependent on any specific consumer, and automatically monitors all consumers to enable objective status analysis.\nHow Burrow Works Burrow uses a sliding window technique to analyze the last N offset commits. LinkedIn recommends using 10 commits (approx. 10 minutes) to evaluate the following:\nIs the consumer committing offsets regularly? Are the offsets increasing? Is lag increasing? Is there a sustained pattern of increasing lag? Based on this, Burrow categorizes the consumer’s status as:\n✅ OK: Operating normally ⚠️ Warning: Lag is increasing ❌ Error: Consumer has stopped or stalled Burrow detects anomalies through pattern analysis rather than thresholds, and exposes this information via HTTP API and alerting integrations.\nExample Burrow API GET /v2/kafka/local/consumer/dingyu/status Returns the current state of a consumer and details about affected topics and partitions.\nIntegration Guide 1. Prerequisites OS: Amazon Linux 2 or Ubuntu 20.04+ Install Docker and Docker Compose Open these ports in the EC2 security group: Prometheus: 9090 Thanos Sidecar: 10901, 10902 Burrow: 8000 2. Install Packages # Install Docker sudo yum install -y docker sudo systemctl enable docker --now # Install Docker Compose sudo curl -L \u0026#34;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose 3. Folder Structure MSK-MONITORING/ ├── templates/ # Configuration templates │ ├── burrow.tmpl.toml # Burrow config template │ ├── prometheus.tmpl.yaml # Prometheus config template │ ├── targets.tmpl.json # Prometheus targets ├── deploy.sh # Deployment script ├── docker-compose.yaml # Docker Compose file ├── Makefile # Build and render utility ├── README.md # Project documentation 4. Core Components 4.1 Burrow Monitors Kafka consumer states Configured using burrow.tmpl.toml with environment variable substitution Connects to MSK with SASL/TLS Exposes status via HTTP Burrow Troubleshooting SASL authentication lacked documentation, resulting in heavy trial and error Even when skipping TLS auth, skip verify had to be explicitly set Required debugging with custom sarama client configuration Burrow supports SCRAM-SHA-512 and SCRAM-SHA-256 mechanisms. Make sure to match the mechanism used by MSK.\n4.2 Prometheus Collects metrics from Kafka and Burrow Config based on prometheus.tmpl.yaml Uses targets.tmpl.json to gather JMX and Node Exporter metrics 4.3 Docker Compose Launches Burrow, Prometheus, and Thanos Sidecar containers Ensures smooth inter-container communication 4.4 Makefile make render: Renders config files with current environment variables into generated/ directory 4.5 Environment Variable Management Create a .env file in the same directory as docker-compose.yaml, for example:\nPROM_CLUSTER={your-cluster-name} PROMETHEUS_PORT=9090 BURROW_PORT=8000 ZOOKEEPER_HOST_1={zookeeper1_endpoint} ZOOKEEPER_HOST_2={zookeeper2_endpoint} ZOOKEEPER_HOST_3={zookeeper3_endpoint} BROKER_HOST_1={broker1_endpoint} BROKER_HOST_2={broker2_endpoint} BROKER_HOST_3={broker3_endpoint} BURROW_USERNAME={user} BURROW_PASSWORD={password} 5. Setup and Launch 5.1 Clone the Project git clone https://github.com/dings-things/msk-monitoring-docker-compose.git cd msk-monitoring-docker-compose 5.2 Configure Environment Variables Create a .env file.\n5.3 Run Deployment Script chmod +x deploy.sh ./deploy.sh 5.4 Manual Startup (optional) make render docker compose up -d Deployment Tips For detailed usage, see the GitHub repo.\nIn production environments, you can use GitLab Snippets to manage environment variables dynamically via API.\nBuilding the Dashboard Here are the key metrics you should monitor:\nStatus per Topic/Partition: Detect anomalies per partition Use burrow_kafka_topic_partition_status Disk Usage: Alert when nearing disk limits Use node_filesystem_avail_bytes vs. size_bytes for disk utilization CPU Usage: Alert on CPU threshold breaches (may require partition scaling) Use node_cpu_seconds_total to measure user vs idle CPU Consumer Group Health: Overall health of consumers (apps) Use burrow_kafka_consumer_status Group/Topic Lag: Lag per topic/group Use burrow_kafka_consumer_partition_lag Lag per Partition: Granular lag analysis Use tabular view of burrow_kafka_consumer_partition_lag Current Offset: Latest committed offset Use burrow_kafka_consumer_status Final Architecture References Burrow Official Docs Prometheus Documentation Thanos for Prometheus Scaling Operating AWS MSK Connect Effectively MSK Monitoring at Yanolja ","permalink":"https://dingyu.dev/en/posts/dance-with-burrow/","summary":"Still monitoring Kafka using only low-level Input/Output Bytes? Let’s build topic-level monitoring using Burrow, JMX, and Node Exporter.","title":"[EDA] Kafka (MSK) Monitoring with Burrow Prometheus And Thanos"},{"content":"Why Schema First? While EDA promotes loose coupling, event schemas inherently form a tight contract between producers and consumers. Let’s explore why this contract matters and how a Schema Registry helps maintain compatibility.\nPurpose of Event Schemas Define structure and format of event data. Enforce data consistency between producers and consumers. Enable validation, compatibility, and documentation. If you\u0026rsquo;re familiar with REST APIs, this is similar to defining OpenAPI contracts between services:\nEvent streams function the same way—producers emit events conforming to predefined schemas; consumers process them based on those expectations.\nBad Example { \u0026#34;user_id\u0026#34;: 123, \u0026#34;user_action\u0026#34;: 1 } // action code instead of expected string Even with agreed-upon schemas, schema drift can occur—leading to broken consumers. Much like skipping ERD when designing databases, skipping event schemas is risky in EDA.\nCommon Schema Formats Format Pros Cons JSON Human-readable, widely supported Large size, lacks strong validation Protobuf Compact, fast, schema-enforced Hard to debug, needs precompiled schema Avro Compact binary, supports schema evolution Less widely adopted, tooling gaps in some ecosystems Text formats like JSON are appealing for debugging. But size and speed matter in stream processing.\nJSON Suitability Checklist Use JSON only if:\n✅ Messages are small. ✅ You can tolerate slow (de)serialization. ✅ Strong type validation isn’t required. ✅ You don’t need a schema registry. ✅ Volume of messages will remain low. ✅ Debugging via raw payload is helpful. Advantages of Avro / Protobuf Strong typing and schema enforcement Fast (de)serialization Built-in backward/forward compatibility Even small messages show over 2x performance gains in binary formats. The difference increases with message size.\nImpact on Kafka Text-based formats like JSON consume more storage and network bandwidth. High volume = performance degradation at produce/consume phases. What Is a Schema Registry? A Schema Registry stores and version-controls data schemas.\nBenefits:\nEnforces compatibility Enables schema evolution (backward/forward) Minimizes payload size by referencing schemas via ID Centralized schema governance Schema Evolution in Action Producer publishes an event with schema v2. Consumer detects version mismatch and fetches v2 from registry. Consumer proceeds with updated schema. No coordination required. Zero downtime schema upgrades!\nSchema Registry vs. Schemaless Format With Schema Registry Without Schema Registry JSON ❌ Schemaless, can\u0026rsquo;t validate or evolve ✅ Easy to debug, but lacks structure Protobuf ✅ Strong schema + evolution support ❌ Needs .proto file everywhere Avro ✅ Compact, evolvable binary format ❌ Schema must be embedded in each message Using a Schema Registry with schema ID avoids inflating messages with repeated schema data. This helps keep message sizes small.\nCentral Schema vs. Shared Code Registry = Central governance, live updates. Submodule .proto = Tight coupling, manual versioning. AWS Glue vs. Confluent Schema Registry Feature AWS Glue Registry Confluent Schema Registry Schema versioning ✅ Supported ✅ Supported URL persistence ✅ ARN-based ✅ REST endpoint-based Auto upgrade for consumer ❌ Needs explicit fetch ✅ Auto fetch Kafka support ✅ MSK ✅ Confluent Kafka Why Use a Schema Registry? Guarantee Compatibility: Prevent mismatched producer-consumer schemas. Support Evolution: Add/remove fields without breaking clients. Centralized Governance: No more shared .proto headaches. Smaller Messages: Send schema ID, not full schema. Schema Validation: Prevent invalid data from entering the stream. Dynamic Updates: Auto-fetch new schemas at runtime. Compatibility Policies: Enforce forward/backward rules. Schema Auditing: View changes via REST API or UI. References Confluent Blog: Schemas and Compatibility Oliveyoung B2B MSK Case Kafka Serialization Overview ","permalink":"https://dingyu.dev/en/posts/schema-registry/","summary":"How can we ensure backward and forward compatibility for event schemas?","title":"[EDA] Schema Registry"},{"content":"What is RPC? RPC stands for Remote Procedure Call. It\u0026rsquo;s an inter-process communication technology that allows functions or procedures to run in another address space without requiring separate remote control logic.\nIn essence, RPC abstracts the underlying connection and data transfer so that developers can invoke remote functions as if they were local.\nIn distributed services (especially MSA), RPC is commonly used for communication between clients and servers or between servers.\nAs applications are split into microservices, the days of simply calling functions locally in a monolith are gone. Now, services must communicate over the network.\nNetwork latency becomes the bottleneck, not your optimized code.\nHow RPC Works Define the interface using IDL (Interface Definition Language). The IDL definition is used to generate client stubs and server skeletons via a compiler (rpcgen). The client calls a stub method which marshals parameters. The stub sends the request in a form the server can understand. The server receives the call and executes the logic. The server sends back the response. Stub?\nA stub is an abstraction layer that lets clients call remote methods as if they were local. It handles marshaling/unmarshaling and networking logic internally.\nWhat is gRPC? gRPC is a modern RPC framework developed by Google. It uses Protocol Buffers for IDL and message serialization/deserialization. Clients can call server methods on remote machines as if they were local. RPC != gRPC. gRPC is a specific implementation of RPC, using HTTP/2 and Protocol Buffers internally.\nHTTP/1.1 vs HTTP/2 HTTP/1.1 Flow TCP handshake Request/response TCP termination Even with Keep-Alive, HTTP/1.1 can only handle one request per connection (sequentially).\nProblems Overhead from many TCP connections HOL Blocking: one slow request blocks the rest HTTP/2 Improvements Proxy + Keep-Alive Problems Proxies may drop Keep-Alive connections without notifying clients. Clients think the connection is still valid, leading to errors. HTTP/2 Fixes Servers send GOAWAY frames to indicate shutdown Proxies send PING to verify liveness Clients reconnect when needed Multiplexing Multiple requests over a single TCP connection Requests are independent and don’t block each other No More HOL Blocking Advantages and Disadvantages of gRPC Feature Pros Cons Performance Multiplexing, header compression, streaming Requires HTTP/2 support IDL Strong typing with .proto files ProtoBuf is not human-readable Communication Unary, Server/Client Streaming, Bidirectional Complex to debug Language Support Multi-language support (Go, Java, Python, etc.) Varies in maturity Code Generation Auto-generates stubs from .proto Requires additional build tooling Security Strong TLS support TLS setup can be tricky Load Balancing Client-side support available Typically used with Envoy or other proxies Streaming Built-in streaming APIs Harder to implement and maintain Transport Binary serialization = compact, fast Not easily debuggable like REST Browser Supported via gRPC-Web Requires gRPC-Web proxy layer More on gRPC performance, debugging, and implementation tips coming next.\n","permalink":"https://dingyu.dev/en/posts/grpc/","summary":"Whenever we hear about RPC communication (or maybe it’s just me?), we tend to assume it involves Protocol Buffers over HTTP/2 — that is, gRPC. But what exactly is RPC? Why is gRPC so fast? And when should we use it?","title":"[Protocol] Exploring RPC... and GRPC in Depth"},{"content":"Background Inconsistent project structures per service made code hard to follow. No shared conventions made common module/CI reuse difficult, reducing productivity. Onboarding was challenging, especially for Go newcomers. Our team has now matured in using Go and has built internal best practices. We want to establish conventions for better maintainability and clarity across services. Proposed Project Structure Sections marked with * are mandatory. This structure assumes MSA-style projects rather than large monoliths.\nExample Structure (Domain: match sampling) . ├── *docs │ ├── *swagger.yaml │ ├── sequence.md │ └── architecture.md ├── *cmd │ └── *main.go ├── pkg │ ├── file_parser.go │ └── time_convertor.go └── *internal ├── *handler │ ├── *v1/sampling_handler.go │ ├── v2/sampling_handler.go │ ├── server.go │ ├── health_handler.go │ ├── swagger_handler.go │ └── auth_middleware.go ├── data │ ├── mysqldb.go │ ├── redis.go │ ├── feature_event_producer.go │ ├── match_repository.go │ └── nass_api.go ├── *service │ ├── kda_sampler.go │ ├── match_sampling_usecase.go │ └── kda_sampler_test.go ├── logger.go ├── constants.go └── *config.go ├── *gitlab-ci.yml ├── *go.mod ├── *go.sum └── *README.md Section Required Description docs ✔ Project-level diagrams and specs cmd ✔ Entry point for DI \u0026amp; execution pkg optional Utility modules safe for external reuse internal ✔ Core domain logic, hidden from outside handler ✔ HTTP/gRPC/Kafka handlers (versioned) data optional Database, external API, Kafka interactions service ✔ Business logic per SRP, unit tested root files ✔ CI, readme, mod/sum files Constant Convention Use PascalCase for constants shared across packages. Use camelCase for internal/private constants. If a private constant must be exposed, wrap it via a public method. Data Model Convention Define models close to their use (not in shared folders). Use DTOs between layers, and convert as needed. For 2+ arguments, use structs. Keep validation logic in methods for readability and testability. // AS-IS if strings.HasPrefix(match.Version, \u0026#34;rc\u0026#34;) \u0026amp;\u0026amp; match.detail == \u0026#34;test\u0026#34; { ... } // TO-BE if match.IsTest() { ... } Test Conventions Testing business logic is mandatory, not optional. You should write test cases for already-defined errors in a concise yet detailed manner.\nDeterministic Asynchronous Unit Testing Avoid relying on time.Sleep or blindly logging after async execution without assertions.\nPrevent flaky tests by leveraging Dependency Injection (DI) and assert.Eventually.\n1. Injecting a Logger // NewQueue creates a Queue instance responsible for business logic func NewQueue( config Config, httpClient *http.Client, logger *zerolog.Logger, ) (queue Queue, err error) { // The queue will execute the thread executor when Start() is called. queue = Queue{ config: config, client: httpClient, logger: logger, quitChan: make(chan struct{}), } return } 2. Testing Output Test case for queue failure logging t.Run(\u0026#34;Logs failure correctly when queue processing fails\u0026#34;, func(t *testing.T) { // given var buffer bytes.Buffer ... inject logger with buffer as output // when ... execute async task event1, err := queue.Push([]byte(validJSON1)) assert.NoError(t, err) event2, err := queue.Push([]byte(validJSON2)) assert.NoError(t, err) // then assert.Eventually(t, func() bool { output := buffer.String() return strings.Contains(output, event1.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, event2.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, `\u0026#34;success\u0026#34;:false`) }, 1*time.Second, 10*time.Millisecond) }) Test case for queue success logging t.Run(\u0026#34;Logs success correctly when queue processes successfully\u0026#34;, func(t *testing.T) { // given var buffer bytes.Buffer ... inject logger with buffer as output // when ... execute async task event1, err := queue.Push([]byte(validJSON1)) assert.NoError(t, err) event2, err := queue.Push([]byte(validJSON2)) assert.NoError(t, err) // then assert.Eventually(t, func() bool { output := buffer.String() return strings.Contains(output, event1.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, event2.TraceID().String()) \u0026amp;\u0026amp; strings.Contains(output, `\u0026#34;success\u0026#34;:true`) }, 1*time.Second, 10*time.Millisecond) }) ","permalink":"https://dingyu.dev/en/posts/go-convention/","summary":"Efficient Go Project Structure Guide","title":"[Go] Go Convention"},{"content":"Why Loki? In backend applications, we often used Elasticsearch queries to investigate user issues and visualized dashboards using Kibana for operational teams.\nIf asked, \u0026ldquo;Why switch to Loki?\u0026rdquo; the answer is simple: \u0026ldquo;To reduce cost!\u0026rdquo;\nCategory Loki 🟢 Elasticsearch 🔴 Storage Model Stores metadata only; raw logs in object storage Indexes and stores all logs Search Method Label-based search Full-text search Indexing Cost Low High (CPU/memory intensive) Storage Cost Cheap (S3/GCS object storage) Expensive (dedicated nodes needed) Performance Efficient for massive log storage Fast for query Scalability Easy to scale with simple configuration Cluster scaling is more complex Ops Overhead Low (no need to manage cluster) High (cluster management required) Use Case Simple log storage and retrieval Complex analysis and search Pre-Migration Checklist No need to expose dashboards to external stakeholders? Do you only need basic log viewing during incidents? Notes When Integrating Loki When labels are too diverse or contain too many unique values, Loki\u0026rsquo;s indexing becomes inefficient.\nExample: assigning fields like user_id or timestamp as labels can cause a rapid index size increase. Consequences:\nSlow query performance High memory usage Increased storage cost Use Fixed, Low-Cardinality Labels To keep cardinality low:\nUse static or limited-range values as labels Examples: Good 😊: region=\u0026quot;us-east-1\u0026quot;, app=\u0026quot;payment-service\u0026quot; Bad 😥: user_id=\u0026quot;12345\u0026quot;, request_id=\u0026quot;abcd-efgh\u0026quot; Only Use Labels Relevant to Filtering Design labels based on how you plan to filter/search logs in dashboards:\nUse labels only for data that will be filtered or analyzed Avoid labels used solely for debugging Examples: Good 😊: For tracking TPS/latency, design labels around logs that appear once per request → e.g. func=\u0026quot;LogMiddleware.Log\u0026quot; Bad 😥: Using latency itself as a label Separate Log Message from Metadata Labels should serve as tags. Dynamic values should go inside the log message.\nExamples: Good 😊: label:func=\u0026quot;RestrictionsService\u0026quot;, log: member_id=\u0026quot;12341512321\u0026quot;, message=\u0026quot;restricted member\u0026quot; Bad 😥: label:member_id=\u0026quot;12341512321\u0026quot;, log: message=\u0026quot;restricted member\u0026quot;, func=\u0026quot;RestrictionsService\u0026quot; Limit the Number of Labels Too many labels = high cardinality → indexing inefficiency.\nLoki\u0026rsquo;s official recommendation: keep labels under 20. Limit Unique Values per Label Loki recommends fewer than 1,000 unique values per label.\nAcceptable: status=\u0026quot;200\u0026quot;, status=\u0026quot;500\u0026quot; Avoid: user_id=\u0026quot;12345\u0026quot;, session_id=\u0026quot;abcd-efgh\u0026quot; Examples: Good 😊: env=\u0026quot;production\u0026quot;, service=\u0026quot;payments\u0026quot;, region=\u0026quot;us-east-1\u0026quot; Bad 😥: user_id=\u0026quot;12345\u0026quot;, request_id=\u0026quot;xyz-789\u0026quot; Tune Chunk Size and Retention Loki stores logs in chunks and flushes them to object storage periodically.\nToo small chunks → low performance Too large chunks → slow search Recommended: chunk_encoding: gzip chunk_target_size: 1MB~2MB (adjust as needed) Building the Dashboard 1. Set Up Variables While filtering with Variables adds complexity, it’s a gift to your future self.\nApplication logs are usually queried using raw log, so choose frequently used filter labels as variables. Also add text box filters to refine the filtered results.\n2. Build Raw Logs Panel Start with a global log viewer using Logs Visualization.\nExample:\n{_type=\u0026#34;loki\u0026#34;} |= `$filter` 3. Filter Error Logs by Log Level From raw log, duplicate and filter only error level logs.\nSome business logic might bypass error tagging—make sure log levels are properly set.\nExample:\n{level=\u0026#34;error\u0026#34;} 4. Track TPS with Quantiles Precision = speed. Use P50, P99 as standard metrics with Gauge Visualization using quantile_over_time.\nNote:\nUnlike Prometheus TSDB, Loki doesn\u0026rsquo;t compute percentiles efficiently High volume log ranges may degrade performance quantile_over_time is memory intensive and expensive at large time ranges Example:\nquantile_over_time(0.50, {func=\u0026#34;LogMiddleware.Log\u0026#34;} |= `$filter` | json | unwrap latency [$__range]) by (func) 5. Build Distributions by Label Use Piechart to understand overall log distribution.\nLabels must be used, so apply this only to unique label values\nExample:\nsum by(method) (count_over_time({func=\u0026#34;LogMiddleware.Log\u0026#34;} |= `$filter` [$__auto])) 6. Use Table to Visualize and Filter Distributions Table is great for at-a-glance inspection. You can make labels clickable for filtering.\nExample:\nsum by(method) (count_over_time({func=\u0026#34;LogMiddleware.Log\u0026#34;} |= `$filter` [$__auto])) Final Dashboard References 6 easy ways to improve your log dashboards with Grafana and Grafana Loki ","permalink":"https://dingyu.dev/en/posts/es-to-loki/","summary":"Is heavy-duty Elasticsearch necessary just for basic application monitoring? This post explores the transition to Loki and the lessons learned.","title":"[LGTM] Elasticsearch to Loki Migration Story"},{"content":" REFS Rules Based Stream Processing with Apache Flink\u0026rsquo;s Broadcast Pattern Advanced Flink Application Patterns Vol.1: Case Study of a Fraud Detection System Build a dynamic rules engine with Amazon Managed Service for Apache Flink Research Background We needed a case study on building a frequency-based detection policy using filtering conditions within a Window Time. Research was necessary on how to handle all policy processing within a single job code. Allocating instances per policy—even when using container virtualization—is resource inefficient. Policies are managed by admins and should not require job redeployment upon updates. Prerequisites Dynamic Partitioning Kafka uses the event key to hash and modulo into partitions.\nIn frameworks like Kafka Streams and Flink, using a non-key field for group by (keyBy) causes reshuffling, which involves cross-network data movement among Task Managers—posing a significant overhead.\nTo solve this, ensure that transactions with the same grouping key are handled by the same subtask.\nExample: Pre-partitioning by target key (e.g., ID or IP) via separate Kafka topics.\nTerminology (Abbreviated explanation of JobManager, TaskManager, SubTask, Broadcast, etc.)\nImplementation Strategy Merge action events with active policy events (via CDC from rule DB) and publish to a topic. If there’s 1 action and N active policies → publish N merged events. Use DynamicKeyFunction to dynamically partition source stream by group-by condition. Handles reshuffling dynamically without job redeployment. Existing keyBy still processed by current TaskSlots. In DynamicEvaluationFunction, evaluate whether each event satisfies a rule → emit restriction event if it does. Broadcast State Broadcast one stream to all parallel subtasks so they can share consistent configuration/rules.\nTypical use case: low-throughput control/config stream broadcasted to high-throughput action stream.\nBroadcast Architecture Source stream: Payment events Broadcast stream: Policy rules (with infinite retention) Merge both → evaluate Dynamic Data Partitioning Create a system that can add/remove rules at runtime without redeploying jobs.\nStatic vs. Dynamic Keys Type Static Key Dynamic Key Definition Pre-defined field Runtime-decided field Flexibility Low High Implementation Simple Complex (rule parsing required) Performance Optimized Slight overhead Example: If rules are grouped by id, all relevant events will go to the same subtask, even if the logic per rule differs.\nPolicies can share subtasks if their groupingKey is the same.\nRule Broadcasting Use a broadcast source (e.g., from a rule DB CDC topic) to continuously update the active rules.\nEach time a rule is added/modified, it is inserted into the broadcast state.\nIf rule.disabled = true, it is removed.\nCustom Window Processing Flink offers multiple window types: Tumbling, Sliding, Session.\nBut\u0026hellip; each has limitations for fraud detection.\nTumbling: May miss events at window boundaries. Sliding: Has inherent delay and overlapping evaluations. → Solution: Implement Custom Windowing using state and timestamps.\nEvents stored as:\nMapState\u0026lt;Long, Set\u0026lt;PaymentEvent\u0026gt;\u0026gt; windowState; Since the state backend is a key-value store, it doesn\u0026rsquo;t support list types directly. This means we need to iterate over all timestamps in the map to find valid entries\u0026hellip; More research is needed here, but since we’re only iterating timestamps (not full events), memory impact may be minimal — though CPU usage could be a concern depending on loop cost.\nConsiderations on Event Retention (TTL) How should we determine the retention period, i.e., the Time-To-Live (TTL) for each event?\nIn DynamicEvaluationFunction(), it is possible to receive payment events with the same key scope but evaluate them under different rules with different time windows.\nTherefore, at the time a rule is consumed from the Rule Stream (Broadcast Stream), we must update and store the longest rule duration for that key.\nExample: UpdateWidestWindow @Override public void processBroadcastElement(Rule rule, Context ctx, Collector\u0026lt;Alert\u0026gt; out) { ... updateWidestWindowRule(rule, broadcastState); } private void updateWidestWindowRule(Rule rule, BroadcastState\u0026lt;Integer, Rule\u0026gt; broadcastState) { Rule widestWindowRule = broadcastState.get(WIDEST_RULE_KEY); if (widestWindowRule == null) { broadcastState.put(WIDEST_RULE_KEY, rule); return; } if (widestWindowRule.getWindowMillis() \u0026lt; rule.getWindowMillis()) { broadcastState.put(WIDEST_RULE_KEY, rule); } } In summary, Dynamic Evaluation uses the rule with the longest duration to determine the TTL for the event.\nSince the state backend is a key-value store, it doesn\u0026rsquo;t support list types directly.\nThis means we need to iterate over all timestamps in the map to find valid entries…\nMore research is needed here, but since we’re only iterating timestamps (not full events), memory impact may be minimal — though CPU usage could be a concern depending on loop cost.\n","permalink":"https://dingyu.dev/en/posts/flink-dynamic-job/","summary":"Do we really need to redeploy the job every time the policy changes? A deep dive into executing Flink jobs dynamically.","title":"[EDA] Flink Dynamic Job Case Study"},{"content":"MSA Requirements for a Go App Configuration management, graceful shutdown Testable code API specifications Logging Profiling, error monitoring, metrics, API tracing Tiny Main Abstraction Instead of using separate .env files, environment variables are read directly from the OS. A question raised: isn\u0026rsquo;t setting every OS argument as a flag wasteful? Graceful Shutdown Testable Code The speaker waits for the server startup with long polling before running tests—not the cleanest method. Since the speaker is focused on vanilla Go, it seems they deliberately chose not to use the httptest package. Health Check Version is set using ldflags, which could be automatically tagged during release. Storing server start time is preferred over tracking total uptime. Documentation is a Must As Gophers say, \u0026ldquo;GoDoc isn\u0026rsquo;t optional—it’s essential.\u0026rdquo;\nWhile the speaker didn\u0026rsquo;t use godoc, they emphasized exposing OpenAPI specs. go:embed embeds OpenAPI files into the binary at build time. Build fails if the file is missing. Swagger endpoints expose the embedded documentation. Maintaining Swagger alongside code updates may not be sustainable. Using swaggo might be more intuitive.\nLogging is a Must JSON logging with slog is adopted—a must for modern apps.\nFollowing the 12-factor app philosophy, logs are written to stdout. This reduces File I/O costs.\nFluentbit handles post-processing with multiple outputs:\nSentry: Error tracking Jaeger: API tracing Elasticsearch: Log storage for Kibana Decorate Decorates the response writer to track HTTP status codes and byte sizes. Reflections This approach contrasts with rigid project structures. The minimal Go style without strict clean architecture was convincing. Some trade-offs are inevitable. Even when using established frameworks, clean and concise code is still achievable. Fluentbit should be adopted company-wide to reduce log coupling in the app. Company-wide tracing adoption (e.g., Jaeger, OpenTelemetry) is a must, potentially mandated by CTOs. Applying to Production // HealthHandler : Server health status handler type HealthHandler struct { version string startTime time.Time } // NewHealthHandler : Creates a new HealthHandler func NewHealthHandler(version string) HealthHandler { return HealthHandler{ version: version, startTime: time.Now(), } } // Check : Returns server status and build metadata func (h HealthHandler) Check(ctx *gin.Context) { type responseBody struct { Version string `json:\u0026#34;version\u0026#34;` Uptime string `json:\u0026#34;up_time\u0026#34;` LastCommitHash string `json:\u0026#34;last_commit_hash\u0026#34;` LastCommitTime time.Time `json:\u0026#34;last_commit_time\u0026#34;` DirtyBuild bool `json:\u0026#34;dirty_build\u0026#34;` } var ( lastCommitHash string lastCommitTime time.Time dirtyBuild bool ) { buildInfo, _ := debug.ReadBuildInfo() for _, kv := range buildInfo.Settings { if kv.Value == \u0026#34;\u0026#34; { continue } switch kv.Key { case \u0026#34;vcs.revision\u0026#34;: lastCommitHash = kv.Value case \u0026#34;vcs.time\u0026#34;: lastCommitTime, _ = time.Parse(time.RFC3339, kv.Value) case \u0026#34;vcs.modified\u0026#34;: dirtyBuild = kv.Value == \u0026#34;true\u0026#34; } } } up := time.Now() ctx.JSON(http.StatusOK, responseBody{ Version: h.version, Uptime: up.Sub(h.startTime).String(), LastCommitHash: lastCommitHash, LastCommitTime: lastCommitTime, DirtyBuild: dirtyBuild, }) } ","permalink":"https://dingyu.dev/en/posts/gopher-con-2024-minimalistic-go/","summary":"No more unnecessary frameworks—pure Go is the ultimate form of tuning. Learn how to build minimalistic applications with vanilla Go.","title":"[Go] Gophercon 2024 - Building Minimalistic Backend Microservice in Go"},{"content":"[Go] Gophercon 2024 - Go Project Guide A-Z Small Projects Low traffic → Limited user base Simple functionality → Undefined user states in new services Need for rapid development → Early feature experimentation required for hypothesis validation For a Simple CLI Tool For a Simple API Server Take a minimalistic approach using standard libraries to build and expand APIs incrementally.\nFeature-Oriented Project Patterns Using Handler Struct (Class-based DI) Pass required parameters for each method via function inputs:\nInjecting Dependencies via HTTP HandlerFunc Use closures internally to capture dependencies. This design is clean, allows for scoped dependency injection, and is highly testable.\nType Advantages When to Use Handler - Easier management of complex state- Clearly structured dependencies- Easy to extend- Good for interface implementation- Helps organize complex logic Complex business logic, many dependencies, stateful apps HandlerFunc - Quick to write- Simple DI with closures- Functional composition- Ideal for prototyping- Easy mocking for testing Simple handlers, small apps, microservices Code Pattern for Enterprise-Scale Services Apply a Layered Architecture:\nLayer Breakdown Presenter = Converts between Domain Model and API Model Handler = Serves API Models Handles HTTP/gRPC requests and responses Clearly separates presentation from application logic Usecase = Executes business logic Depends on Services or Repositories Avoids using API or DB-specific models, focusing on domain logic (Service) = Implements complex business logic Extracted when Usecases become complex or involve multiple dependencies Usecases orchestrate; Services execute logic (similar to xxxExecutor pattern) Repository = Manages CRUD for domain models Encapsulates data access logic Inverts dependency: Repositories return domain models and depend on domain layer (Recorder) = Manages DB-specific models Used for handling NoSQL/RDBMS-specific logic Enables DB migrations by swapping out recorder DI Test Code per Layer Use the mocking library counterfeiter.\nFeature Go Mockery Counterfeiter Mock Creation Easy auto-generation with args Fake objects allow for advanced simulation Setup Ease Simple and intuitive Ideal for complex test scenarios Test Maintainability Best for simple interfaces Long-term test stability with fake objects Best Use Case Isolated module testing Complex logic and varied response scenarios Fake mocks are usually stored within the same layer they belong to.\nClosing Thoughts While Go\u0026rsquo;s ecosystem has experimented with many structural approaches, no clear standard has emerged. This pattern seems highly practical. Concern: Does a layered architecture cause unnecessary data structure conversions? (e.g., Presentation, Business, Data layers could each use separate structs for a single API) Service vs. Usecase? Seems to hinge on whether multiple services are involved. On APM: What about cost? → APM isn\u0026rsquo;t applied globally. It is only used for business-critical logic. ","permalink":"https://dingyu.dev/en/posts/gopher-con-2024-go-project-guide/","summary":"How to structure projects in Go, a language without rigid frameworks like Spring. This post introduces practical patterns for feature-driven development and enterprise-level application design.","title":"[Go] Gophercon 2024 - Go Project Guide from A to Z"},{"content":"Kubernetes API An interface to interact with the Kubernetes platform. Used by users, administrators, and applications to manage clusters and resources. Enables a series of operations such as listing or creating resources. Supports application deployment and status monitoring. Provided as an HTTP API, compatible with various languages and tools. https://\u0026lt;APISERVER\u0026gt;/\u0026lt;GROUP\u0026gt;/\u0026lt;VERSION\u0026gt;/\u0026lt;RESOURCE\u0026gt;/\u0026lt;NAME\u0026gt;\nAPI SERVER: The address of the API server GROUP: /api: Core API group (e.g., Pod, Service) /apis/*: Extended API group for deployment and management features VERSION: e.g., v1 (stable), v1beta, v1alpha1 RESOURCE: The type of resource (e.g., /pods, /services) \u0026lt;NAMESPACE\u0026gt;/\u0026lt;RESOURCE\u0026gt;: Namespace-specific queries NAME: The specific name of the target resource client-go Library Abstracts Kubernetes API calls into convenient Go functions.\nAccess both core and extended resource groups.\nSupports initialization inside/outside the cluster.\nProvides Informers to cache resource changes.\nQuerying Pods(\u0026quot;\u0026quot;) allows access to all namespaces (within permission scope).\nWhen Updating a Deployment Concurrently The first request that reaches the server is accepted, others fail. Similar to MVCC: version conflict detection prevents lost updates. Simultaneous Image Change from Different Servers One request will succeed; the rest will fail with a Conflict error. Optimistic Concurrency Step 1 Step 2 Step 3 Submit update with resource version Apply if version matches stored version Fail on version mismatch Use retry.RetryOnConflict() to automatically retry on version conflicts.\nControllers and Operators in Kubernetes Controller:\nManages native Kubernetes resources (Pods, Deployments, Services). Operator:\nManages complex state via custom resources. Automates application deployment and management. kubebuilder Framework Go-based framework to develop Kubernetes controllers/operators. Generates boilerplate for CRD management and reconciliation. Helps focus only on core business logic. Alternative to client-go (which is more complex). Component Overview:\nProcess: Application configuration/init\nManager: API server communication, event caching, metrics\nWebhook: Initialization and validation logic\nController: Filters resource events and triggers Reconciler\nReconciler: Implements logic for managing custom resources\nSPEC: Desired state definition\nSTATUS: Current state reporting\nController Option Configuration Settings for the Reconcile() function triggered on resource changes.\nIncludes filters, performance settings, resource scopes.\nGeneration field can be used to only trigger reconciliation on spec changes.\nConfigure max concurrent reconciliations via controller options.\nWhen is Reconcile() Triggered? On operator startup (traverses all resources) On resource updates Default: any field change Customizable to spec-only changes Takeaways While CPU and memory-based autoscaling is standard, customizing based on application nature can improve performance. e.g., for queue-based workers, scale via queue length. Conflict errors during concurrent deployment can be mitigated using custom controllers. Instead of manually triggering deployment via Rundeck, a DaemonSet-based operator could handle automation (with caution). ","permalink":"https://dingyu.dev/en/posts/gopher-con-2024-kubernetes-programing/","summary":"How to use the Kubernetes API with Go… and deploy applications using Operators","title":"[Go] Gophercon 2024 - Kubernetes Platform Programming"},{"content":"\nFor how to apply pprof, refer to Tuning GC with pprof\nBackground Our service server was performing one HTTP request per event—triggered frequently and handled asynchronously. However, the more requests we sent, the more linear the delay became.\nProblems Excessive network requests on every event Increased latency due to queuing Lack of scalability — implementation differences between services caused repeated maintenance issues Architecture Improvement Solutions Queue events locally and send batch requests when either buffer size or interval is exceeded. Reduce latency via batching. Build a common library with loose coupling between service servers and the batch loader. Design Decisions We decided to build a shared module called Batch Processor to manage event queues.\nRequirements Optimize for IO-bound tasks Manage goroutine lifecycle cleanly Option 1: Worker Pool (Sync IO) Pros Avoids deep copy overhead Tunable performance via pool size Cons Potentially multiple HTTP requests per interval Hard to tune optimal pool size for every service Option 2: Single Worker + Async HTTP Pros Only one HTTP request per interval Simpler integration without tuning Cons Minor CPU/memory overhead from deep copy GC pressure may increase due to heap allocations Given the trade-offs, we opted for Option 2, but needed to validate that deep copy overhead wouldn’t impact performance.\nProfiling Goals Measure throughput at 2000 RPS Quantify memory impact of deepCopy() Analyze GC overhead from buffer copies func deepCopy[T any](src []T) []T { if src == nil { return nil } dst := make([]T, len(src)) copy(dst, src) return dst } Methodology Compare 3 implementations:\n10 Workers + Sync IO 100 Workers + Sync IO 1 Worker + Async IO Track:\nThroughput via logs Heap profile with: curl {endpoint}/debug/pprof/heap?seconds=30 --output {output_path} Log parsing example:\n2024-10-14T05:11:06Z INF count=1020 2024-10-14T05:11:07Z INF count=1000 2024-10-14T05:11:07Z INF stopping BatchProcessor... CPU Profiling 100 Workers + Sync IO 85% of time spent on sellock and acquireSudog High contention on channel access 10 Workers + Sync IO Lock contention reduced to 66% 1 Worker + Async IO Deep copy overhead ~10ms 50% time split between API calls and deep copy Heap Profiling Worker Pool ~8.2MB total, ~7.7MB from HTTP No deep copy impact 1 Worker + Async IO ~12.2MB total, ~11.4MB from HTTP Deep copy impact: 150kB (~1.22%) — negligible Results Setup Throughput/min CPU Overhead Memory Overhead 10 Workers 83,663 66% 0% 100 Workers 84,042 85% 0% 1 Worker + Async 119,720 50% 1.22% Conclusion Worker pools introduce significant concurrency overhead Increasing worker count doesn’t scale linearly Async execution outperforms worker pools for IO-bound tasks Worker pools remain ideal for CPU-bound tasks When order matters, prefer worker pool even for IO-bound jobs ","permalink":"https://dingyu.dev/en/posts/worker-pool-async/","summary":"Is it better to distribute tasks across multiple workers or handle each task asynchronously? Let’s find out through performance profiling.","title":"[Go] Profiling Worker Pool vs. Async Processing"},{"content":" Source Code: https://github.com/dings-things/coffee_pal\nPurpose Great cultures make great organizations — not just getting things done, but doing them well. Good culture enhances productivity and quality alike.\nI created “CoffeePal” with the idea of helping people stuck on problems or simply wanting a quick small talk boost. The name reflects its purpose: your “coffee pal” during the workday.\nBefore Development The app needed to be easy and fast, seamlessly matching users via our internal Slack tool.\nTo understand the interaction flow, I started with Slack Workflow.\nCreate a Trigger Point\nFor testing, I used a :coffee: emoji reaction to initiate the workflow.\nActivate Workflow\nA message is sent to the reacting user to confirm whether they want to start a coffee chat.\nCollect User Information via Form\nFor personalization, users can optionally input their MBTI and birthday.\nSet Target, Time, and Topic\nDefine the coffee partner, schedule, and discussion topic ahead of time.\nSend Invitation Message to Partner\nUsing the collected data, send an invitation message to the designated partner.\nI used Slack Workflow for the first time and found the UI clean and intuitive.\nHowever, it felt more like a one-way communication than true interaction. So, I decided to expand it into a full Slack app.\nIt felt like being force-fed food I didn’t want\u0026hellip;\nRequirements Match users based on pre-saved profile data\nTrigger matching after personal data is submitted Secure compute resources to handle app interactions\nInternal Server Firewall complexity Fast response via socket communication AWS Lambda Free under Free Tier Serverless Can connect with API Gateway Consider future cost after Free Tier Intuitive UI for users at a glance\nUse Slack Block Kit Implementation Using Lambda Create Slack App Set up Lambda Configure Socket Mode Step 1: Create Slack App Go to Slack API and create a new app\nStep 2: Click From scratch Step 3: Choose workspace and name Step 4: Set bot for user interaction Step 5: Enable events Lambda Step 1: Go to Lambda Console Lambda Step 2: Configure function, runtime, architecture Lambda Step 3: Set up API Gateway API Gateway routes HTTP requests to Lambda — essential to integrate Slack Events API with Lambda.\nBut then I asked myself\u0026hellip; If we already have a server, why use Lambda? Wouldn’t not using it be wasteful?\nSo I decided to use our spare dev server and implement it via socket mode.\nSocket Mode Configuration What is Socket Mode?\nInstead of sending payloads to public HTTP endpoints, Slack uses a WebSocket connection to deliver app interactions and events.\nAvoiding public endpoints means better security, and you don\u0026rsquo;t need to subscribe to each event individually.\nStep 1: Generate App-Level Token Grant permissions to a global token for Slack app control inside the server.\nStep 2: Set Token Scope channels:read: Get user info from channels chat:write: Post messages to channels chat:write.public: Write to channels even if not invited groups:read: Access private channel info groups:write: Write in private channels im:write: Send DMs mpim:write: Send group DMs reminders:write: Set reminders Development Slack Bolt A framework for building Slack apps, supporting languages like Node.js, Python, and JavaScript.\nWhile you might consider skipping it… using Bolt avoids a lot of redundant work. Skipping it results in slower development and difficult maintenance.\nWithout Bolt: from slack_sdk import WebClient from slack_sdk.errors import SlackApiError import os, json, base64, urllib.parse def lambda_handler(event, context): bot_token = os.getenv(\u0026#39;BOT_KEY\u0026#39;) client = WebClient(token=bot_token) if event.get(\u0026#39;isBase64Encoded\u0026#39;, False): decoded_body = base64.b64decode(event[\u0026#39;body\u0026#39;]).decode(\u0026#39;utf-8\u0026#39;) event_body = urllib.parse.unquote_plus(decoded_body)[8:] else: event_body = event[\u0026#39;body\u0026#39;] try: event_data = json.loads(event_body) except json.JSONDecodeError: return { \u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;message\u0026#39;: \u0026#39;Invalid request\u0026#39;}) } event_type = event_data.get(\u0026#39;event\u0026#39;, {}).get(\u0026#39;type\u0026#39;) # handle event_type... With Bolt: app = App(token=settings.SLACK_BOT_TOKEN) @app.event(\u0026#34;app_home_opened\u0026#34;) def handle_home(event, client: WebClient = None): ... @app.action(\u0026#34;suggest_coffee_chat_button\u0026#34;) def handle_chat_button(ack, body, client: WebClient = None): ... Bolt simplifies:\nAuth Routing by event/action/view Middleware chaining Block Kit Block Kit is at the heart of Slack app UI design.\nEach visual unit is a \u0026ldquo;block\u0026rdquo;. You compose a full view by stacking them.\nHOME_VIEW = { \u0026#34;type\u0026#34;: \u0026#34;home\u0026#34;, \u0026#34;blocks\u0026#34;: [ ... ] } HOME layout Horizontal Section Vertical Section Input Slack API is stateless. Every request is isolated, so inputs are critical for maintaining interaction context.\nEven if the server disconnects, the state can be restored based on input fields.\nRandom Coffee Chat example Improvements Use Slack DataStore for stateful user data with TTLs Integrate HR info (rank/age/role) to personalize matches Improve UI for clarity and friendliness Add CI/CD and infra for production usage Final Thoughts As a developer, I resonate with the Ubuntu philosophy:\n“I am because we are.”\nFostering collaboration is just as important as individual contribution.\nThat’s why I built CoffeePal ☕\nIf you hit technical roadblocks or need an ice-breaker for teamwork, consider building your own Slack bot!\n","permalink":"https://dingyu.dev/en/posts/coffee-pal/","summary":"I’m a developer, chatting on Slack with the person next to me… Let’s become friends!","title":"[DX] Building a Slack Bot for Internal Coffee Chats"},{"content":"Applying pprof import \u0026#34;github.com/gin-contrib/pprof\u0026#34; // initialize server var ( router *gin.Engine ) { router = gin.New() // register pprof routes pprof.Register(router, APIPrefix+\u0026#34;/debug/pprof\u0026#34;) } Script echo \u0026#34;User API v1.2.3\u0026#34; # start profiling curl https://{endpoint}/debug/pprof/trace?seconds=10 --output v1.2.3-trace.out \u0026amp; curl https://{endpoint}/debug/pprof/heap?seconds=10 --output v1.2.3-heap.prof \u0026amp; curl https://{endpoint}/debug/pprof/profile?seconds=10 --output v1.2.3-cpu.prof \u0026amp; # Run load test while profiling bombardier -t 1s -l -c 30 -d 10s \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -m GET https://{endpoint}/v1/users/51387659 wait # view pprof output locally go tool trace -http 127.0.0.1:9094 v1.2.3-trace.out \u0026amp; go tool pprof -http 127.0.0.1:9095 v1.2.3-heap.prof \u0026amp; go tool pprof -http 127.0.0.1:9096 v1.2.3-cpu.prof \u0026amp; Note: Bash \u0026amp; runs commands asynchronously.\npprof Usage Steps Start the application with pprof routes registered. If the app isn’t under load, run a load test on a target API. Use curl to trigger profiling. If you’re behind a Load Balancer, ensure profiling completes within the LB timeout.\nHeap Profiling Metrics inuse_objects: number of currently used objects inuse_space: amount of memory currently in use alloc_objects: total number of objects ever allocated alloc_space: total memory ever allocated Notes on Inuse vs Alloc Same Inuse/Alloc (e.g. 1GB/1GB): object was allocated once and reused — no GC triggered, CPU efficient. Alloc much larger than Inuse (e.g. 1TB/1GB): indicates high churn and frequent GC — may cause Stop-The-World (STW) pauses. Tuning Opportunities Flame Graph Flame Graphs help distinguish between user-defined logic and framework/internal code like ServeHTTP.\nThe wider the bar, the more allocations are made.\nContrary to expectations, Redis accounted for only 7.9% of allocations. Setting context values consumed 13.9% of heap allocations. Each context Set allocates a new map if the key doesn\u0026rsquo;t exist. Since no capacity is preallocated, this results in additional memory usage.\nIf the overhead for grouping logs by request ID is too high, consider removing it.\nAllocation Graph Larger rectangles = higher memory usage. Prioritize tuning large allocations with high % usage. Zerolog Overhead Despite using a shared Logger instance, allocations still occurred due to the use of With().Str().Logger() on every request.\nvar logEntry zerolog.Logger = h.logger.With(). Str(\u0026#34;trace_id\u0026#34;, ctx.GetString(RequestIDContextKey)).Logger() Although cleaner for readability, this approach allocates new memory on each invocation.\nSolution: Accept the readability tradeoff or switch logging frameworks.\nHowever, other loggers may allocate even more due to interface marshalling. Always benchmark alternatives.\nGC Tuning Goal: minimize STW (Stop-The-World) latency caused by excessive GC.\nUsing GOMEMLIMIT GOGC uses heap growth rate to trigger GC. Default GOGC = 100 → GC triggers when heap doubles. Tuning Based on Program Behavior Low GOGC → GC runs too often, especially early on when heap is small. High GOGC → Risk of OOM. E.g., with 40GB heap and GOGC=50, GC only runs after heap reaches 60GB. GOMEMLIMIT caps memory usage before GC triggers. Set it below peak usage to reduce GC frequency.\nGOMEMLIMIT is a soft limit — Go may allocate slightly more than configured.\nSuggested Workflow:\nStart with default GOGC. Measure memory usage. Set GOMEMLIMIT to ~80% of peak. Monitor GC behavior. Benchmarking Production Code Use go test -benchmem to evaluate memory allocations.\ngo test -bench=. -benchmem Example benchmark of StructuredLogger:\nBenchmarkStructuredZeroLoggerMessage-12 596910 1724 ns/op 1000 B/op 30 allocs/op BenchmarkStructuredZeroLoggerMessageWithFields-12 285715 3926 ns/op 1633 B/op 60 allocs/op BenchmarkStructuredZeroLoggerMessageWithFieldsWithContext-12 222223 5346 ns/op 3418 B/op 68 allocs/op BenchmarkStandardZeroLoggerMessage-12 11927823 90.17 ns/op 0 B/op 0 allocs/op BenchmarkStandardZeroLoggerMessageWithDeterminedFields-12 5649648 217.6 ns/op 0 B/op 0 allocs/op BenchmarkStandardZeroLoggerWithFields-12 300001 3894 ns/op 1553 B/op 59 allocs/op Even with ZeroLogger, repeated use of WithFields leads to significant allocations.\nUse Case Driven Pointer Usage While pointers are efficient due to passing references instead of values, Go may allocate them on the heap, unlike value types which may reside on the stack.\nWhen to Use Pointers Large structs: avoid copying Mutability: need to modify original value Consistency: stick to pointer receivers for uniformity Nullable fields: to distinguish between zero value and nil Ref: https://articles.wesionary.team/use-case-of-pointers-in-go-w-practical-example-heap-stack-pointer-receiver-60b8950473da\nPreallocate Slice Capacity Without preallocating, append() causes reallocation on each operation.\nGo can optimize stack allocation for small-sized slices:\nMaxStackVarSize: up to 10MB for explicitly declared vars MaxImplicitStackVarSize: up to 64KB for implicit vars MaxSmallArraySize: arrays \u0026lt;= 256 bytes are stack-allocated Ref: https://go.dev/src/cmd/compile/internal/ir/cfg.go\n","permalink":"https://dingyu.dev/en/posts/go-pprof-gc/","summary":"Ever heard of \u0026ldquo;many a mickle makes a muckle\u0026rdquo;? By identifying bottlenecks and minimizing memory allocations, you can reduce GC overhead and build highly optimized applications!","title":"[Go] Tuning GC with pprof"},{"content":"Why Sentry? Traditional log monitoring has limitations such as: Elasticsearch field mapping errors causing dropped log events Need to request system logs from SEs to retrieve stack traces Sentry provides a centralized, visual, and automated error tracking and performance monitoring solution.\nWhat Sentry Provides Exception Capture: Automatically detects and logs application errors with stack trace and context. Transaction Monitoring: Tracks API/web request performance metrics. Tracing: Visualizes end-to-end latency across services and components. Alerting: Integrates with tools like Slack, email to notify devs of issues. Release Tracking: Monitors how errors correlate to new application releases. Alert Rule Examples Issues: Trigger alerts based on type of exception or service. Number of Errors: Trigger when a particular error exceeds a threshold. Users Experiencing Errors: Notify when errors affect multiple users, e.g. 100 users hitting login errors. Client Configuration DSN: Unique key per project for event ingestion. [Settings → Projects → Client Keys] Environment: Differentiate between production, staging, etc. Release: Tag errors by release version. SampleRate / TracesSampleRate: Control event sampling to optimize cost and performance. BeforeSend: Hook to redact sensitive data or conditionally send events. AttachStacktrace: Attach stack traces even for non-errors. ServerName: Identify source server. Transport: Custom HTTP config for timeouts and retries. Init Best Practice err := sentry.Init(sentry.ClientOptions{ Dsn: config.Sentry.DSN, SampleRate: config.Sentry.SampleRate, EnableTracing: config.Sentry.EnableTrace, Debug: config.Sentry.Debug, TracesSampleRate: config.Sentry.TracesSampleRate, Environment: config.Sentry.Environment, AttachStacktrace: true, Transport: \u0026amp;sentry.HTTPSyncTransport{ Timeout: config.Sentry.Timeout }, }) Error Capturing Capture with Context hub := sentry.GetHubFromContext(ctx) if hub == nil { hub = sentry.CurrentHub().Clone() ctx = sentry.SetHubOnContext(ctx, hub) } hub.CaptureException(err) Standard vs. pkg/errors errors.New: no stack trace pkg/errors.Wrap: includes full stack trace Contextual Scopes hub := sentry.GetHubFromContext(r.Context()) if hub == nil { hub = sentry.CurrentHub().Clone() r = r.WithContext(sentry.SetHubOnContext(r.Context(), hub)) } hub.Scope().SetRequest(r) Singleton Initialization type SentryInitializer struct { conf *Config enabled bool mutex sync.RWMutex } func (si *SentryInitializer) Init() error { si.mutex.Lock() defer si.mutex.Unlock() err := sentry.Init(...) si.enabled = true return err } Capturing via Interface type ErrorCapturer interface { CaptureError(ctx context.Context, err error) } func (ec *sentryErrorCapturer) CaptureError(ctx context.Context, err error) { if err == nil { return } if !ec.Enabled() { ec.Init() } hub := sentry.GetHubFromContext(ctx) if hub == nil { hub = sentry.CurrentHub().Clone() ctx = sentry.SetHubOnContext(ctx, hub) } hub.CaptureException(err) } Panic Recovery Middleware func (rm *RecoverMiddleware) Register(h http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { hub := sentry.GetHubFromContext(r.Context()) if hub == nil { hub = sentry.CurrentHub().Clone() r = r.WithContext(sentry.SetHubOnContext(r.Context(), hub)) } hub.Scope().SetRequest(r) defer func() { if err := recover(); err != nil { hub.RecoverWithContext(r.Context(), err) // log stack trace, respond 500 } }() h.ServeHTTP(w, r) }) } Summary Sentry simplifies APM and error tracking for Go applications:\nAutomatic stack trace Release correlation Slack/email alerts Low overhead It’s a powerful ally for ensuring observability and reliability in production.\n","permalink":"https://dingyu.dev/en/posts/sentry/","summary":"A tool that supports both APM and error traceability?","title":"[Third-Party] Integrating Sentry in Go"},{"content":"Background During load testing in a Kubernetes environment, intermittent 502/504 errors were observed. Pods were terminated before they could complete serving responses → 504 Gateway Timeout New Pods were created and started serving traffic before they were fully ready → 502 Bad Gateway Rolling updates proceed smoothly only if Readiness Probes are correctly configured.\nSetup Load Testing Tools bombardier: Simple Go CLI load testing tool vegeta: Flexible script-based HTTP load testing Readiness Probe Mechanism to determine if a Pod is ready to accept traffic\nIf a container is not ready (e.g. app not fully initialized), it should not receive traffic.\nWithout a readiness probe, incoming traffic may reach Pods before the application is ready, resulting in 502 errors.\nExample Deployment Snippet readinessProbe: httpGet: port: 8080 path: /alive scheme: HTTP initialDelaySeconds: 30 periodSeconds: 30 Test Output bombardier -c 200 -d 3m -l https://{endpoint} Result (simplified):\nHTTP codes: 4xx - 753060, 5xx - 12 5XX errors still present.\nlifecycle \u0026amp; preStop Hook Used to execute a shutdown script before the container terminates.\nThis enables graceful shutdown: disconnect service → finish pending requests → terminate.\nExample Deployment Snippet lifecycle: preStop: exec: command: - /bin/sh - -c - sleep 40 This introduces a 40s delay before actual container shutdown.\nTest Output bombardier -c 200 -d 3m -l https://{endpoint} Result (simplified):\nHTTP codes: 2xx - 751239, 5xx - 3 Still not perfect.\nterminationGracePeriodSeconds Time Kubernetes waits for a Pod to shut down before forcefully terminating with SIGKILL.\nDefault is 30 seconds, which may be shorter than your preStop delay.\nExample Deployment Snippet terminationGracePeriodSeconds: 50 Ensure the following relationship: preStop (40s) \u0026lt; terminationGracePeriodSeconds (50s) \u0026lt; ALB timeout (60s)\nTest Output bombardier -c 200 -d 3m -l https://{endpoint} Result:\nHTTP codes: 2xx - 770240, 5xx - 0 🎉 No more 5xx errors!\nReferences Kakao Tech - Zero Downtime Deployment in Kubernetes Kubernetes Pod Lifecycle Docs ","permalink":"https://dingyu.dev/en/posts/k8s-zero-downtime/","summary":"How can we prevent elusive 502 and 504 errors during rolling updates in Kubernetes?","title":"[Infra] Zero-Downtime Kubernetes Deployment Guide"},{"content":"AWS Architected Best Practice Well-Architected Can you confidently answer these questions when reviewing your team’s systems or applications? Sometimes you design a system, but you’re unsure if it’s well-designed — that’s where this knowledge comes in handy.\nWhat defines a good design? Core elements Benefits General design principles Cost Performance CPU/memory configuration based on RPS (Requests Per Second) Operational Excellence Security Reliability Performance efficiency Cost optimization Sustainability Design Principles Faster development and deployment Risk mitigation or reduction Is it safe to open port 22 on a server? Should we rely on a Bash script or online guides to open well-known ports? With AWS Systems Manager Session Manager, you can access servers without opening port 22 SCALE UP / SCALE OUT SCALE UP: Vertical scaling — upgrading hardware specs like CPU/memory (UP/DOWN) SCALE OUT: Horizontal scaling — adding more servers with the same specifications (IN/OUT) Best Practices for EC2 Infrastructure Perspective Estimate capacity first → this leads to cost estimation Automate response to security events: trigger automatic actions based on event or condition-based alerts Operational Excellence Core focus: How the organization supports its business objectives Effectively running workloads, gaining operational insights, and continuously improving processes and procedures to deliver business value Design Principles Over-provisioning = waste (based on peak traffic estimates) Under-provisioning = overload risk Test systems at production scale Use identical environments for testing to ensure stability Since it\u0026rsquo;s cloud-based, you can terminate unused resources (e.g., Blue/Green deployment) Architecture experimentation becomes easier with automation Enabling Innovative Architectures MSA (Microservices) ↔ Monolith Use PoCs to identify better migration strategies — don’t get stuck Data-driven architecture Don’t rely on gut feeling; base decisions on data Improvements Through Real-World Testing Failing to prepare for failure can lead to expensive recoveries Human resources should be included as part of the workload Infrastructure as Code (IaC) Manage infrastructure using code\nMake small, reversible changes frequently Like Merge Requests Build CI/CD workflows using CodePipeline Canary Deployment: Deploy to a subset of servers, test and monitor, then roll out to the rest Re-defining Operational Procedures Frequently Opening SSH = opening port 22 Opening well-known ports increases the risk of hacking attempts AWS Session Manager allows browser-based management You can perform prompt-based operations just like with SSH Failure Prediction and Response Build failure-tolerant architectures Perform health checks before routing traffic If a health check fails, isolate the traffic → prevent failure propagation e.g., Case Study: Any Company\n[Issue]\nWhen Availability Zone A goes down, all services go down\n→ Split services across multiple Availability Zones\nInstalling a database directly on EC2 increases the management burden\n→ Use redundancy and dedicated instances to ensure high availability\n[Best Practice Review]\nMost operations were performed manually Product catalog application needed a highly available architecture Security was the top priority Database Replication Use Active/Standby setup Data is synchronized in real-time RPO (Recovery Point Objective): How often data is backed up — RPO can be zero Reliability Key Elements Recover from infrastructure or service failures Dynamically acquire computing resources based on demand Mitigate interruptions due to misconfigurations or temporary network issues Auto Recovery From Failures Use Rolling, Canary, or Blue-Green deployment strategies Configure Auto Scaling with min:max:desired capacity settings Horizontal Scaling Scale across multiple Availability Zones Elastic Load Balancer adds capacity automatically when traffic increases (Auto Scaling) Security Security involves protecting your data, systems, and assets using cloud technology and improving your overall security posture.\nMultiple teams sharing a single account\nUse separate AWS accounts for each function\nApply Security at Every Layer Strong Identity and Access Management Like Git commits, changes can be tracked similarly to ChangeLogs — know when and how changes occurred Protect Data In Transit and At Rest Encryption: Amazon Macie For key management, use AWS KMS for common resources, and AWS CloudHSM for team-specific keys Cost Optimization Continuously monitor usage and operate systems that deliver business value at the lowest possible cost Pay-as-you-go architecture Minimize resources for development/test environments Efficient Cost Management Measure value as workloads evolve Consider switching to serverless architectures Use tags for cost tracking: Identify where usage or costs are increasing ","permalink":"https://dingyu.dev/en/posts/aws-well-architected/","summary":"The more you know, the more value you get from AWS. It can be a money pit, but also a highly efficient tool depending on how you use it. Let\u0026rsquo;s explore the best practices for AWS infrastructure design.","title":"[Infra] AWS Well Architected"},{"content":"Purpose Unlike traditional RDBMS, which implement transactions through isolation levels and mechanisms like rollback and commit, Redis has no native transactional locking system. This post discusses how to isolate a transaction and preserve all-or-nothing atomicity in Redis using available tools.\nOptions Redis TxPipeline Lua Script Redis TxPipeline A natural first approach is to use Pipelines, which send multiple commands in batch. However, Pipelines alone don’t ensure atomicity—commands can still be interleaved with others from different clients.\nEdge Cases Network Delay: Commands and responses may arrive out of sync. Multithreaded Clients: Multiple clients using pipelines simultaneously can cause inconsistent ordering. Replica Configuration: Data consistency may not be guaranteed when replication settings like slaveof are active. This is where TxPipeline shines.\nPros Atomic batch execution via MULTI/EXEC. Improved performance by reducing round trips. Transactional consistency when used with WATCH. Cons Memory overhead: All commands and responses are buffered. Complexity: Mixed usage with normal pipelines may cause unintended behavior. Testing Example MULTI SET key1 value1 SET key2 value2 SET key3 value3 EXEC If SET key2 value2 fails:\nOOM: Out of memory WRONGTYPE: Type mismatch on key Even with WATCH, if the connection drops mid-transaction, rollback is not guaranteed. TxPipeline ensures consistency, but not full rollback support.\nLua Script Use Lua to execute multiple Redis commands atomically.\nEdge Cases Script transmission failure due to network issues. Post-execution result fetch failures on the client side. Pros Lightweight and fast Atomic: Entire script runs as one unit Embedded scripting for customization Cons Smaller ecosystem Fewer data types Tougher concurrency and threading Testing Scripts cannot be rolled back either. Worst case? Network cut-off during the process.\nluaScript := ` for i = 1, #KEYS do if KEYS[i] == \u0026#39;key3\u0026#39; then error(\u0026#39;error on key3\u0026#39;) else redis.call(\u0026#39;SET\u0026#39;, KEYS[i], ARGV[i]) end end ` Keys: key1 to key5 → key3 triggers error.\nSummary TxPipeline Lua Script Overview Redis-like syntax, optimistic locking via WATCH, consistent Full atomic block via Lua, less intuitive for some users Rollback ❌ No rollback ❌ No rollback Drawbacks Slightly slower than Lua, potential complexity Extra script management, steeper learning curve Both methods ensure data consistency, not rollback. Client-side validation is crucial. Network errors remain the weakest link. Benchmark Setting 1,000 keys:\nMethod Runs Avg Time (ms) Lua Script 1424 0.83 TxPipeline 460 2.56 Basic Pipeline 506 2.34 While not dramatic, Lua is slightly faster—but choose based on structure and consistency needs.\n","permalink":"https://dingyu.dev/en/posts/redis-transaction/","summary":"How does Redis guarantee transactionality without traditional locks?","title":"[DB] Redis Transaction"},{"content":"About FastAPI Convention What is FastAPI? Advantages of FastAPI Dependency Injection Automatic Documentation Asynchronous Support Pydantic Model With the rise of cloud services, MSA (Microservice Architecture) has gained attention, leading to changes in server ecosystems. Now, stateless architectures using lightweight RESTful APIs have become mainstream, and FastAPI is optimized for such small, modular services.\nDependency: (If A must run after B, they are dependent) Depends is the core function that lowers coupling for tasks like authentication and DB connections. Auto Documentation: Documentation often feels like homework to developers. Unlike other frameworks, FastAPI automatically generates ReDoc and OpenAPI docs without additional dependencies, increasing developer productivity. Async Support: While Python is synchronous by default and its GIL (Global Interpreter Lock) discourages threading, FastAPI supports async operations out of the box. Pydantic: FastAPI loves Pydantic. It provides great features like serialization, type validation, and path variable parsing. Characteristics The biggest issue is the lack of a de facto standard. Unlike long-standing frameworks, FastAPI is relatively young and emphasizes flexibility and lightweight design, which naturally results in the absence of a consistent coding standard. While this freedom can be positive, it may also be seen as a lack of structure.\nPurpose Due to the absence of class-based structures and consistent design patterns, collaboration within teams often feels chaotic, even among teammates. The goal is to establish our own convention to ensure intuitive and maintainable project architecture.\nClass-Based Convention Current Issues Utility-dependent methods that lack structure Classes take on too many responsibilities (low cohesion) Non-intuitive structure Business logic is scattered, making code hard to read No consistent convention for Dataclass, Pydantic Models, etc. Requirements Project structure must be consistent and intuitive Each class should have a single responsibility Organize packages by business layer Convention Based on Requirements 1. Consistent and Intuitive Project Structure [FastAPI\u0026rsquo;s Suggested Project Structure]\n. ├── app │ ├── __init__.py │ ├── main.py │ ├── dependencies.py │ └── routers │ │ ├── __init__.py │ │ ├── items.py │ │ └── users.py │ └── internal │ ├── __init__.py │ └── admin.py [Proposed Project Structure]\nfastapi-project ├── app │ ├── worker │ │ ├── enums.py │ │ ├── models.py │ │ ├── dependencies.py │ │ ├── constants.py │ │ ├── exceptions.py │ │ └── utils.py │ ├── configs │ │ ├── config.py │ │ └── log_config.py │ ├── models.py │ ├── utils.py │ ├── exceptions.py │ ├── database.py │ └── main.py ├── aws │ ├── client.py │ ├── models.py │ ├── constants.py │ ├── exceptions.py │ └── utils.py ├── tests/ │ ├── domain │ └── aws ├── templates/ │ └── index.html ├── requirements │ ├── dev.txt │ ├── stg.txt │ └── prod.txt ├── .env └── .gitignore The root of all domain directories is app.\nmain.py initializes the FastAPI app (like src/ in other projects) controller: Module endpoints enums: Enum definitions models: Pydantic models entities: Entity models service: Business logic dependencies: Validation and injection logic constants: Internal constants config: Configuration files exceptions: Custom exceptions If multiple methods share similar concerns, group them into a dedicated package.\nExternal packages are maintained outside the app directory since they aren\u0026rsquo;t app-dependent.\n2. Each Class Should Have a Single Responsibility The author previously misunderstood cohesion. Grouping many methods with related but broad concerns into one class does not improve cohesion—it dilutes it.\nThis follows the SRP (Single Responsibility Principle) from SOLID design.\nCommon examples of so-called \u0026ldquo;GOD\u0026rdquo; classes include: XXXService, XXXClient, XXXHandler, XXXWorker\nEven I fell into this trap, grouping anything remotely related under one service. But this leads to poor readability and becomes a nightmare during unit testing.\nFor example:\nImplement a feature to write user logs into a txt file\n[Bad Example]\nclass UserService: def write_log_file(self, user_id: str) -\u0026gt; None [SRP Applied]\nclass UserLogWriter: def __init__(self, user: User): self.user = user def write(self) -\u0026gt; None While this is a simplified example, when services accumulate various unrelated features, it hurts readability and maintainability.\nTo be more Pythonic and adhere to OOP principles, I also refactored the router to be class-based.\nIt might seem unnecessary, but I aimed to manage controllers as containers using inheritance to reduce boilerplate.\nExample: [BaseController] [HelloController] 3. Organize by Business Layer If you’re building a CRUD app for a User domain, the structure might look like:\nfastapi-project ├── app │ ├── user_manager │ │ ├── user_getter.py │ │ ├── user_updater.py │ │ ├── user_creator.py │ │ ├── enums.py │ │ ├── models.py │ │ ├── entities.py │ │ ├── user_database.py │ │ ├── dependencies.py │ │ ├── constants.py │ │ ├── exceptions.py │ │ └── utils.py Want to retrieve a User entity from the DB? Just use UserGetter.get()—clean and predictable.\nEven if you later add a Manager layer using the Facade pattern, this structure still holds.\nWhat if model class names collide? Yes, of course. Entity and DTO classes often share names, so we separate them using namespaces:\nimport app.user_manager.entities as entity import app.user_manager.models as dto user_dto = dto.User user_entity = entity.User Design is more important than implementation. Even at the implementation level, clear conventions allow consistent class, sequence, and module diagrams to be built.\nThis convention isn\u0026rsquo;t a universal truth. But it\u0026rsquo;s a solid example of how one could be done.\n","permalink":"https://dingyu.dev/en/posts/fastapi-convention/","summary":"A Class-based FastAPI Structure Guide","title":"[Python] FastAPI Convention"},{"content":"Before We Begin Purpose Automating login steps is one of the most common challenges in data scraping from the web. Some portal sites implement Google Recaptcha to block Selenium-based automation, and this post aims to bypass that. A secondary goal is to gain a deeper understanding of how ChromeDriver works under Selenium. While this project uses a Chrome extension, many paid Recaptcha solvers rely on real humans to solve captchas. This post investigates why that is the case. Basic Setup 1. Setting up ChromeDriver Check the version of Chrome browser you are using: Settings → About Chrome Download the ChromeDriver version that matches your browser: https://chromedriver.chromium.org/downloads -\u0026gt; Download the driver version that supports your current Chrome version Or add as a dependency / NuGet package Automatically matching the ChromeDriver version with the installed browser version via code will be added in future enhancements.\n2. Download Buster: Recaptcha Solver Buster is a Chrome extension that solves Google\u0026rsquo;s Recaptcha using voice recognition and inputs the result automatically. Search \u0026ldquo;buster recaptcha\u0026rdquo; on Google and install the extension from the Chrome Web Store. You can configure the Speech service (Wit.ai by default and free to use). Google Speech-to-Text is also available and was reported in some articles to achieve a 97% pass rate. However, since Recaptcha is a Google service, it is likely that the same dataset was used to train the AI, and this speech service is paid, so we’ll skip it.\nUsing Wit.ai, Recaptcha could be solved in up to 8 retries. This will be an important issue explained further below.\n3. Extension Setup Selenium\u0026rsquo;s ChromeDriver() class provides an option called addExtension() for adding extensions.\naddExtension(String extensionPath) accepts the path to the .crx extension package file.\nIn Chrome, go to More Tools → Extensions or visit chrome://extensions Enable Developer Mode and remember the ID of the extension you want to use Unzip the extension and place it where it can be loaded by ChromeDriver (e.g., folder like 1.3.1_0) Typically located in Users-{Username}-AppData-Local-Google-Default-Extensions — find the folder using the extension ID. ! If AppData is hidden, manually enter \\AppData in the File Explorer path bar !\nAnalyzing Recaptcha on the Webpage via Browser Before writing any code, analyzing the webpage structure is crucial.\nWe\u0026rsquo;ll test with this URL: https://patrickhlauke.github.io/recaptcha/ — it always loads Recaptcha.\nPacket Monitoring with Fiddler If Recaptcha is enabled during login, the login page will return something like ex:needRecaptcha in the response. If needRecaptcha = true, Recaptcha solving becomes mandatory. A successful login POST request must include the Recaptcha token. Services offering Recaptcha API solve the captcha and return this token. Changes in the Browser First, focus on the top Recaptcha checkbox widget:\nIt includes a data-sitekey attribute — a unique identifier per site (site key) Recaptcha solve APIs use this sitekey to simulate solving and return tokens. Inside this div is an iframe, accessible using Selenium’s SwitchTo().Frame() From analysis, we found that Recaptcha success can be detected by checking if aria-checked=\u0026quot;false\u0026quot; becomes true This change is used as a checkpoint for scraping automation: This iframe also holds the Recaptcha token once solved Second, another iframe has the title recaptcha challenge expires in two minutes\nIf it’s your first captcha attempt, it might pass instantly. After repeated attempts, image/audio challenges appear here This is also where the Buster button lives Wit.ai often fails on the first few tries. You’ll need to hit the reload and Buster buttons until successful.\n! Google detects repetitive actions and may completely block Recaptcha for your IP. Hence, this is an experimental-only project !\nTo Code (C# Base) We design the flow based on the previous analysis:\nVisit login page → Check needRecaptcha = true? → Control browser with Selenium → Use Buster and reload button until checkbox is ticked → Extract token → Send POST login request\nWe’ll use MSTest as the testing framework.\nInstall Selenium.WebDriver via NuGet package\nCreate ChromeDriver\nChromeOptions _options = new ChromeOptions(); _options.AddExtension(@\u0026#34;...path_to_.crx\u0026#34;); ChromeDriver _driver = new ChromeDriver(_options); If using standalone ChromeDriver:\nChromeOptions _options = new ChromeOptions(); _options.AddExtension(@\u0026#34;...path_to_.crx\u0026#34;); ChromeDriver _driver = new ChromeDriver(@\u0026#34;...path_to_chromedriver_folder\u0026#34;, _options); Extra: Use Chrome Profile for ChromeDriver If you can access a non-sensitive user profile, you can reuse it:\nClick profile picture in Chrome Continue without sign-in Create new profile Profile path: Users-{User}-AppData-Local-Google-Profile{X} Download Buster extension into that profile Set profile via ChromeOptions: ChromeOptions _options = new ChromeOptions(); _options.AddArgument(\u0026#34;--user-data-dir=\u0026#34; + @\u0026#34;path_to_profile\u0026#34;); _options.AddArgument(\u0026#34;--profile-directory=ProfileNumber\u0026#34;); ChromeDriver _driver = new ChromeDriver(_options); Warning: If Chrome is already running with this profile, Selenium will fail to open the browser\nUtility methods to check Recaptcha state: public static bool IsChecked(ChromeDriver driver) { return driver.FindElement(By.Id(\u0026#34;recaptcha-anchor\u0026#34;)).GetAttribute(\u0026#34;aria-checked\u0026#34;) == \u0026#34;true\u0026#34;; } public static bool IsExistByCss(ChromeDriver driver, string cssQuery) { try { driver.FindElement(By.CssSelector(cssQuery)); } catch { return false; } return true; } Check if captcha exists: if (IsExistByCss(_driver, \u0026#34;iframe[title=\\\u0026#34;reCAPTCHA\\\u0026#34;]\u0026#34;)) Find two Recaptcha-related iframes: IWebElement first = _driver.FindElement(By.CssSelector(\u0026#34;iframe[title=\\\u0026#34;reCAPTCHA\\\u0026#34;]\u0026#34;)); IWebElement second = _driver.FindElement(By.CssSelector(\u0026#34;iframe[title=\\\u0026#34;reCAPTCHA challenge expires in two minutes\\\u0026#34;]\u0026#34;)); Switch to first iframe and click the checkbox: _driver.SwitchTo().Frame(first); var checkBox = _driver.FindElement(By.ClassName(\u0026#34;recaptcha-checkbox\u0026#34;)); _driver.ExecuteScript(\u0026#34;arguments[0].click()\u0026#34;, checkBox); Check if it passed: if (!IsChecked(_driver)) Retry with second iframe and click Buster button: _driver.SwitchTo().DefaultContent(); _driver.SwitchTo().Frame(second); _driver.Manage().Timeouts().ImplicitWait = TimeSpan.FromSeconds(200); var busterHolder = _driver.FindElement(By.ClassName(\u0026#34;help-button-holder\u0026#34;)); busterHolder.Click(); If not passed, refresh and try again: if (!IsChecked(_driver)) { _driver.SwitchTo().DefaultContent(); _driver.SwitchTo().Frame(second); _driver.FindElement(By.ClassName(\u0026#34;rc-button-image\u0026#34;)).Click(); _driver.FindElement(By.ClassName(\u0026#34;help-button-holder\u0026#34;)).Click(); } else { isChecked = true; break; } Extract token: if (isChecked) { _driver.SwitchTo().DefaultContent(); _driver.SwitchTo().Frame(second); _token = _driver.FindElement(By.Id(\u0026#34;recaptcha-token\u0026#34;)).GetAttribute(\u0026#34;value\u0026#34;); return _token; } Improvements Through this project, I came to appreciate Google\u0026rsquo;s anti-bot intelligence.\nIf I had used the paid Google Speech-to-Text, the success rate would’ve been higher even under heavier traffic. However, since we used free tools, Recaptcha eventually flagged the automation and blocked us.\nRecaptcha v3 is known to detect cursor movement and behavior patterns — even if this project is enhanced, v3 may still detect it.\nThat’s why many Recaptcha solving APIs hire real humans to manually solve captchas and return the token.\nKey improvement areas:\nChromeDriver version must be manually matched with Chrome browser version Heavy traffic and repeated failures lead to Recaptcha ban Login must complete within 2 minutes of solving the captcha ","permalink":"https://dingyu.dev/en/posts/crawling-selenium-solver/","summary":"Nothing can stop me! Having trouble with Recaptcha while crawling? Have you tried Buster?","title":"[Crawler] Recaptcha Solver"}]
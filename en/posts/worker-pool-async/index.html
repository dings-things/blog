<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[Go] Profiling Worker Pool vs. Async Processing | Ding's Coding Forge</title>
<meta name=keywords content="go,async,sync,pprof"><meta name=description content="This post explores profiling and optimizing worker pools vs. asynchronous execution in Go using pprof. It analyzes the performance impact of concurrent HTTP requests, comparing sync worker pools (10 vs. 100 workers) and a single async worker in terms of throughput, CPU overhead, and memory allocation. Profiling results reveal that worker pools suffer from high concurrency overhead, while asynchronous execution significantly improves throughput with minimal memory cost. Additionally, the post discusses when to use worker pools vs. async processing, highlighting key trade-offs for IO-bound vs. CPU-bound tasks."><meta name=author content="dingyu"><link rel=canonical href=https://dingyu.dev/en/posts/worker-pool-async/><meta name=google-site-verification content="8XY1hI6NVxQIrN7bQbnX-9TG9HHFw5HOQmlb6vcsFdQ"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.ba0de23ad40e17ca82720b577f8ae6ec11a26fb07407316cff70888e344ad129.css integrity="sha256-ug3iOtQOF8qCcgtXf4rm7BGib7B0BzFs/3CIjjRK0Sk=" rel="preload stylesheet" as=style><link rel=icon href=https://dingyu.dev/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://dingyu.dev/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://dingyu.dev/favicon-32x32.png><link rel=apple-touch-icon href=https://dingyu.dev/apple-touch-icon.png><link rel=mask-icon href=https://dingyu.dev/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=ko href=https://dingyu.dev/posts/worker-pool-async/><link rel=alternate hreflang=en href=https://dingyu.dev/en/posts/worker-pool-async/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-XH8830R9KK"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XH8830R9KK")}</script><meta property="og:url" content="https://dingyu.dev/en/posts/worker-pool-async/"><meta property="og:site_name" content="Ding's Coding Forge"><meta property="og:title" content="[Go] Profiling Worker Pool vs. Async Processing"><meta property="og:description" content="This post explores profiling and optimizing worker pools vs. asynchronous execution in Go using pprof. It analyzes the performance impact of concurrent HTTP requests, comparing sync worker pools (10 vs. 100 workers) and a single async worker in terms of throughput, CPU overhead, and memory allocation. Profiling results reveal that worker pools suffer from high concurrency overhead, while asynchronous execution significantly improves throughput with minimal memory cost. Additionally, the post discusses when to use worker pools vs. async processing, highlighting key trade-offs for IO-bound vs. CPU-bound tasks."><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-27T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-27T00:00:00+00:00"><meta property="article:tag" content="Go"><meta property="article:tag" content="Async"><meta property="article:tag" content="Sync"><meta property="article:tag" content="Pprof"><meta property="og:image" content="https://dingyu.dev/en/posts/worker-pool-async/img/go-thumbnail.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://dingyu.dev/en/posts/worker-pool-async/img/go-thumbnail.png"><meta name=twitter:title content="[Go] Profiling Worker Pool vs. Async Processing"><meta name=twitter:description content="This post explores profiling and optimizing worker pools vs. asynchronous execution in Go using pprof. It analyzes the performance impact of concurrent HTTP requests, comparing sync worker pools (10 vs. 100 workers) and a single async worker in terms of throughput, CPU overhead, and memory allocation. Profiling results reveal that worker pools suffer from high concurrency overhead, while asynchronous execution significantly improves throughput with minimal memory cost. Additionally, the post discusses when to use worker pools vs. async processing, highlighting key trade-offs for IO-bound vs. CPU-bound tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://dingyu.dev/en/posts/"},{"@type":"ListItem","position":2,"name":"[Go] Profiling Worker Pool vs. Async Processing","item":"https://dingyu.dev/en/posts/worker-pool-async/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[Go] Profiling Worker Pool vs. Async Processing","name":"[Go] Profiling Worker Pool vs. Async Processing","description":"This post explores profiling and optimizing worker pools vs. asynchronous execution in Go using pprof. It analyzes the performance impact of concurrent HTTP requests, comparing sync worker pools (10 vs. 100 workers) and a single async worker in terms of throughput, CPU overhead, and memory allocation. Profiling results reveal that worker pools suffer from high concurrency overhead, while asynchronous execution significantly improves throughput with minimal memory cost. Additionally, the post discusses when to use worker pools vs. async processing, highlighting key trade-offs for IO-bound vs. CPU-bound tasks.","keywords":["go","async","sync","pprof"],"articleBody":"\nFor how to apply pprof, refer to Tuning GC with pprof\nBackground Our service server was performing one HTTP request per event—triggered frequently and handled asynchronously. However, the more requests we sent, the more linear the delay became.\nProblems Excessive network requests on every event Increased latency due to queuing Lack of scalability — implementation differences between services caused repeated maintenance issues Architecture Improvement Solutions Queue events locally and send batch requests when either buffer size or interval is exceeded. Reduce latency via batching. Build a common library with loose coupling between service servers and the batch loader. Design Decisions We decided to build a shared module called Batch Processor to manage event queues.\nRequirements Optimize for IO-bound tasks Manage goroutine lifecycle cleanly Option 1: Worker Pool (Sync IO) Pros Avoids deep copy overhead Tunable performance via pool size Cons Potentially multiple HTTP requests per interval Hard to tune optimal pool size for every service Option 2: Single Worker + Async HTTP Pros Only one HTTP request per interval Simpler integration without tuning Cons Minor CPU/memory overhead from deep copy GC pressure may increase due to heap allocations Given the trade-offs, we opted for Option 2, but needed to validate that deep copy overhead wouldn’t impact performance.\nProfiling Goals Measure throughput at 2000 RPS Quantify memory impact of deepCopy() Analyze GC overhead from buffer copies func deepCopy[T any](src []T) []T { if src == nil { return nil } dst := make([]T, len(src)) copy(dst, src) return dst } Methodology Compare 3 implementations:\n10 Workers + Sync IO 100 Workers + Sync IO 1 Worker + Async IO Track:\nThroughput via logs Heap profile with: curl {endpoint}/debug/pprof/heap?seconds=30 --output {output_path} Log parsing example:\n2024-10-14T05:11:06Z INF count=1020 2024-10-14T05:11:07Z INF count=1000 2024-10-14T05:11:07Z INF stopping BatchProcessor... CPU Profiling 100 Workers + Sync IO 85% of time spent on sellock and acquireSudog High contention on channel access 10 Workers + Sync IO Lock contention reduced to 66% 1 Worker + Async IO Deep copy overhead ~10ms 50% time split between API calls and deep copy Heap Profiling Worker Pool ~8.2MB total, ~7.7MB from HTTP No deep copy impact 1 Worker + Async IO ~12.2MB total, ~11.4MB from HTTP Deep copy impact: 150kB (~1.22%) — negligible Results Setup Throughput/min CPU Overhead Memory Overhead 10 Workers 83,663 66% 0% 100 Workers 84,042 85% 0% 1 Worker + Async 119,720 50% 1.22% Conclusion Worker pools introduce significant concurrency overhead Increasing worker count doesn’t scale linearly Async execution outperforms worker pools for IO-bound tasks Worker pools remain ideal for CPU-bound tasks When order matters, prefer worker pool even for IO-bound jobs ","wordCount":"431","inLanguage":"en","image":"https://dingyu.dev/en/posts/worker-pool-async/img/go-thumbnail.png","datePublished":"2024-10-27T00:00:00Z","dateModified":"2024-10-27T00:00:00Z","author":{"@type":"Person","name":"dingyu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://dingyu.dev/en/posts/worker-pool-async/"},"publisher":{"@type":"Organization","name":"Ding's Coding Forge","logo":{"@type":"ImageObject","url":"https://dingyu.dev/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://dingyu.dev/en/ accesskey=h title="Ding's Coding Forge (Alt + H)"><img src=https://dingyu.dev/apple-touch-icon.png alt aria-label=logo height=35>Ding's Coding Forge</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li><li><a href=https://dingyu.dev/ title=한국어 aria-label=한국어>Ko</a></li></ul></div></div><ul id=menu><li><a href=https://dingyu.dev/en/about/ title=About><span>About</span></a></li><li><a href=https://dingyu.dev/en/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://dingyu.dev/en/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://dingyu.dev/en/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://dingyu.dev/en/search/ title=🔍><span>🔍</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://dingyu.dev/en/>Home</a>&nbsp;»&nbsp;<a href=https://dingyu.dev/en/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">[Go] Profiling Worker Pool vs. Async Processing</h1><div class=post-description>This post explores profiling and optimizing worker pools vs. asynchronous execution in Go using pprof. It analyzes the performance impact of concurrent HTTP requests, comparing sync worker pools (10 vs. 100 workers) and a single async worker in terms of throughput, CPU overhead, and memory allocation. Profiling results reveal that worker pools suffer from high concurrency overhead, while asynchronous execution significantly improves throughput with minimal memory cost. Additionally, the post discusses when to use worker pools vs. async processing, highlighting key trade-offs for IO-bound vs. CPU-bound tasks.</div><div class=post-meta><span title='2024-10-27 00:00:00 +0000 UTC'>October 27, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;431 words&nbsp;·&nbsp;dingyu&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://dingyu.dev/posts/worker-pool-async/>Ko</a></li></ul>&nbsp;|&nbsp;<a href=https://github.com/dings-things/blog/tree/main/content/posts/worker-pool-async/index.en.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><figure class=entry-cover><img loading=eager src=https://dingyu.dev/img/go-thumbnail.png alt></figure><aside id=toc-container class="toc-container wide"><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#background aria-label=Background>Background</a><ul><li><a href=#problems aria-label=Problems>Problems</a></li></ul></li><li><a href=#architecture-improvement aria-label="Architecture Improvement">Architecture Improvement</a><ul><li><a href=#solutions aria-label=Solutions>Solutions</a></li></ul></li><li><a href=#design-decisions aria-label="Design Decisions">Design Decisions</a><ul><li><a href=#requirements aria-label=Requirements>Requirements</a></li><li><a href=#option-1-worker-pool-sync-io aria-label="Option 1: Worker Pool (Sync IO)">Option 1: Worker Pool (Sync IO)</a><ul><li><a href=#pros aria-label=Pros>Pros</a></li><li><a href=#cons aria-label=Cons>Cons</a></li></ul></li><li><a href=#option-2-single-worker--async-http aria-label="Option 2: Single Worker + Async HTTP">Option 2: Single Worker + Async HTTP</a><ul><li><a href=#pros-1 aria-label=Pros>Pros</a></li><li><a href=#cons-1 aria-label=Cons>Cons</a></li></ul></li></ul></li><li><a href=#profiling-goals aria-label="Profiling Goals">Profiling Goals</a></li><li><a href=#methodology aria-label=Methodology>Methodology</a></li><li><a href=#cpu-profiling aria-label="CPU Profiling">CPU Profiling</a><ul><li><a href=#100-workers--sync-io aria-label="100 Workers + Sync IO">100 Workers + Sync IO</a></li><li><a href=#10-workers--sync-io aria-label="10 Workers + Sync IO">10 Workers + Sync IO</a></li><li><a href=#1-worker--async-io aria-label="1 Worker + Async IO">1 Worker + Async IO</a></li></ul></li><li><a href=#heap-profiling aria-label="Heap Profiling">Heap Profiling</a><ul><li><a href=#worker-pool aria-label="Worker Pool">Worker Pool</a></li><li><a href=#1-worker--async-io-1 aria-label="1 Worker + Async IO">1 Worker + Async IO</a></li></ul></li><li><a href=#results aria-label=Results>Results</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div></aside><script>let activeElement,elements;window.addEventListener("DOMContentLoaded",function(){checkTocPosition(),elements=document.querySelectorAll("h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]"),activeElement=elements[0];const t=encodeURI(activeElement.getAttribute("id")).toLowerCase();document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active")},!1),window.addEventListener("resize",function(){checkTocPosition()},!1),window.addEventListener("scroll",()=>{activeElement=Array.from(elements).find(e=>{if(getOffsetTop(e)-window.pageYOffset>0&&getOffsetTop(e)-window.pageYOffset<window.innerHeight/2)return e})||activeElement,elements.forEach(e=>{const t=encodeURI(e.getAttribute("id")).toLowerCase();e===activeElement?document.querySelector(`.inner ul li a[href="#${t}"]`).classList.add("active"):document.querySelector(`.inner ul li a[href="#${t}"]`).classList.remove("active")})},!1);const main=parseInt(getComputedStyle(document.body).getPropertyValue("--article-width"),10),toc=parseInt(getComputedStyle(document.body).getPropertyValue("--toc-width"),10),gap=parseInt(getComputedStyle(document.body).getPropertyValue("--gap"),10);function checkTocPosition(){const e=document.body.scrollWidth;e-main-toc*2-gap*4>0?document.getElementById("toc-container").classList.add("wide"):document.getElementById("toc-container").classList.remove("wide")}function getOffsetTop(e){if(!e.getClientRects().length)return 0;let t=e.getBoundingClientRect(),n=e.ownerDocument.defaultView;return t.top+n.pageYOffset}</script><div class=post-content><p><img loading=lazy src=img/image.png></p><blockquote><p>For how to apply <code>pprof</code>, refer to <a href=https://velog.io/@wjddn3711/pprof%EB%A1%9C-GC-%ED%8A%9C%EB%8B%9D%ED%95%98%EA%B8%B0>Tuning GC with pprof</a></p></blockquote><h2 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h2><p>Our service server was performing one HTTP request per event—triggered frequently and handled asynchronously. However, the more requests we sent, the more linear the delay became.</p><p><img loading=lazy src=img/image-1.png></p><h3 id=problems>Problems<a hidden class=anchor aria-hidden=true href=#problems>#</a></h3><ol><li><strong>Excessive network requests</strong> on every event</li><li><strong>Increased latency</strong> due to queuing</li><li><strong>Lack of scalability</strong> — implementation differences between services caused repeated maintenance issues</li></ol><h2 id=architecture-improvement>Architecture Improvement<a hidden class=anchor aria-hidden=true href=#architecture-improvement>#</a></h2><p><img loading=lazy src=img/image-2.png></p><h3 id=solutions>Solutions<a hidden class=anchor aria-hidden=true href=#solutions>#</a></h3><ol><li>Queue events locally and send batch requests when either buffer size or interval is exceeded.</li><li>Reduce latency via batching.</li><li>Build a common library with loose coupling between service servers and the batch loader.</li></ol><h2 id=design-decisions>Design Decisions<a hidden class=anchor aria-hidden=true href=#design-decisions>#</a></h2><p>We decided to build a shared module called <strong>Batch Processor</strong> to manage event queues.</p><h3 id=requirements>Requirements<a hidden class=anchor aria-hidden=true href=#requirements>#</a></h3><ul><li>Optimize for <strong>IO-bound tasks</strong></li><li>Manage goroutine lifecycle cleanly</li></ul><h3 id=option-1-worker-pool-sync-io>Option 1: Worker Pool (Sync IO)<a hidden class=anchor aria-hidden=true href=#option-1-worker-pool-sync-io>#</a></h3><p><img loading=lazy src=img/image-3.png></p><h4 id=pros>Pros<a hidden class=anchor aria-hidden=true href=#pros>#</a></h4><ul><li>Avoids deep copy overhead</li><li>Tunable performance via pool size</li></ul><h4 id=cons>Cons<a hidden class=anchor aria-hidden=true href=#cons>#</a></h4><ul><li>Potentially multiple HTTP requests per interval</li><li>Hard to tune optimal pool size for every service</li></ul><h3 id=option-2-single-worker--async-http>Option 2: Single Worker + Async HTTP<a hidden class=anchor aria-hidden=true href=#option-2-single-worker--async-http>#</a></h3><p><img loading=lazy src=img/image-4.png></p><h4 id=pros-1>Pros<a hidden class=anchor aria-hidden=true href=#pros-1>#</a></h4><ul><li>Only one HTTP request per interval</li><li>Simpler integration without tuning</li></ul><h4 id=cons-1>Cons<a hidden class=anchor aria-hidden=true href=#cons-1>#</a></h4><ul><li>Minor CPU/memory overhead from deep copy</li><li>GC pressure may increase due to heap allocations</li></ul><p>Given the trade-offs, we opted for <strong>Option 2</strong>, but needed to validate that deep copy overhead wouldn’t impact performance.</p><h2 id=profiling-goals>Profiling Goals<a hidden class=anchor aria-hidden=true href=#profiling-goals>#</a></h2><ul><li>Measure <strong>throughput</strong> at 2000 RPS</li><li>Quantify <strong>memory impact</strong> of <code>deepCopy()</code></li><li>Analyze <strong>GC overhead</strong> from buffer copies</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-go data-lang=go><span class=line><span class=cl><span class=kd>func</span> <span class=nx>deepCopy</span><span class=p>[</span><span class=nx>T</span> <span class=kt>any</span><span class=p>](</span><span class=nx>src</span> <span class=p>[]</span><span class=nx>T</span><span class=p>)</span> <span class=p>[]</span><span class=nx>T</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=nx>src</span> <span class=o>==</span> <span class=kc>nil</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=kc>nil</span>
</span></span><span class=line><span class=cl>    <span class=p>}</span>
</span></span><span class=line><span class=cl>    <span class=nx>dst</span> <span class=o>:=</span> <span class=nb>make</span><span class=p>([]</span><span class=nx>T</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=nx>src</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    <span class=nb>copy</span><span class=p>(</span><span class=nx>dst</span><span class=p>,</span> <span class=nx>src</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=nx>dst</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=methodology>Methodology<a hidden class=anchor aria-hidden=true href=#methodology>#</a></h2><p>Compare 3 implementations:</p><ul><li><strong>10 Workers + Sync IO</strong></li><li><strong>100 Workers + Sync IO</strong></li><li><strong>1 Worker + Async IO</strong></li></ul><p>Track:</p><ul><li>Throughput via logs</li><li>Heap profile with:</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>curl <span class=o>{</span>endpoint<span class=o>}</span>/debug/pprof/heap?seconds<span class=o>=</span><span class=m>30</span> --output <span class=o>{</span>output_path<span class=o>}</span>
</span></span></code></pre></div><p>Log parsing example:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-text data-lang=text><span class=line><span class=cl>2024-10-14T05:11:06Z INF count=1020
</span></span><span class=line><span class=cl>2024-10-14T05:11:07Z INF count=1000
</span></span><span class=line><span class=cl>2024-10-14T05:11:07Z INF stopping BatchProcessor...
</span></span></code></pre></div><h2 id=cpu-profiling>CPU Profiling<a hidden class=anchor aria-hidden=true href=#cpu-profiling>#</a></h2><h3 id=100-workers--sync-io>100 Workers + Sync IO<a hidden class=anchor aria-hidden=true href=#100-workers--sync-io>#</a></h3><p><img loading=lazy src=img/image-5.png></p><ul><li>85% of time spent on <code>sellock</code> and <code>acquireSudog</code></li><li>High contention on channel access</li></ul><h3 id=10-workers--sync-io>10 Workers + Sync IO<a hidden class=anchor aria-hidden=true href=#10-workers--sync-io>#</a></h3><p><img loading=lazy src=img/image-6.png></p><ul><li>Lock contention reduced to 66%</li></ul><h3 id=1-worker--async-io>1 Worker + Async IO<a hidden class=anchor aria-hidden=true href=#1-worker--async-io>#</a></h3><p><img loading=lazy src=img/image-7.png></p><ul><li>Deep copy overhead ~10ms</li><li>50% time split between API calls and deep copy</li></ul><h2 id=heap-profiling>Heap Profiling<a hidden class=anchor aria-hidden=true href=#heap-profiling>#</a></h2><h3 id=worker-pool>Worker Pool<a hidden class=anchor aria-hidden=true href=#worker-pool>#</a></h3><p><img loading=lazy src=img/image-8.png></p><ul><li>~8.2MB total, ~7.7MB from HTTP</li><li>No deep copy impact</li></ul><h3 id=1-worker--async-io-1>1 Worker + Async IO<a hidden class=anchor aria-hidden=true href=#1-worker--async-io-1>#</a></h3><p><img loading=lazy src=img/image-9.png></p><ul><li>~12.2MB total, ~11.4MB from HTTP</li><li>Deep copy impact: 150kB (~1.22%) — negligible</li></ul><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><table><thead><tr><th>Setup</th><th>Throughput/min</th><th>CPU Overhead</th><th>Memory Overhead</th></tr></thead><tbody><tr><td>10 Workers</td><td>83,663</td><td>66%</td><td>0%</td></tr><tr><td>100 Workers</td><td>84,042</td><td>85%</td><td>0%</td></tr><tr><td>1 Worker + Async</td><td>119,720</td><td>50%</td><td>1.22%</td></tr></tbody></table><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><ul><li>Worker pools introduce significant concurrency overhead</li><li>Increasing worker count doesn’t scale linearly</li><li><strong>Async execution outperforms worker pools</strong> for IO-bound tasks</li><li>Worker pools remain ideal for CPU-bound tasks</li><li>When order matters, prefer worker pool even for IO-bound jobs</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://dingyu.dev/en/tags/go/>Go</a></li><li><a href=https://dingyu.dev/en/tags/async/>Async</a></li><li><a href=https://dingyu.dev/en/tags/sync/>Sync</a></li><li><a href=https://dingyu.dev/en/tags/pprof/>Pprof</a></li></ul><nav class=paginav><a class=prev href=https://dingyu.dev/en/posts/gopher-con-2024-kubernetes-programing/><span class=title>« Prev</span><br><span>[Go] Gophercon 2024 - Kubernetes Platform Programming</span>
</a><a class=next href=https://dingyu.dev/en/posts/coffee-pal/><span class=title>Next »</span><br><span>[DX] Building a Slack Bot for Internal Coffee Chats</span></a></nav></footer><div id=giscus_thread><script src=https://giscus.app/client.js data-repo=dings-things/blog data-repo-id=R_kgDON9IuAw data-category=Announcements data-category-id=DIC_kwDON9IuA84CnTai data-mapping=og:title data-strict=0 data-reactions-enabled=1 data-emit-metadata=0 data-input-position=bottom data-theme=preferred_color_scheme data-lang=ko data-loading=lazy crossorigin=anonymous async></script></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://dingyu.dev/en/>Ding's Coding Forge</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>